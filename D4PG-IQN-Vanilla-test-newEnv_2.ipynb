{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "import random\n",
    "import math\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from collections import deque, namedtuple\n",
    "import time\n",
    "import gym\n",
    "import os\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "def weight_init(layers):\n",
    "    for layer in layers:\n",
    "        torch.nn.init.kaiming_normal_(layer.weight, nonlinearity='relu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3.common.vec_env.base_vec_env import VecEnv, VecEnvStepReturn, VecEnvWrapper\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Actor(nn.Module):\n",
    "    \"\"\"Actor (Policy) Model.\"\"\"\n",
    "\n",
    "    def __init__(self, state_size, action_size, seed, hidden_size=256):\n",
    "        \"\"\"Initialize parameters and build model.\n",
    "        Params\n",
    "        ======\n",
    "            state_size (int): Dimension of each state\n",
    "            action_size (int): Dimension of each action\n",
    "            seed (int): Random seed\n",
    "            fc1_units (int): Number of nodes in first hidden layer\n",
    "            fc2_units (int): Number of nodes in second hidden layer\n",
    "        \"\"\"\n",
    "        super(Actor, self).__init__()\n",
    "        self.seed = torch.manual_seed(seed)\n",
    "        self.fc1 = nn.Linear(state_size, hidden_size)\n",
    "        self.batch_norm = nn.BatchNorm1d(hidden_size) ## seems to improve the final performance a lot\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc3 = nn.Linear(hidden_size, action_size)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        self.fc1.weight.data.uniform_(*hidden_init(self.fc1))\n",
    "        self.fc2.weight.data.uniform_(*hidden_init(self.fc2))\n",
    "        self.fc3.weight.data.uniform_(-3e-3, 3e-3)\n",
    "\n",
    "    def forward(self, state):\n",
    "        \"\"\"Build an actor (policy) network that maps states -> actions.\"\"\"\n",
    "        x = torch.relu(self.fc1(state)) #self.batch_norm\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        return torch.tanh(self.fc3(x))\n",
    "\n",
    "\n",
    "class Critic(nn.Module):\n",
    "    \"\"\"Critic (Value) Model.\"\"\"\n",
    "\n",
    "    def __init__(self, state_size, action_size, seed, hidden_size=256):\n",
    "        \"\"\"Initialize parameters and build model.\n",
    "        Params\n",
    "        ======\n",
    "            state_size (int): Dimension of each state\n",
    "            action_size (int): Dimension of each action\n",
    "            seed (int): Random seed\n",
    "            fcs1_units (int): Number of nodes in the first hidden layer\n",
    "            fc2_units (int): Number of nodes in the second hidden layer\n",
    "        \"\"\"\n",
    "        super(Critic, self).__init__()\n",
    "        self.seed = torch.manual_seed(seed)\n",
    "        self.fcs1 = nn.Linear(state_size, hidden_size)\n",
    "        self.batch_norm = nn.BatchNorm1d(hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size+action_size, hidden_size)\n",
    "        self.fc3 = nn.Linear(hidden_size, 1)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        self.fcs1.weight.data.uniform_(*hidden_init(self.fcs1))\n",
    "        self.fc2.weight.data.uniform_(*hidden_init(self.fc2))\n",
    "        self.fc3.weight.data.uniform_(-3e-3, 3e-3)\n",
    "\n",
    "    def forward(self, state, action):\n",
    "        \"\"\"Build a critic (value) network that maps (state, action) pairs -> Q-values.\"\"\"\n",
    "        xs = F.relu(self.fcs1(state)) # self.batch_norm\n",
    "        x = torch.cat((xs, action), dim=1)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return self.fc3(x)\n",
    "\n",
    "class IQN(nn.Module):\n",
    "    def __init__(self, state_size, action_size, layer_size, seed, N, dueling=False, device=\"cuda:0\"):\n",
    "        super(IQN, self).__init__()\n",
    "        self.seed = torch.manual_seed(seed)\n",
    "        self.input_shape = state_size\n",
    "        self.action_size = action_size\n",
    "        self.N = N  \n",
    "        self.n_cos = 64\n",
    "        self.layer_size = layer_size\n",
    "        self.pis = torch.FloatTensor([np.pi*i for i in range(1,self.n_cos+1)]).view(1,1,self.n_cos).to(device) # Starting from 0 as in the paper \n",
    "        self.dueling = dueling\n",
    "        self.device = device\n",
    "\n",
    "        # Network Architecture\n",
    "\n",
    "        self.head = nn.Linear(self.action_size+self.input_shape, layer_size) \n",
    "        self.cos_embedding = nn.Linear(self.n_cos, layer_size)\n",
    "        self.ff_1 = nn.Linear(layer_size, layer_size)\n",
    "        self.ff_2 = nn.Linear(layer_size, 1)    \n",
    "        #weight_init([self.head_1, self.ff_1])\n",
    "\n",
    "    def calc_input_layer(self):\n",
    "        x = torch.zeros(self.input_shape).unsqueeze(0)\n",
    "        x = self.head(x)\n",
    "        return x.flatten().shape[0]\n",
    "        \n",
    "    def calc_cos(self, batch_size, n_tau=32):\n",
    "        \"\"\"\n",
    "        Calculating the cosinus values depending on the number of tau samples\n",
    "        \"\"\"\n",
    "        taus = torch.rand(batch_size, n_tau).unsqueeze(-1).to(self.device) #(batch_size, n_tau, 1)  .to(self.device)\n",
    "        cos = torch.cos(taus*self.pis)\n",
    "\n",
    "        assert cos.shape == (batch_size,n_tau,self.n_cos), \"cos shape is incorrect\"\n",
    "        return cos, taus\n",
    "    \n",
    "    def forward(self, input, action, num_tau=32):\n",
    "        \"\"\"\n",
    "        Quantile Calculation depending on the number of tau\n",
    "        \n",
    "        Return:\n",
    "        quantiles [ shape of (batch_size, num_tau, action_size)]\n",
    "        taus [shape of ((batch_size, num_tau, 1))]\n",
    "        \n",
    "        \"\"\"\n",
    "        batch_size = input.shape[0]\n",
    "\n",
    "        x = torch.cat((input, action), dim=1)\n",
    "        x = torch.relu(self.head(x  ))\n",
    "        \n",
    "        cos, taus = self.calc_cos(batch_size, num_tau) # cos shape (batch, num_tau, layer_size)\n",
    "        cos = cos.view(batch_size*num_tau, self.n_cos)\n",
    "        cos_x = torch.relu(self.cos_embedding(cos)).view(batch_size, num_tau, self.layer_size) # (batch, n_tau, layer)\n",
    "        \n",
    "        # x has shape (batch, layer_size) for multiplication â€“> reshape to (batch, 1, layer)\n",
    "        x = (x.unsqueeze(1)*cos_x).view(batch_size*num_tau, self.layer_size)  #batch_size*num_tau, self.cos_layer_out\n",
    "        # Following reshape and transpose is done to bring the action in the same shape as batch*tau:\n",
    "        # first 32 entries are tau for each action -> thats why each action one needs to be repeated 32 times \n",
    "        # x = [[tau1   action = [[a1\n",
    "        #       tau1              a1   \n",
    "        #        ..               ..\n",
    "        #       tau2              a2\n",
    "        #       tau2              a2\n",
    "        #       ..]]              ..]]  \n",
    "        #action = action.repeat(num_tau,1).reshape(num_tau,batch_size*self.action_size).transpose(0,1).reshape(batch_size*num_tau,self.action_size)\n",
    "        #x = torch.cat((x,action),dim=1)\n",
    "        x = torch.relu(self.ff_1(x))\n",
    "\n",
    "        out = self.ff_2(x)\n",
    "        \n",
    "        return out.view(batch_size, num_tau, 1), taus\n",
    "    \n",
    "    def get_qvalues(self, inputs, action):\n",
    "        quantiles, _ = self.forward(inputs, action, self.N)\n",
    "        actions = quantiles.mean(dim=1)\n",
    "        return actions  \n",
    "\n",
    "\n",
    "\n",
    "class DeepActor(nn.Module):\n",
    "    \"\"\"Actor (Policy) Model.\"\"\"\n",
    "\n",
    "    def __init__(self, state_size, action_size, seed, hidden_size=256):\n",
    "        \"\"\"Initialize parameters and build model.\n",
    "        Params\n",
    "        ======\n",
    "            state_size (int): Dimension of each state\n",
    "            action_size (int): Dimension of each action\n",
    "            seed (int): Random seed\n",
    "            fc1_units (int): Number of nodes in first hidden layer\n",
    "            fc2_units (int): Number of nodes in second hidden layer\n",
    "        \"\"\"\n",
    "        super(DeepActor, self).__init__()\n",
    "        self.seed = torch.manual_seed(seed)\n",
    "        self.input_size = hidden_size+state_size\n",
    "        self.fc1 = nn.Linear(state_size, hidden_size)\n",
    "        #self.batch_norm = nn.BatchNorm1d(fc1_units)\n",
    "        self.fc2 = nn.Linear(self.input_size, hidden_size)\n",
    "        self.fc3 = nn.Linear(self.input_size, hidden_size)\n",
    "        self.fc4 = nn.Linear(self.input_size, hidden_size)\n",
    "        self.fc5 = nn.Linear(hidden_size, action_size)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        self.fc1.weight.data.uniform_(*hidden_init(self.fc1))\n",
    "        self.fc2.weight.data.uniform_(*hidden_init(self.fc2))\n",
    "        self.fc3.weight.data.uniform_(*hidden_init(self.fc3)) \n",
    "        self.fc4.weight.data.uniform_(*hidden_init(self.fc4))\n",
    "        self.fc5.weight.data.uniform_(-3e-3, 3e-3)\n",
    "\n",
    "    def forward(self, state):\n",
    "        \"\"\"Build an actor (policy) network that maps states -> actions.\"\"\"\n",
    "        x = torch.relu(self.fc1(state))\n",
    "        x = torch.cat((x,state), dim=1)\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = torch.cat((x,state), dim=1)\n",
    "        x = torch.relu(self.fc3(x))\n",
    "        x = torch.cat((x,state), dim=1)\n",
    "        x = torch.relu(self.fc4(x))\n",
    "        return torch.tanh(self.fc5(x))\n",
    "\n",
    "\n",
    "class DeepCritic(nn.Module):\n",
    "    \"\"\"Critic (Value) Model.\"\"\"\n",
    "\n",
    "    def __init__(self, state_size, action_size, seed, hidden_size=256):\n",
    "        \"\"\"Initialize parameters and build model.\n",
    "        Params\n",
    "        ======\n",
    "            state_size (int): Dimension of each state\n",
    "            action_size (int): Dimension of each action\n",
    "            seed (int): Random seed\n",
    "            fcs1_units (int): Number of nodes in the first hidden layer\n",
    "            fc2_units (int): Number of nodes in the second hidden layer\n",
    "        \"\"\"\n",
    "        super(DeepCritic, self).__init__()\n",
    "        self.seed = torch.manual_seed(seed)\n",
    "        self.input_dim = hidden_size+action_size+state_size\n",
    "        self.fc1 = nn.Linear(state_size+action_size, hidden_size)\n",
    "        #.batch_norm = nn.BatchNorm1d(fcs1_units)\n",
    "        self.fc2 = nn.Linear(self.input_dim, hidden_size)\n",
    "        self.fc3 = nn.Linear(self.input_dim, hidden_size)\n",
    "        self.fc4 = nn.Linear(self.input_dim, hidden_size)\n",
    "        self.fc5 = nn.Linear(hidden_size, 1)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        self.fc1.weight.data.uniform_(*hidden_init(self.fc1))\n",
    "        self.fc2.weight.data.uniform_(*hidden_init(self.fc2))\n",
    "        self.fc3.weight.data.uniform_(*hidden_init(self.fc3))\n",
    "        self.fc4.weight.data.uniform_(*hidden_init(self.fc4))\n",
    "        self.fc5.weight.data.uniform_(-3e-3, 3e-3)\n",
    "\n",
    "    def forward(self, state, action):\n",
    "        \"\"\"Build a critic (value) network that maps (state, action) pairs -> Q-values.\"\"\"\n",
    "        xu = torch.cat((state, action), dim=1)\n",
    "        x = F.relu(self.fc1(xu))\n",
    "        x = torch.cat((x, xu), dim=1)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = torch.cat((x, xu), dim=1)\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = torch.cat((x, xu), dim=1)\n",
    "        x = F.relu(self.fc4(x))\n",
    "        return self.fc5(x)\n",
    "\n",
    "class DeepIQN(nn.Module):\n",
    "    def __init__(self, state_size, action_size, layer_size, seed, N, dueling=False, device=\"cuda:0\"):\n",
    "        super(DeepIQN, self).__init__()\n",
    "        self.seed = torch.manual_seed(seed)\n",
    "        self.input_shape = state_size\n",
    "        self.action_size = action_size\n",
    "        self.input_dim = action_size+state_size+layer_size\n",
    "        self.N = N  \n",
    "        self.n_cos = 64\n",
    "        self.layer_size = layer_size\n",
    "        self.pis = torch.FloatTensor([np.pi*i for i in range(1,self.n_cos+1)]).view(1,1,self.n_cos).to(device) # Starting from 0 as in the paper \n",
    "        self.dueling = dueling\n",
    "        self.device = device\n",
    "\n",
    "        # Network Architecture\n",
    "\n",
    "        self.head = nn.Linear(self.action_size+self.input_shape, layer_size) \n",
    "        self.ff_1 = nn.Linear(self.input_dim, layer_size)\n",
    "        self.ff_2 = nn.Linear(self.input_dim, layer_size)\n",
    "        self.cos_embedding = nn.Linear(self.n_cos, layer_size)\n",
    "        self.ff_3 = nn.Linear(self.input_dim, layer_size)\n",
    "        self.ff_4 = nn.Linear(self.layer_size, 1)    \n",
    "        #weight_init([self.head_1, self.ff_1])  \n",
    "\n",
    "    def calc_input_layer(self):\n",
    "        x = torch.zeros(self.input_shape).unsqueeze(0)\n",
    "        x = self.head(x)\n",
    "        return x.flatten().shape[0]\n",
    "        \n",
    "    def calc_cos(self, batch_size, n_tau=32):\n",
    "        \"\"\"\n",
    "        Calculating the cosinus values depending on the number of tau samples\n",
    "        \"\"\"\n",
    "        taus = torch.rand(batch_size, n_tau).unsqueeze(-1).to(self.device) #(batch_size, n_tau, 1)  .to(self.device)\n",
    "        cos = torch.cos(taus*self.pis)\n",
    "\n",
    "        assert cos.shape == (batch_size,n_tau,self.n_cos), \"cos shape is incorrect\"\n",
    "        return cos, taus\n",
    "    \n",
    "    def forward(self, input, action, num_tau=32):\n",
    "        \"\"\"\n",
    "        Quantile Calculation depending on the number of tau\n",
    "        \n",
    "        Return:\n",
    "        quantiles [ shape of (batch_size, num_tau, action_size)]\n",
    "        taus [shape of ((batch_size, num_tau, 1))]\n",
    "        \n",
    "        \"\"\"\n",
    "        batch_size = input.shape[0]\n",
    "        xs = torch.cat((input, action), dim=1)\n",
    "        x = torch.relu(self.head(xs))\n",
    "        x = torch.cat((x, xs), dim=1)\n",
    "        x = torch.relu(self.ff_1(x))\n",
    "        x = torch.cat((x, xs), dim=1)\n",
    "        x = torch.relu(self.ff_2(x))\n",
    "\n",
    "        cos, taus = self.calc_cos(batch_size, num_tau) # cos shape (batch, num_tau, layer_size)\n",
    "        cos = cos.view(batch_size*num_tau, self.n_cos)\n",
    "        cos_x = torch.relu(self.cos_embedding(cos)).view(batch_size, num_tau, self.layer_size) # (batch, n_tau, layer)\n",
    "        \n",
    "        # x has shape (batch, layer_size) for multiplication â€“> reshape to (batch, 1, layer)\n",
    "        x = (x.unsqueeze(1)*cos_x).view(batch_size*num_tau, self.layer_size)  #batch_size*num_tau, self.cos_layer_out\n",
    "        # Following reshape and transpose is done to bring the action in the same shape as batch*tau:\n",
    "        # first 32 entries are tau for each action -> thats why each action one needs to be repeated 32 times \n",
    "        # x = [[tau1   action = [[a1\n",
    "        #       tau1              a1   \n",
    "        #        ..               ..\n",
    "        #       tau2              a2\n",
    "        #       tau2              a2\n",
    "        #       ..]]              ..]]  \n",
    "        action = action.repeat(num_tau,1).reshape(num_tau,batch_size*self.action_size).transpose(0,1).reshape(batch_size*num_tau,self.action_size)\n",
    "        state = input.repeat(num_tau,1).reshape(num_tau,batch_size*self.input_shape).transpose(0,1).reshape(batch_size*num_tau,self.input_shape)\n",
    "        \n",
    "        x = torch.cat((x,action,state),dim=1)\n",
    "        x = torch.relu(self.ff_3(x))\n",
    "\n",
    "        out = self.ff_4(x)\n",
    "        \n",
    "        return out.view(batch_size, num_tau, 1), taus\n",
    "    \n",
    "    def get_qvalues(self, inputs, action):\n",
    "        quantiles, _ = self.forward(inputs, action, self.N)\n",
    "        actions = quantiles.mean(dim=1)\n",
    "        return actions  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from stockstats import StockDataFrame as Sdf\n",
    "\n",
    "import datetime\n",
    "\n",
    "import datetime\n",
    "import os\n",
    "\n",
    "TRAINING_DATA_FILE = \"dataprocessing/Yfinance_Data.csv\"\n",
    "\n",
    "now = datetime.datetime.now()\n",
    "TRAINED_MODEL_DIR = f\"trained_models/{now}\"\n",
    "os.makedirs(TRAINED_MODEL_DIR)\n",
    "\n",
    "TESTING_DATA_FILE = \"test.csv\"\n",
    "\n",
    "def load_dataset(*, file_name: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    load csv dataset from path\n",
    "    :return: (df) pandas dataframe\n",
    "    \"\"\"\n",
    "    # _data = pd.read_csv(f\"{config.DATASET_DIR}/{file_name}\")\n",
    "    _data = pd.read_csv(file_name)\n",
    "\n",
    "    return _data\n",
    "\n",
    "\n",
    "def data_split(df, start, end):\n",
    "    \"\"\"\n",
    "    split the dataset into training or testing using date\n",
    "    :param data: (df) pandas dataframe, start, end\n",
    "    :return: (df) pandas dataframe\n",
    "    \"\"\"\n",
    "    data = df[(df.datadate >= start) & (df.datadate < end)]\n",
    "    data = data.sort_values(['datadate', 'tic'], ignore_index=True)\n",
    "\n",
    "\n",
    "    # data  = data[final_columns]\n",
    "    data.index = data.datadate.factorize()[0]\n",
    "\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def calculate_price(df):\n",
    "    \"\"\"\n",
    "    calcualte adjusted close price, open-high-low price and volume\n",
    "    :param data: (df) pandas dataframe\n",
    "    :return: (df) pandas dataframe\n",
    "    \"\"\"\n",
    "    data = df.copy()\n",
    "\n",
    "    data = data[['Date', 'tic', 'Close', 'Open', 'High', 'Low', 'Volume','datadate']]\n",
    "    data = data.sort_values(['tic', 'datadate'], ignore_index=True)\n",
    "    return data\n",
    "\n",
    "\n",
    "def add_technical_indicator(df):\n",
    "    \"\"\"\n",
    "    calcualte technical indicators\n",
    "    use stockstats package to add technical inidactors\n",
    "    :param data: (df) pandas dataframe\n",
    "    :return: (df) pandas dataframe\n",
    "    \"\"\"\n",
    "    stock = Sdf.retype(df.copy())\n",
    "\n",
    "    #print(stock)\n",
    "\n",
    "    unique_ticker = stock.tic.unique()\n",
    "\n",
    "    macd = pd.DataFrame()\n",
    "    rsi = pd.DataFrame()\n",
    "    cci = pd.DataFrame()\n",
    "    dx = pd.DataFrame()\n",
    "\n",
    "    # temp = stock[stock.tic == unique_ticker[0]]['macd']\n",
    "    for i in range(len(unique_ticker)):\n",
    "        ## macd\n",
    "        temp_macd = stock[stock.tic == unique_ticker[i]]['macd']\n",
    "        temp_macd = pd.DataFrame(temp_macd)\n",
    "        macd = macd.append(temp_macd, ignore_index=True)\n",
    "        ## rsi\n",
    "        temp_rsi = stock[stock.tic == unique_ticker[i]]['rsi_30']\n",
    "        temp_rsi = pd.DataFrame(temp_rsi)\n",
    "        rsi = rsi.append(temp_rsi, ignore_index=True)\n",
    "        ## cci\n",
    "        temp_cci = stock[stock.tic == unique_ticker[i]]['cci_30']\n",
    "        temp_cci = pd.DataFrame(temp_cci)\n",
    "        cci = cci.append(temp_cci, ignore_index=True)\n",
    "        ## adx\n",
    "        temp_dx = stock[stock.tic == unique_ticker[i]]['dx_30']\n",
    "        temp_dx = pd.DataFrame(temp_dx)\n",
    "        dx = dx.append(temp_dx, ignore_index=True)\n",
    "\n",
    "    df['macd'] = macd\n",
    "    df['rsi'] = rsi\n",
    "    df['cci'] = cci\n",
    "    df['adx'] = dx\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def preprocess_data():\n",
    "    \"\"\"data preprocessing pipeline\"\"\"\n",
    "    start = datetime.datetime(2010, 12, 1)\n",
    "    df = load_dataset(file_name=TRAINING_DATA_FILE)\n",
    "    # get data after 2010\n",
    "    # df = df[df.Date >= start]\n",
    "    # calcualte adjusted price\n",
    "    df_preprocess = calculate_price(df)\n",
    "    # add technical indicators using stockstats\n",
    "    df_final = add_technical_indicator(df_preprocess)\n",
    "    # fill the missing values at the beginning\n",
    "    df_final.fillna(method='bfill', inplace=True)\n",
    "    return df_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "\n",
    "class Agent():\n",
    "    \"\"\"Interacts with and learns from the environment.\"\"\"\n",
    "    \n",
    "    def __init__(self, state_size,\n",
    "                      action_size,\n",
    "                      n_step,\n",
    "                      per, \n",
    "                      munchausen,\n",
    "                      distributional,\n",
    "                      D2RL,\n",
    "                      random_seed=2,\n",
    "                      hidden_size=400,\n",
    "                      BUFFER_SIZE = int(1e6),  # replay buffer size\n",
    "                      BATCH_SIZE = 32,        # minibatch size\n",
    "                      GAMMA = 0.99,            # discount factor\n",
    "                      TAU = 1e-3,              # for soft update of target parameters\n",
    "                      LR_ACTOR = 1e-3,         # learning rate of the actor \n",
    "                      LR_CRITIC = 1e-3,        # learning rate of the critic\n",
    "                      WEIGHT_DECAY = 1e-2,        # L2 weight decay\n",
    "                      LEARN_EVERY = 1,\n",
    "                      LEARN_NUMBER = 1,\n",
    "                      EPSILON = 1.0,\n",
    "                      EPSILON_DECAY = 1,\n",
    "                      device = \"cuda\",\n",
    "                      frames = 100000,\n",
    "                      worker=1\n",
    "                      ):\n",
    "        \"\"\"Initialize an Agent object.\n",
    "        \n",
    "        Params\n",
    "        ======\n",
    "            state_size (int): dimension of each state\n",
    "            action_size (int): dimension of each action\n",
    "            random_seed (int): random seed\n",
    "        \"\"\"\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.BUFFER_SIZE = BUFFER_SIZE\n",
    "        self.BATCH_SIZE = BATCH_SIZE\n",
    "        self.per = per\n",
    "        self.munchausen = munchausen\n",
    "        self.n_step = n_step\n",
    "        self.distributional = distributional\n",
    "        self.D2RL = D2RL\n",
    "        self.GAMMA = GAMMA\n",
    "        self.TAU = TAU\n",
    "        self.LEARN_EVERY = LEARN_EVERY\n",
    "        self.LEARN_NUMBER = LEARN_NUMBER\n",
    "        self.EPSILON_DECAY = EPSILON_DECAY\n",
    "        self.epsilon = 0.04\n",
    "        self.device = device\n",
    "        self.seed = random.seed(random_seed)\n",
    "        # distributional Values\n",
    "        self.N = 32\n",
    "        self.entropy_coeff = 0.001\n",
    "        # munchausen values\n",
    "        self.entropy_tau = 0.03\n",
    "        self.lo = -1\n",
    "        self.alpha = 0.9\n",
    "        self.last_action = []\n",
    "        self.action = []\n",
    "        self.eta = torch.FloatTensor([.1]).to(device)\n",
    "        \n",
    "        print(\"Using: \", device)\n",
    "        print(\"seed agent: \", self.seed)\n",
    "        \n",
    "        # Actor Network (w/ Target Network)\n",
    "        self.actor_local = Actor(state_size, action_size, random_seed, hidden_size=hidden_size).to(device)\n",
    "        self.actor_target = Actor(state_size, action_size, random_seed, hidden_size=hidden_size).to(device)\n",
    "\n",
    "        self.actor_optimizer = optim.Adam(self.actor_local.parameters(), lr=LR_ACTOR)\n",
    "\n",
    "        # Critic Network (w/ Target Network)\n",
    "\n",
    "        #self.critic_local = DeepIQN(state_size, action_size, layer_size=hidden_size, device=device, seed=random_seed, dueling=None, N=self.N).to(device)\n",
    "        #self.critic_target = DeepIQN(state_size, action_size, layer_size=hidden_size, device=device, seed=random_seed, dueling=None, N=self.N).to(device)\n",
    "        \n",
    "        self.critic_local = Critic(state_size, action_size, random_seed).to(device)\n",
    "        self.critic_target = Critic(state_size, action_size, random_seed).to(device)\n",
    "        \n",
    "        self.critic_optimizer = optim.Adam(self.critic_local.parameters(), lr=LR_CRITIC, weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "        print(\"Actor: \\n\", self.actor_local)\n",
    "        print(\"\\nCritic: \\n\", self.critic_local)\n",
    "\n",
    "        #self.memory = ReplayBuffer(BUFFER_SIZE, BATCH_SIZE, n_step=n_step, parallel_env=worker, device=device, seed=random_seed, gamma=GAMMA)\n",
    "        #self.memory = PrioritizedReplay(device, BUFFER_SIZE, self.BATCH_SIZE, gamma=self.GAMMA, n_step=n_step, parallel_env=1)\n",
    "        self.memory = PrioritizedReplay(BUFFER_SIZE, BATCH_SIZE, device=device, seed=random_seed, gamma=GAMMA, n_step=n_step, parallel_env=worker, beta_frames=frames)\n",
    "        self.noise = OUNoise(action_size, random_seed)\n",
    "        self.learn = self.learn_\n",
    "        \n",
    "        \n",
    "\n",
    "        \n",
    "    def step(self, state, action, reward, next_state, done, timestamp, writer):\n",
    "        \"\"\"Save experience in replay memory, and use random sample from buffer to learn.\"\"\"\n",
    "        # Save experience / reward\n",
    "        \n",
    "        #rint('agent step reward:{}'.format(reward))\n",
    "        self.memory.add(state, action, reward, next_state, done)\n",
    "\n",
    "        \n",
    "        \n",
    "        # Learn, if enough samples are available in memory\n",
    "        if len(self.memory) > self.BATCH_SIZE and timestamp % self.LEARN_EVERY == 0:\n",
    "            for _ in range(self.LEARN_NUMBER):\n",
    "                \n",
    "                experiences = self.memory.sample()\n",
    "                #print(experiences)\n",
    "                losses = self.learn(experiences, self.GAMMA)\n",
    "            writer.add_scalar(\"Critic_loss\", losses[0], timestamp)\n",
    "            writer.add_scalar(\"Actor_loss\", losses[1], timestamp)\n",
    "\n",
    "    def act(self, state):\n",
    "        \"\"\"Returns actions for given state as per current policy.\"\"\"\n",
    "        #state = torch.from_numpy(state).float().to(self.device)\n",
    "        state = torch.from_numpy(state).float().unsqueeze(0).to(device)\n",
    "        #print(state)\n",
    "        \n",
    "        print('state.shape[0]:{}'.format(state.shape))\n",
    "        #print('self.state_size:{}'.format(self.state_size))\n",
    "    \n",
    "\n",
    "        assert state.shape == (state.shape[0],self.state_size), \"shape: {}\".format(state.shape)\n",
    "        self.actor_local.eval()\n",
    "        with torch.no_grad():\n",
    "                action = self.actor_local(state).cpu().data.numpy().squeeze(0)\n",
    "        self.actor_local.train()\n",
    "        \n",
    "        # Epsilon-greedy action selection\n",
    "\n",
    "        # Epsilon-greedy action selection\n",
    "        \"\"\"\n",
    "        if random.random() > eps: # select greedy action if random number is higher than epsilon or noisy network is used!\n",
    "            action = np.argmax(action_values.cpu().data.numpy())\n",
    "            self.last_action = action\n",
    "            return action\n",
    "        else:\n",
    "            action = random.choice(np.arange(self.action_size))\n",
    "            self.last_action = action \"\"\"\n",
    "        action += self.noise.sample() * self.epsilon\n",
    "        \n",
    "        return action #np.clip(action, -1, 1)\n",
    "\n",
    "    def reset(self):\n",
    "        #print('self.seed',self.seed)\n",
    "        self.noise.reset()\n",
    "\n",
    "    \n",
    "    def soft_update(self, local_model, target_model):\n",
    "        \"\"\"Soft update model parameters.\n",
    "        Î¸_target = Ï„*Î¸_local + (1 - Ï„)*Î¸_target\n",
    "\n",
    "        Params\n",
    "        ======\n",
    "            local_model: PyTorch model (weights will be copied from)\n",
    "            target_model: PyTorch model (weights will be copied to)\n",
    "            tau (float): interpolation parameter \n",
    "        \"\"\"\n",
    "        for target_param, local_param in zip(target_model.parameters(), local_model.parameters()):\n",
    "            target_param.data.copy_(self.TAU*local_param.data + (1.0-self.TAU)*target_param.data)\n",
    "\n",
    "\n",
    "    def learn_(self, experiences, gamma):\n",
    "        \"\"\"Update policy and value parameters using given batch of experience tuples.\n",
    "        Q_targets = r + Î³ * critic_target(next_state, actor_target(next_state))\n",
    "        where:\n",
    "            actor_target(state) -> action\n",
    "            critic_target(state, action) -> Q-value\n",
    "        Params\n",
    "        ======\n",
    "            experiences (Tuple[torch.Tensor]): tuple of (s, a, r, s', done) tuples \n",
    "            gamma (float): discount factor\n",
    "        \"\"\"\n",
    "        states, actions, rewards, next_states, dones, idx, weights = experiences\n",
    "        icm_loss = 0\n",
    "\n",
    "\n",
    "        # ---------------------------- update critic ---------------------------- #\n",
    "\n",
    "        with torch.no_grad():\n",
    "            actions_next = self.actor_target(next_states.to(self.device))\n",
    "            Q_targets_next = self.critic_target(next_states.to(self.device), actions_next.to(self.device))\n",
    "            # Compute Q targets for current states (y_i)\n",
    "            Q_targets = rewards + (gamma**self.n_step * Q_targets_next * (1 - dones))\n",
    "       \n",
    "        # Compute critic loss\n",
    "        Q_expected = self.critic_local(states, actions)\n",
    "        td_error =  Q_targets - Q_expected\n",
    "        critic_loss = (td_error.pow(2)*weights.to(self.device)).mean().to(self.device)\n",
    "\n",
    "        # Minimize the loss\n",
    "        self.critic_optimizer.zero_grad()\n",
    "        critic_loss.backward()\n",
    "        clip_grad_norm_(self.critic_local.parameters(), 1)\n",
    "        self.critic_optimizer.step()\n",
    "\n",
    "        # ---------------------------- update actor ---------------------------- #\n",
    "        # Compute actor loss\n",
    "        actions_pred = self.actor_local(states)\n",
    "        actor_loss = -self.critic_local(states, actions_pred).mean()\n",
    "        # Minimize the loss\n",
    "        self.actor_optimizer.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        self.actor_optimizer.step()\n",
    "\n",
    "        # ----------------------- update target networks ----------------------- #\n",
    "        self.soft_update(self.critic_local, self.critic_target)\n",
    "        self.soft_update(self.actor_local, self.actor_target)                     \n",
    "\n",
    "        self.memory.update_priorities(idx, np.clip(abs(td_error.data.cpu().numpy()),-1,1))\n",
    "        # ----------------------- update epsilon and noise ----------------------- #\n",
    "        \n",
    "        self.epsilon *= self.EPSILON_DECAY\n",
    "        \n",
    "        self.noise.reset()\n",
    "        return critic_loss.detach().cpu().numpy(), actor_loss.detach().cpu().numpy(), icm_loss\n",
    "            \n",
    "    def learn_distribution(self, experiences, gamma):\n",
    "            \"\"\"Update policy and value parameters using given batch of experience tuples.\n",
    "            Q_targets = r + Î³ * critic_target(next_state, actor_target(next_state))\n",
    "            where:\n",
    "                actor_target(state) -> action\n",
    "                critic_target(state, action) -> Q-value\n",
    "\n",
    "            Params\n",
    "            ======\n",
    "                experiences (Tuple[torch.Tensor]): tuple of (s, a, r, s', done) tuples \n",
    "                gamma (float): discount factor\n",
    "            \"\"\"\n",
    "            states, actions, rewards, next_states, dones, idx, weights = experiences\n",
    "            \n",
    "            #print('states:{}'.format(states.shape))\n",
    "            #print('rewards:{}'.format(rewards))\n",
    "            # ---------------------------- update critic ---------------------------- #\n",
    "            # Get predicted next-state actions and Q values from target models\n",
    "\n",
    "            # Get max predicted Q values (for next states) from target model\n",
    "            with torch.no_grad():\n",
    "                next_actions = self.actor_local(next_states)\n",
    "                Q_targets_next, _ = self.critic_target(next_states, next_actions, self.N)\n",
    "                Q_targets_next = Q_targets_next.transpose(1,2)\n",
    "            # Compute Q targets for current states \n",
    "            Q_targets = rewards.unsqueeze(-1) + (self.GAMMA**self.n_step * Q_targets_next.to(self.device) * (1. - dones.unsqueeze(-1)))\n",
    "                \n",
    "            # Get expected Q values from local model\n",
    "            Q_expected, taus = self.critic_local(states, actions, self.N)\n",
    "            assert Q_targets.shape == (self.BATCH_SIZE, 1, self.N)\n",
    "            assert Q_expected.shape == (self.BATCH_SIZE, self.N, 1)\n",
    "    \n",
    "            # Quantile Huber loss\n",
    "            td_error = Q_targets - Q_expected\n",
    "            assert td_error.shape == (self.BATCH_SIZE, self.N, self.N), \"wrong td error shape\"\n",
    "            huber_l = calculate_huber_loss(td_error, 1.0)\n",
    "            quantil_l = abs(taus -(td_error.detach() < 0).float()) * huber_l / 1.0\n",
    "\n",
    "            critic_loss = quantil_l.sum(dim=1).mean(dim=1).mean()\n",
    "            # Minimize the loss\n",
    "            self.critic_optimizer.zero_grad()\n",
    "            critic_loss.backward()\n",
    "            clip_grad_norm_(self.critic_local.parameters(), 1)\n",
    "            self.critic_optimizer.step()\n",
    "\n",
    "            # ---------------------------- update actor ---------------------------- #\n",
    "            # Compute actor loss\n",
    "            actions_pred = self.actor_local(states)\n",
    "            actor_loss = -self.critic_local.get_qvalues(states, actions_pred).mean()\n",
    "            # Minimize the loss\n",
    "            self.actor_optimizer.zero_grad()\n",
    "            actor_loss.backward()\n",
    "            self.actor_optimizer.step()\n",
    "\n",
    "            # ----------------------- update target networks ----------------------- #\n",
    "            self.soft_update(self.critic_local, self.critic_target)\n",
    "            self.soft_update(self.actor_local, self.actor_target)\n",
    "            \n",
    "            self.epsilon *= self.EPSILON_DECAY\n",
    "            self.noise.reset()\n",
    "\n",
    "            return critic_loss.detach().cpu().numpy(), actor_loss.detach().cpu().numpy()\n",
    "\n",
    "\n",
    "    \n",
    "def calculate_huber_loss(td_errors, k=1.0):\n",
    "    \"\"\"\n",
    "    Calculate huber loss element-wisely depending on kappa k.\n",
    "    \"\"\"\n",
    "    loss = torch.where(td_errors.abs() <= k, 0.5 * td_errors.pow(2), k * (td_errors.abs() - 0.5 * k))\n",
    "    assert loss.shape == (td_errors.shape[0], 32, 32), \"huber loss has wrong shape\"\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class DeepActor(nn.Module):\n",
    "    \"\"\"Actor (Policy) Model.\"\"\"\n",
    "\n",
    "    def __init__(self, state_size, action_size, seed, hidden_size=256):\n",
    "        \"\"\"Initialize parameters and build model.\n",
    "        Params\n",
    "        ======\n",
    "            state_size (int): Dimension of each state\n",
    "            action_size (int): Dimension of each action\n",
    "            seed (int): Random seed\n",
    "            fc1_units (int): Number of nodes in first hidden layer\n",
    "            fc2_units (int): Number of nodes in second hidden layer\n",
    "        \"\"\"\n",
    "        super(DeepActor, self).__init__()\n",
    "        self.seed = torch.manual_seed(seed)\n",
    "        self.input_size = hidden_size+state_size\n",
    "        self.fc1 = nn.Linear(state_size, hidden_size)\n",
    "        #self.batch_norm = nn.BatchNorm1d(fc1_units)\n",
    "        self.fc2 = nn.Linear(self.input_size, hidden_size)\n",
    "        self.fc3 = nn.Linear(self.input_size, hidden_size)\n",
    "        self.fc4 = nn.Linear(self.input_size, hidden_size)\n",
    "        self.fc5 = nn.Linear(hidden_size, action_size)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        self.fc1.weight.data.uniform_(*hidden_init(self.fc1))\n",
    "        self.fc2.weight.data.uniform_(*hidden_init(self.fc2))\n",
    "        self.fc3.weight.data.uniform_(*hidden_init(self.fc3)) \n",
    "        self.fc4.weight.data.uniform_(*hidden_init(self.fc4))\n",
    "        self.fc5.weight.data.uniform_(-3e-3, 3e-3)\n",
    "\n",
    "    def forward(self, state):\n",
    "        \"\"\"Build an actor (policy) network that maps states -> actions.\"\"\"\n",
    "        x = torch.relu(self.fc1(state))\n",
    "        x = torch.cat((x,state), dim=1)\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = torch.cat((x,state), dim=1)\n",
    "        x = torch.relu(self.fc3(x))\n",
    "        x = torch.cat((x,state), dim=1)\n",
    "        x = torch.relu(self.fc4(x))\n",
    "        return torch.tanh(self.fc5(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "class OUNoise:\n",
    "    \"\"\"Ornstein-Uhlenbeck process.\"\"\"\n",
    "\n",
    "    def __init__(self, size, seed, mu=0., theta=0.15, sigma=0.2):\n",
    "        \"\"\"Initialize parameters and noise process.\"\"\"\n",
    "        self.mu = mu * np.ones(size)\n",
    "        self.theta = theta\n",
    "        self.sigma = sigma\n",
    "        self.seed = random.seed(seed)\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"Reset the internal state (= noise) to mean (mu).\"\"\"\n",
    "        self.state = copy.copy(self.mu)\n",
    "\n",
    "    def sample(self):\n",
    "        \"\"\"Update internal state and return it as a noise sample.\"\"\"\n",
    "        x = self.state\n",
    "        dx = self.theta * (self.mu - x) + self.sigma * np.array([random.random() for i in range(len(x))])\n",
    "        self.state = x + dx\n",
    "        return self.state\n",
    "\n",
    "\n",
    "def hidden_init(layer):\n",
    "    fan_in = layer.weight.data.size()[0]\n",
    "    lim = 1. / np.sqrt(fan_in)\n",
    "    return (-lim, lim)\n",
    "\n",
    "def weight_init(layers):\n",
    "    for layer in layers:\n",
    "        torch.nn.init.kaiming_normal_(layer.weight, nonlinearity='relu')\n",
    "\n",
    "def weight_init_xavier(layers):\n",
    "    for layer in layers:\n",
    "        torch.nn.init.xavier_uniform_(layer.weight, gain=0.01)\n",
    "\n",
    "\n",
    "class IQN(nn.Module):\n",
    "    def __init__(self, state_size, action_size, layer_size, seed, N, dueling=False, device=\"cuda:0\"):\n",
    "        super(IQN, self).__init__()\n",
    "        self.seed = torch.manual_seed(seed)\n",
    "        self.input_shape = state_size\n",
    "        self.action_size = action_size\n",
    "        self.N = N  \n",
    "        self.n_cos = 64\n",
    "        self.layer_size = layer_size\n",
    "        self.pis = torch.FloatTensor([np.pi*i for i in range(1,self.n_cos+1)]).view(1,1,self.n_cos).to(device) # Starting from 0 as in the paper \n",
    "        self.dueling = dueling\n",
    "        self.device = device\n",
    "\n",
    "        # Network Architecture\n",
    "\n",
    "        self.head = nn.Linear(self.action_size+self.input_shape, layer_size) \n",
    "        self.cos_embedding = nn.Linear(self.n_cos, layer_size)\n",
    "        self.ff_1 = nn.Linear(layer_size, layer_size)\n",
    "        self.ff_2 = nn.Linear(layer_size, 1)    \n",
    "        #weight_init([self.head_1, self.ff_1])\n",
    "\n",
    "    def calc_input_layer(self):\n",
    "        x = torch.zeros(self.input_shape).unsqueeze(0)\n",
    "        x = self.head(x)\n",
    "        return x.flatten().shape[0]\n",
    "        \n",
    "    def calc_cos(self, batch_size, n_tau=32):\n",
    "        \"\"\"\n",
    "        Calculating the cosinus values depending on the number of tau samples\n",
    "        \"\"\"\n",
    "        taus = torch.rand(batch_size, n_tau).unsqueeze(-1).to(self.device) #(batch_size, n_tau, 1)  .to(self.device)\n",
    "        cos = torch.cos(taus*self.pis)\n",
    "\n",
    "        assert cos.shape == (batch_size,n_tau,self.n_cos), \"cos shape is incorrect\"\n",
    "        return cos, taus\n",
    "    \n",
    "    def forward(self, input, action, num_tau=32):\n",
    "        \"\"\"\n",
    "        Quantile Calculation depending on the number of tau\n",
    "        \n",
    "        Return:\n",
    "        quantiles [ shape of (batch_size, num_tau, action_size)]\n",
    "        taus [shape of ((batch_size, num_tau, 1))]\n",
    "        \n",
    "        \"\"\"\n",
    "        batch_size = input.shape[0]\n",
    "\n",
    "        print('input', input.shape)\n",
    "        x = torch.cat((input, action), dim=1)\n",
    "        x = torch.relu(self.head(x  ))\n",
    "        \n",
    "        cos, taus = self.calc_cos(batch_size, num_tau) # cos shape (batch, num_tau, layer_size)\n",
    "        cos = cos.view(batch_size*num_tau, self.n_cos)\n",
    "        cos_x = torch.relu(self.cos_embedding(cos)).view(batch_size, num_tau, self.layer_size) # (batch, n_tau, layer)\n",
    "        \n",
    "        # x has shape (batch, layer_size) for multiplication â€“> reshape to (batch, 1, layer)\n",
    "        x = (x.unsqueeze(1)*cos_x).view(batch_size*num_tau, self.layer_size)  #batch_size*num_tau, self.cos_layer_out\n",
    "        # Following reshape and transpose is done to bring the action in the same shape as batch*tau:\n",
    "        # first 32 entries are tau for each action -> thats why each action one needs to be repeated 32 times \n",
    "        # x = [[tau1   action = [[a1\n",
    "        #       tau1              a1   \n",
    "        #        ..               ..\n",
    "        #       tau2              a2\n",
    "        #       tau2              a2\n",
    "        #       ..]]              ..]]  \n",
    "        #action = action.repeat(num_tau,1).reshape(num_tau,batch_size*self.action_size).transpose(0,1).reshape(batch_size*num_tau,self.action_size)\n",
    "        #x = torch.cat((x,action),dim=1)\n",
    "        x = torch.relu(self.ff_1(x))\n",
    "\n",
    "        out = self.ff_2(x)\n",
    "        \n",
    "        return out.view(batch_size, num_tau, 1), taus\n",
    "    \n",
    "    def get_qvalues(self, inputs, action):\n",
    "        quantiles, _ = self.forward(inputs, action, self.N)\n",
    "        actions = quantiles.mean(dim=1)\n",
    "        return actions  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class PrioritizedReplay(object):\n",
    "    \"\"\"\n",
    "    Proportional Prioritization\n",
    "    \"\"\"\n",
    "    def __init__(self, capacity, batch_size, device, seed, gamma=0.99, n_step=1, parallel_env=1, alpha=0.6, beta_start = 0.4, beta_frames=100000):\n",
    "        self.alpha = alpha\n",
    "        self.beta_start = beta_start\n",
    "        self.beta_frames = beta_frames\n",
    "        self.device = device\n",
    "        self.frame = 1 #for beta calculation\n",
    "        self.batch_size = batch_size\n",
    "        self.capacity   = capacity\n",
    "        self.buffer     = deque(maxlen=capacity)\n",
    "        self.experience = namedtuple(\"Experience\", field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
    "        self.pos        = 0\n",
    "        self.priorities = deque(maxlen=capacity)\n",
    "        self.seed = np.random.seed(seed)\n",
    "        self.parallel_env = parallel_env\n",
    "        self.n_step = n_step\n",
    "        self.n_step_buffer = [deque(maxlen=self.n_step) for i in range(parallel_env)]\n",
    "        self.iter_ = 0\n",
    "        self.memory = deque(maxlen=capacity) \n",
    "        self.gamma = gamma\n",
    "\n",
    "    def calc_multistep_return(self,n_step_buffer):\n",
    "        Return = 0\n",
    "        for idx in range(self.n_step):\n",
    "            Return += self.gamma**idx * n_step_buffer[idx][2]\n",
    "        \n",
    "        return n_step_buffer[0][0], n_step_buffer[0][1], Return, n_step_buffer[-1][3], n_step_buffer[-1][4]\n",
    "\n",
    "    def beta_by_frame(self, frame_idx):\n",
    "        \"\"\"\n",
    "        Linearly increases beta from beta_start to 1 over time from 1 to beta_frames.\n",
    "        \n",
    "        3.4 ANNEALING THE BIAS (Paper: PER)\n",
    "        We therefore exploit the flexibility of annealing the amount of importance-sampling\n",
    "        correction over time, by defining a schedule on the exponent \n",
    "        that reaches 1 only at the end of learning. In practice, we linearly anneal from its initial value 0 to 1\n",
    "        \"\"\"\n",
    "        return min(1.0, self.beta_start + frame_idx * (1.0 - self.beta_start) / self.beta_frames)\n",
    "    \n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        if self.iter_ == self.parallel_env:\n",
    "            self.iter_ = 0\n",
    "        assert state.ndim == next_state.ndim\n",
    "        state      = np.expand_dims(state, 0)\n",
    "        next_state = np.expand_dims(next_state, 0)\n",
    "        action = torch.from_numpy(action).unsqueeze(0)\n",
    "\n",
    "        # n_step calc\n",
    "        self.n_step_buffer[self.iter_].append((state, action, reward, next_state, done))\n",
    "        if len(self.n_step_buffer[self.iter_]) == self.n_step:\n",
    "            state, action, reward, next_state, done = self.calc_multistep_return(self.n_step_buffer[self.iter_])\n",
    "            e = self.experience(state, action, reward, next_state, done)\n",
    "            self.memory.append(e)\n",
    "\n",
    "        max_prio = np.array(self.priorities, dtype=float).max() if self.buffer else 1.0 # gives max priority if buffer is not empty else 1\n",
    "        \n",
    "\n",
    "        self.buffer.append((state, action, reward, next_state, done))\n",
    "        self.priorities.append(max_prio)\n",
    "        self.iter_ += 1\n",
    "\n",
    "        \n",
    "    def sample(self):\n",
    "        N = len(self.buffer)\n",
    "        prios = np.array(self.priorities, dtype=float)\n",
    "        assert N == len(prios)\n",
    "            \n",
    "        # calc P = p^a/sum(p^a)\n",
    "        probs  = prios ** self.alpha\n",
    "        P = probs/probs.sum()\n",
    "        \n",
    "        #gets the indices depending on the probability p\n",
    "        indices = np.random.choice(N, self.batch_size, p=P) \n",
    "        samples = [self.buffer[idx] for idx in indices]\n",
    "        \n",
    "        beta = self.beta_by_frame(self.frame)\n",
    "        #print(beta)\n",
    "        self.frame+=1\n",
    "                \n",
    "        #Compute importance-sampling weight\n",
    "        weights  = (N * P[indices])**(-beta)\n",
    "        # normalize weights\n",
    "        weights /= weights.max() \n",
    "        weights  = np.array(weights, dtype=np.float32) \n",
    "        \n",
    "        \n",
    "        CurrentSequence = indices[0]\n",
    "        \n",
    "        if CurrentSequence < 8:\n",
    "            if len(self.memory) < 16:\n",
    "                indices = np.random.choice(N, 8, p=P)\n",
    "                #print('second indices:{}'.format(indices))\n",
    "                SequenceOfSampling=indices\n",
    "            else: \n",
    "                SequenceOfSampling = [CurrentSequence, CurrentSequence+1,CurrentSequence+2,CurrentSequence+3,CurrentSequence+4,CurrentSequence+5,CurrentSequence+6,CurrentSequence+7]\n",
    "        else:\n",
    "            SequenceOfSampling = [CurrentSequence-7,CurrentSequence-6,CurrentSequence-5,CurrentSequence-4,CurrentSequence-3,CurrentSequence-2,CurrentSequence-1,CurrentSequence]\n",
    "        \n",
    "        #print(SequenceOfSampling)\n",
    "        #print(len(self.memory))\n",
    "        experiences = [self.memory[SequenceOfSampling[0]],self.memory[SequenceOfSampling[1]],self.memory[SequenceOfSampling[2]],self.memory[SequenceOfSampling[3]],self.memory[SequenceOfSampling[4]],self.memory[SequenceOfSampling[5]],self.memory[SequenceOfSampling[6]],self.memory[SequenceOfSampling[7]]]\n",
    "        #print(experiences)\n",
    "        \n",
    "        \n",
    "        states, actions, rewards, next_states, dones = zip(*samples) \n",
    "\n",
    "        states      = torch.FloatTensor(np.float32(np.concatenate(states))).to(self.device)\n",
    "        next_states = torch.FloatTensor(np.float32(np.concatenate(next_states))).to(self.device)\n",
    "        actions     = torch.cat(actions).to(self.device)\n",
    "        rewards     = torch.FloatTensor(rewards).to(self.device).unsqueeze(1) \n",
    "        dones       = torch.FloatTensor(dones).to(self.device).unsqueeze(1)\n",
    "        weights    = torch.FloatTensor(weights).unsqueeze(1)\n",
    "        #print(\"s\",states.shape)\n",
    "        #print(\"ns\", next_states.shape)\n",
    "        #print(\"a\", actions.shape)\n",
    "        \n",
    "        #print(\"r:{}\".format(rewards))\n",
    "        #print(\"d\", dones.shape)\n",
    "        #print(\"w\", weights.shape)\n",
    "        \n",
    "        return states, actions, rewards, next_states, dones, indices, weights\n",
    "    \n",
    "\n",
    "    def update_priorities(self, batch_indices, batch_priorities):\n",
    "        for idx, prio in zip(batch_indices, batch_priorities):\n",
    "            self.priorities[idx] = prio \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from IPython import display\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from gym.utils import seeding\n",
    "import gym\n",
    "import os\n",
    "from gym import spaces\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "\n",
    "# shares normalization factor\n",
    "# 100 shares per trade\n",
    "HMAX_NORMALIZE = 10\n",
    "# initial amount of money we have in our account\n",
    "INITIAL_ACCOUNT_BALANCE= 1000\n",
    "# total number of stocks in our portfolio\n",
    "STOCK_DIM = 3\n",
    "# transaction fee: 1/1000 reasonable percentage\n",
    "TRANSACTION_FEE_PERCENT = 0.001\n",
    "REWARD_SCALING = 1e-4\n",
    "\n",
    "class StockEnvTrain(gym.Env):\n",
    "    \"\"\"A stock trading environment for OpenAI gym\"\"\"\n",
    "    metadata = {'render.modes': ['human']}\n",
    "\n",
    "    def __init__(self, df,day = 0):\n",
    "        #super(StockEnv, self).__init__()\n",
    "        #money = 10 , scope = 1\n",
    "        self.day = day\n",
    "        self.df = df\n",
    "        self.agent_stock_iteration_index = 0\n",
    "        self.penalty = 0\n",
    "\n",
    "        # action_space normalization and shape is STOCK_DIM\n",
    "        self.action_space = spaces.Box(low = -1, high = 1,shape = (STOCK_DIM,)) \n",
    "        # Shape = 181: [Current Balance]+[prices 1-30]+[owned shares 1-30] \n",
    "        # +[macd 1-30]+ [rsi 1-30] + [cci 1-30] + [adx 1-30]\n",
    "        self.observation_space = spaces.Box(low=0, high=np.inf, shape = (7,))\n",
    "        # load data from a pandas dataframe\n",
    "        #print('df: {}'.format(self.df))\n",
    "        #print('day: {}'.format(self.day))\n",
    "        self.data = self.df.loc[self.day,:]\n",
    "        \n",
    "        self.terminal = False\n",
    "\n",
    "        # initalize state\n",
    "        self.state = [INITIAL_ACCOUNT_BALANCE] + \\\n",
    "                      self.data.Close.values.tolist() + \\\n",
    "                      [0]*STOCK_DIM \n",
    "        # initialize reward\n",
    "        self.reward = 0\n",
    "        self.cost = 0\n",
    "        # memorize all the total balance change\n",
    "        self.asset_memory = [INITIAL_ACCOUNT_BALANCE]\n",
    "        self.rewards_memory = []\n",
    "        self.final_asset_value = 0\n",
    "        self.trades = 0\n",
    "        self.previous_trades = 0 \n",
    "        #self.reset()\n",
    "        self._seed()\n",
    "\n",
    "\n",
    "    def _sell_stock(self, index, action):\n",
    "        action = action\n",
    "        #print('index:{}'.format(index))\n",
    "        #print('selling:{}'.format(action))\n",
    "        # perform sell action based on the sign of the action\n",
    "        if self.state[index+STOCK_DIM+1] > 0:\n",
    "            #update balance\n",
    "            self.state[0] += \\\n",
    "            self.state[index+1]*min(abs(action),self.state[index+STOCK_DIM+1]) * \\\n",
    "             (1- TRANSACTION_FEE_PERCENT)\n",
    "\n",
    "            self.state[index+STOCK_DIM+1] -= min(abs(action), self.state[index+STOCK_DIM+1])\n",
    "            self.cost +=self.state[index+1]*min(abs(action),self.state[index+STOCK_DIM+1]) * \\\n",
    "             TRANSACTION_FEE_PERCENT\n",
    "            self.trades+=1\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "    \n",
    "    def _buy_stock(self, index, action):\n",
    "        #print('index:{}'.format(index))\n",
    "        #print('buying:{}'.format(action))\n",
    "        action = action\n",
    "        # perform buy action based on the sign of the action\n",
    "        available_amount = self.state[0] // self.state[index+1]\n",
    "        # print('available_amount:{}'.format(available_amount))\n",
    "\n",
    "        #update balance\n",
    "        self.state[0] -= self.state[index+1]*min(available_amount, action)* \\\n",
    "                          (1+ TRANSACTION_FEE_PERCENT)\n",
    "\n",
    "        self.state[index+STOCK_DIM+1] += min(available_amount, action)\n",
    "\n",
    "        self.cost+=self.state[index+1]*min(available_amount, action)* \\\n",
    "                          TRANSACTION_FEE_PERCENT\n",
    "        \n",
    "        if available_amount>0:\n",
    "            self.trades+=1\n",
    "    \n",
    "        \n",
    "    def step(self, actions):\n",
    "        # print(self.day)\n",
    "        #print(self.day)\n",
    "        self.terminal = self.day >= len(self.df.index.unique())-1\n",
    "        #print(actions)\n",
    "        self.actions = actions\n",
    "        if self.terminal:\n",
    "            print(\"Finished\")\n",
    "            print(self.state)\n",
    "            end_total_asset = self.state[0]+ \\\n",
    "            sum(np.array(self.state[1:(STOCK_DIM+1)])*np.array(self.state[(STOCK_DIM+1):(STOCK_DIM*2+1)]))\n",
    "\n",
    "            print(\"end_total_asset:{}\".format(end_total_asset))\n",
    "            df_total_value = pd.DataFrame(self.asset_memory)\n",
    "            #df_total_value.to_csv('results/account_value_train.csv')\n",
    "            #print(\"total_reward:{}\".format(self.state[0]+sum(np.array(self.state[1:(STOCK_DIM+1)])*np.array(self.state[(STOCK_DIM+1):61]))- INITIAL_ACCOUNT_BALANCE ))\n",
    "            #print(\"total_cost: \", self.cost)\n",
    "            #print(\"total_trades: \", self.trades)\n",
    "            df_total_value.columns = ['account_value']\n",
    "            df_total_value['daily_return']=df_total_value.pct_change(1)\n",
    "            sharpe = (252**0.5)*df_total_value['daily_return'].mean()/ \\\n",
    "                  df_total_value['daily_return'].std()\n",
    "            print(\"Sharpe: \",sharpe)\n",
    "            #print(\"=================================\")\n",
    "            df_rewards = pd.DataFrame(self.rewards_memory)\n",
    "            #df_rewards.to_csv('results/account_rewards_train.csv')\n",
    "\n",
    "            # print('total asset: {}'.format(self.state[0]+ sum(np.array(self.state[1:29])*np.array(self.state[29:]))))\n",
    "            #with open('obs.pkl', 'wb') as f:  \n",
    "            #    pickle.dump(self.state, f)\n",
    "\n",
    "            return self.state, self.reward, self.terminal,{}\n",
    "\n",
    "        else:\n",
    "            # print(np.array(self.state[1:29]))\n",
    "            #print(\"The actions is: {}\".format(self.actions))\n",
    "\n",
    "            #action = np.array([4,4,5])\n",
    "            #actions = np.array([4,0,0,0,0,0,0,0,4,0,4,0,-3,0,0,0,0,0,0,0,4,0,0,0,0,0,0,0,0,0])\n",
    "\n",
    "            #actions = self.actions * HMAX_NORMALIZE #WHY??\n",
    "            #print(\"actions-index------:{}\".format(actions))\n",
    "            #actions = (actions.astype(int))\n",
    "            print('here:',actions)\n",
    "            actions = actions * HMAX_NORMALIZE\n",
    "            #print('all-actions:{}'.format(actions))\n",
    "\n",
    "            begin_total_asset = self.state[0]+ \\\n",
    "            sum(np.array(self.state[1:(STOCK_DIM+1)])*np.array(self.state[(STOCK_DIM+1):(STOCK_DIM*2+1)]))\n",
    "            #print(\"begin_total_asset:{}\".format(begin_total_asset))\n",
    "\n",
    "            argsort_actions = np.argsort(actions) #TODO: this may not be touched.\n",
    "            #print(\"The actions is: {}\".format(actions))\n",
    "            \n",
    "            \n",
    "\n",
    "            sell_index = argsort_actions[:np.where(actions < 0.5 )[0].shape[0]]\n",
    "            buy_index = argsort_actions[::-1][:np.where(actions > -0.5 )[0].shape[0]]\n",
    "            \n",
    "            \n",
    "            for index in sell_index:\n",
    "                # print('take sell action'.format(actions[index]))\n",
    "                self._sell_stock(index, actions[index])\n",
    "\n",
    "            for index in buy_index:\n",
    "                # print('take buy action: {}'.format(actions[index]))\n",
    "                self._buy_stock(index, actions[index])\n",
    "                \n",
    "            \n",
    "            #print(\"self.day:{}\".format(self.day))\n",
    "            #--print('trades:{}'.format(self.trades))\n",
    "            \n",
    "            \n",
    "            \n",
    "            if self.previous_trades == self.trades:\n",
    "                self.penalty = 10\n",
    "                #self.reward = -1\n",
    "            else: \n",
    "                self.penalty = 0\n",
    "                #self.reward = self.trades\n",
    "                \n",
    "            self.previous_trades = float(self.trades)\n",
    "                \n",
    "            #load next state\n",
    "            # print(\"stock_shares:{}\".format(self.state[29:]))\n",
    "            self.state =  [self.state[0]] + \\\n",
    "                self.data.Close.values.tolist() + \\\n",
    "                list(self.state[(STOCK_DIM+1):(STOCK_DIM*2+1)]) \n",
    "\n",
    "            end_total_asset = self.state[0]+ \\\n",
    "            sum(np.array(self.state[1:(STOCK_DIM+1)])*np.array(self.state[(STOCK_DIM+1):(STOCK_DIM*2+1)]))\n",
    "            self.asset_memory.append(end_total_asset)\n",
    "            #print(\"end_total_asset:{}\".format(end_total_asset))\n",
    "            \n",
    "            \n",
    "\n",
    "            self.reward = end_total_asset - begin_total_asset - self.penalty\n",
    "            \n",
    "            print(\"trades:{}\".format(self.trades))\n",
    "            print('previous:{}'.format(self.previous_trades))\n",
    "            print(\"penalty:{}\".format(self.penalty))\n",
    "            print(\"step_reward:{}\".format(self.reward))\n",
    "        \n",
    "            self.rewards_memory.append(self.reward)\n",
    "            #self.reward = self.reward*REWARD_SCALING\n",
    "            print(\"step_reward:{}\".format(self.reward))\n",
    "            \n",
    "            self.day += 1\n",
    "            self.data = self.df.loc[self.day,:]\n",
    "\n",
    "        return self.state, self.reward, self.terminal, {}\n",
    "\n",
    "    def reset(self):\n",
    "        self.final_asset_value = 0\n",
    "        self.trades = 0\n",
    "        self.previous_trades = 0\n",
    "        self.penalty = 0 \n",
    "        self.asset_memory = [INITIAL_ACCOUNT_BALANCE]\n",
    "        self.day = 0\n",
    "        self.data = self.df.loc[self.day,:]\n",
    "        self.cost = 0\n",
    "        self.trades = 0\n",
    "        self.terminal = False \n",
    "        self.rewards_memory = []\n",
    "        self.agent_stock_iteration_index = 0\n",
    "        #initiate state\n",
    "        self.state = [INITIAL_ACCOUNT_BALANCE] + \\\n",
    "                      self.data.Close.values.tolist() + \\\n",
    "                      [0]*STOCK_DIM \n",
    "        # iteration += 1 \n",
    "        #print(\"[0]*STOCK_DIM:{}\".format([0]*STOCK_DIM))\n",
    "        #print(\"self.state:{}\".format(len(self.state)))\n",
    "        #print(np.array(self.state))\n",
    "        return np.array(self.state)\n",
    "    \n",
    "    def render(self, mode='human'):\n",
    "        return self.state\n",
    "\n",
    "    def _seed(self, seed=None):\n",
    "        self.np_random, seed = seeding.np_random(seed)\n",
    "        return [seed]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "def run(env_train,frames=100000, eps_fixed=False, eps_frames=1e6, min_eps=0.001):\n",
    "    \"\"\"Deep Q-Learning.\n",
    "    \n",
    "    Params\n",
    "    ======\n",
    "        n_episodes (int): maximum number of training episodes\n",
    "        max_t (int): maximum number of timesteps per episode\n",
    "        eps_start (float): starting value of epsilon, for epsilon-greedy action selection\n",
    "        eps_end (float): minimum value of epsilon\n",
    "        eps_decay (float): multiplicative factor (per epaisode) for decreasing epsilon\n",
    "    \"\"\"\n",
    "    scores = []                        # list containing scores from each episode\n",
    "    scores_window = deque(maxlen=100)  # last 100 scores\n",
    "    output_history = []\n",
    "    frame = 0\n",
    "    if eps_fixed:\n",
    "        eps = 0\n",
    "    else:\n",
    "        eps = 1\n",
    "    eps_start = 1\n",
    "    i_episode = 1\n",
    "    state = env_train.reset()\n",
    "   # state = state[0,:]\n",
    "    #print(\"state space:{}\".format(state[0,:].shape))\n",
    "    score = 0 \n",
    "    \n",
    "    action_high = env_train.action_space.high[0]\n",
    "    action_low = env_train.action_space.low[0]\n",
    "    state_size = env_train.observation_space.shape[0]\n",
    "    action_size = env_train.action_space.shape[0]\n",
    "    \n",
    "    for frame in range(1, frames+1):\n",
    "        \n",
    "        \n",
    "        \n",
    "        if frame  == 0:\n",
    "            # inital state\n",
    "\n",
    "            initial = True\n",
    "        else:\n",
    "            # previous state\n",
    "            initial = False\n",
    "        #print('initial state:{}'.format(initial))\n",
    "        \n",
    "        if eps_fixed == False:\n",
    "            if frame < eps_frames:\n",
    "                eps = max(eps_start - (frame*(1/eps_frames)), min_eps)\n",
    "            else:\n",
    "                eps = max(min_eps - min_eps*((frame-eps_frames)/(frames-eps_frames)), 0.001)\n",
    "        \n",
    "\n",
    "        action = agent.act(state) #TODO: getting one dimension back.\n",
    "        #\n",
    "        action = np.array([action])\n",
    "\n",
    "        #--print('My Action_V: {}'.format(action_v))\n",
    "        \n",
    "        next_state, reward, done, info = env_train.step(action) #TODO: Wants a list of actions of size a\n",
    "\n",
    "        \n",
    "        #print(\"env_trainNext State: {}\".format(next_state.shape))\n",
    "        \n",
    "        for s, a, r, ns, d in zip(state, action, reward, next_state, done):\n",
    "            agent.step(s, a, r, ns, d, frame, writer)\n",
    "\n",
    "        print('agent seed', agent.seed)\n",
    "        state = next_state\n",
    "        score += reward\n",
    "        # linear annealing to the min epsilon value until eps_frames and from there slowly decease epsilon to 0 until the end of training\n",
    "\n",
    "                \n",
    "                \n",
    "                \n",
    "        # evaluation runs\n",
    "        if frame % 1 == 0:\n",
    "            print('My Action: {}'.format(action))\n",
    "            print(\"state: {}\".format(state))\n",
    "            print(\"score: {}\".format(score))\n",
    "            print(\"state: {}\".format(state))\n",
    "            print(\"action:{}, Number:{}\".format(action,frame))\n",
    "            print(\"-------------------------\")\n",
    "        \n",
    "        if done:\n",
    "            \n",
    "            scores_window.append(score)       # save most recent score\n",
    "            scores.append(score)              # save most recent score\n",
    "            writer.add_scalar(\"Average100\", np.mean(scores_window), frame*worker)\n",
    "            \n",
    "            print('\\rEpisode {}\\tFrame {} \\tAverage100 Score: {:.2f}'.format(i_episode*worker, frame*worker, np.mean(scores_window)), end=\"\")\n",
    "            #if i_episode % 100 == 0:\n",
    "            #    print('\\rEpisode {}\\tFrame \\tReward: {}\\tAverage100 Score: {:.2f}'.format(i_episode*worker, frame*worker, round(eval_reward,2), np.mean(scores_window)), end=\"\", flush=True)\n",
    "            i_episode +=1 \n",
    "            state = env_train.reset()\n",
    "            score = 0\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "1.0\n",
      "-1.0\n",
      "run seed 4\n",
      "Using:  cpu\n",
      "seed agent:  None\n",
      "Actor: \n",
      " Actor(\n",
      "  (fc1): Linear(in_features=7, out_features=400, bias=True)\n",
      "  (batch_norm): BatchNorm1d(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc2): Linear(in_features=400, out_features=400, bias=True)\n",
      "  (fc3): Linear(in_features=400, out_features=3, bias=True)\n",
      ")\n",
      "\n",
      "Critic: \n",
      " Critic(\n",
      "  (fcs1): Linear(in_features=7, out_features=256, bias=True)\n",
      "  (batch_norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc2): Linear(in_features=259, out_features=256, bias=True)\n",
      "  (fc3): Linear(in_features=256, out_features=1, bias=True)\n",
      ")\n",
      "state.shape[0]:torch.Size([1, 1, 7])\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "shape: torch.Size([1, 1, 7])",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[0;32mIn [280]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     56\u001b[0m     run(eval_env,frames\u001b[38;5;241m=\u001b[39mframes, eps_fixed\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, eps_frames\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000\u001b[39m, min_eps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.025\u001b[39m)\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:    \n\u001b[0;32m---> 58\u001b[0m     \u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43meval_env\u001b[49m\u001b[43m,\u001b[49m\u001b[43mframes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mframes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meps_fixed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meps_frames\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmin_eps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.025\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     60\u001b[0m eval_env\u001b[38;5;241m.\u001b[39mclose()\n\u001b[1;32m     61\u001b[0m \u001b[38;5;66;03m# save trained model \u001b[39;00m\n",
      "Input \u001b[0;32mIn [279]\u001b[0m, in \u001b[0;36mrun\u001b[0;34m(env_train, frames, eps_fixed, eps_frames, min_eps)\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m         eps \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(min_eps \u001b[38;5;241m-\u001b[39m min_eps\u001b[38;5;241m*\u001b[39m((frame\u001b[38;5;241m-\u001b[39meps_frames)\u001b[38;5;241m/\u001b[39m(frames\u001b[38;5;241m-\u001b[39meps_frames)), \u001b[38;5;241m0.001\u001b[39m)\n\u001b[0;32m---> 54\u001b[0m action \u001b[38;5;241m=\u001b[39m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mact\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m#TODO: getting one dimension back.\u001b[39;00m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m     56\u001b[0m action \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([action])\n",
      "Input \u001b[0;32mIn [274]\u001b[0m, in \u001b[0;36mAgent.act\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstate.shape[0]:\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(state\u001b[38;5;241m.\u001b[39mshape))\n\u001b[1;32m    125\u001b[0m \u001b[38;5;66;03m#print('self.state_size:{}'.format(self.state_size))\u001b[39;00m\n\u001b[0;32m--> 128\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m state\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;241m==\u001b[39m (state\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m],\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate_size), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshape: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(state\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m    129\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactor_local\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m    130\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n",
      "\u001b[0;31mAssertionError\u001b[0m: shape: torch.Size([1, 1, 7])"
     ]
    }
   ],
   "source": [
    "\n",
    "if __name__ == \"__main__\":\n",
    "    seed = 4\n",
    "    frames = 30000\n",
    "    worker = 1\n",
    "    GAMMA = 0.99\n",
    "    TAU = 1e-3\n",
    "    HIDDEN_SIZE = 400\n",
    "    BUFFER_SIZE = int(1e6)\n",
    "    BATCH_SIZE = 32\n",
    "    LR_ACTOR = 3e-3         # learning rate of the actor \n",
    "    \n",
    "    LR_CRITIC = 3e-3     # learning rate of the critic\n",
    "    saved_model = None #'D4PG.pth'\n",
    "    D2RL = 0\n",
    "\n",
    "    writer = SummaryWriter(\"\")\n",
    "    \n",
    "    preprocessed_path = \"done_3stocks.csv\"\n",
    "    if os.path.exists(preprocessed_path):\n",
    "        data = pd.read_csv(preprocessed_path, index_col=0)\n",
    "\n",
    "    unique_trade_date = data[(data.datadate > 20151001)&(data.datadate <= 20200707)].datadate.unique()\n",
    "    #print(unique_trade_date)\n",
    "\n",
    "    \n",
    "    train = data_split(data, start=20100101, end=20100202)\n",
    "    \n",
    "    eval_env = DummyVecEnv([lambda: StockEnvTrain(train)])\n",
    "    \n",
    "    eval_env.seed(seed+1)\n",
    "    \n",
    "    torch.manual_seed(seed)\n",
    "    \n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    \n",
    "    \n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"Using device: {}\".format(device))\n",
    "    \n",
    "    action_high = eval_env.action_space.high[0]\n",
    "    print(action_high)\n",
    "    action_low = eval_env.action_space.low[0]\n",
    "    print(action_low)\n",
    "    state_size = eval_env.observation_space.shape[0]\n",
    "    action_size = eval_env.action_space.shape[0]\n",
    "    print('run seed', seed)\n",
    "    agent = Agent(state_size=state_size, action_size=action_size, n_step=1, per=0, munchausen=0,distributional=1,\n",
    "                 D2RL=D2RL, random_seed=seed, hidden_size=HIDDEN_SIZE, BATCH_SIZE=BATCH_SIZE, BUFFER_SIZE=BUFFER_SIZE, GAMMA=GAMMA,\n",
    "                 LR_ACTOR=LR_ACTOR, LR_CRITIC=LR_CRITIC, TAU=TAU, LEARN_EVERY=1, LEARN_NUMBER=1, device=device, frames=frames, worker=1) \n",
    "    \n",
    "    t0 = time.time()\n",
    "    if saved_model != None:\n",
    "        agent.actor_local.load_state_dict(torch.load(saved_model))\n",
    "        #evaluate(1)\n",
    "        run(eval_env,frames=frames, eps_fixed=False, eps_frames=1000, min_eps=0.025)\n",
    "    else:    \n",
    "        run(eval_env,frames=frames, eps_fixed=False, eps_frames=1000, min_eps=0.025)\n",
    "\n",
    "    eval_env.close()\n",
    "    # save trained model \n",
    "    torch.save(agent.actor_local.state_dict(), 'D4PG_2.pth')\n",
    "    #writer.export_scalars_to_json('./scalars.json')\n",
    "    # save parameter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Evaluation part \n",
    "1) Reading JSON and PTH\n",
    "2) Setting up trading env\n",
    "3) Running test\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "def evaluate(eval_runs=5):\n",
    "    \"\"\"\n",
    "    Makes an evaluation run \n",
    "    \n",
    "    \"\"\"\n",
    "    seed = 4\n",
    "    frames = 10000\n",
    "    worker = 1\n",
    "    GAMMA = 0.99\n",
    "    TAU = 1e-3\n",
    "    HIDDEN_SIZE = 256\n",
    "    BUFFER_SIZE = int(1e6)\n",
    "    BATCH_SIZE = 256\n",
    "    LR_ACTOR = 3e-4         # learning rate of the actor \n",
    "    LR_CRITIC = 3e-4     # learning rate of the critic\n",
    "    saved_model = 'D4PG.pth'\n",
    "    D2RL = 0\n",
    "\n",
    "    writer = SummaryWriter(\"\")\n",
    "    \n",
    "    preprocessed_path = \"done_3stocks.csv\"\n",
    "    if os.path.exists(preprocessed_path):\n",
    "        data = pd.read_csv(preprocessed_path, index_col=0)\n",
    "\n",
    "    unique_trade_date = data[(data.datadate > 20151001)&(data.datadate <= 20200707)].datadate.unique()\n",
    "    #print(unique_trade_date)\n",
    "\n",
    "    \n",
    "    train = data_split(data, start=20150101, end=20180101)\n",
    "    \n",
    "    env_train = DummyVecEnv([lambda: StockEnvTrain(train)])\n",
    "    \n",
    "    env_train.seed(seed+1)\n",
    "    \n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"Using device: {}\".format(device))\n",
    "    \n",
    "    scores = []                        # list containing scores from each episode\n",
    "    scores_window = deque(maxlen=100)  # last 100 scores\n",
    "    output_history = []\n",
    "    frame = 0\n",
    "\n",
    "    eps = 1\n",
    "    eps_start = 1\n",
    "    i_episode = 1\n",
    "    state = env_train.reset()\n",
    "   # state = state[0,:]\n",
    "    #print(\"state space:{}\".format(state[0,:].shape))\n",
    "    score = 0 \n",
    "    \n",
    "    action_high = env_train.action_space.high[0]\n",
    "    action_low = env_train.action_space.low[0]\n",
    "    state_size = env_train.observation_space.shape[0]\n",
    "    action_size = env_train.action_space.shape[0]\n",
    "\n",
    "    for _ in range(eval_runs):\n",
    "        state = env_train.reset()\n",
    "\n",
    "        rewards = 0\n",
    "        while True:\n",
    "            action = agent.act(state) #TODO: getting one dimension back.\n",
    "            print(action)\n",
    "            print(state)\n",
    "            action_v = np.clip(action, action_low, action_high)\n",
    "\n",
    "            next_state, reward, done, info = env_train.step(action_v) #TODO: Wants a list of actions of size a\n",
    "            state = next_state\n",
    "            rewards += reward\n",
    "            if done:\n",
    "                print(\"Episode Rewards: {}\".format(rewards))\n",
    "                break\n",
    "\n",
    "class dotdict(dict):\n",
    "    def __getattr__(self, name):\n",
    "        return self[name]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0001\n"
     ]
    }
   ],
   "source": [
    "print(1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.05056170714293955"
      ]
     },
     "execution_count": 255,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.normal(0, scale=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "a = np.random.seed(4)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"Pendulum-v0\")\n",
    "state_size = env.observation_space.shape[0]\n",
    "action_size = env.action_space.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using:  cpu\n",
      "seed agent:  None\n",
      "Actor: \n",
      " Actor(\n",
      "  (fc1): Linear(in_features=3, out_features=400, bias=True)\n",
      "  (batch_norm): BatchNorm1d(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc2): Linear(in_features=400, out_features=400, bias=True)\n",
      "  (fc3): Linear(in_features=400, out_features=1, bias=True)\n",
      ")\n",
      "\n",
      "Critic: \n",
      " Critic(\n",
      "  (fcs1): Linear(in_features=3, out_features=256, bias=True)\n",
      "  (batch_norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc2): Linear(in_features=257, out_features=256, bias=True)\n",
      "  (fc3): Linear(in_features=256, out_features=1, bias=True)\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/h3/d5kvsb7d4rl8jq8scv_520vw0000gn/T/ipykernel_1882/1294247666.py:58: DeprecationWarning: setting an array element with a sequence. This was supported in some cases where the elements are arrays with a single element. For example `np.array([1, np.array([2])], dtype=int)`. In the future this will raise the same ValueError as `np.array([1, [2]], dtype=int)`.\n",
      "  max_prio = np.array(self.priorities, dtype=float).max() if self.buffer else 1.0 # gives max priority if buffer is not empty else 1\n",
      "/var/folders/h3/d5kvsb7d4rl8jq8scv_520vw0000gn/T/ipykernel_1882/1294247666.py:68: DeprecationWarning: setting an array element with a sequence. This was supported in some cases where the elements are arrays with a single element. For example `np.array([1, np.array([2])], dtype=int)`. In the future this will raise the same ValueError as `np.array([1, [2]], dtype=int)`.\n",
      "  prios = np.array(self.priorities, dtype=float)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 10  Min_reward: -1369.81  Max_reward: -1369.81  Mean_reward: -1369.81  Average100 Score: -1363.84\n",
      "Episode 20  Min_reward: -1359.51  Max_reward: -1359.51  Mean_reward: -1359.51  Average100 Score: -1336.87\n",
      "Episode 30  Min_reward: -1390.69  Max_reward: -1390.69  Mean_reward: -1390.69  Average100 Score: -1365.07\n",
      "Episode 40  Min_reward: -1332.02  Max_reward: -1332.02  Mean_reward: -1332.02  Average100 Score: -1376.11\n",
      "Episode 50  Min_reward: -1714.50  Max_reward: -1714.50  Mean_reward: -1714.50  Average100 Score: -1354.81\n",
      "Episode 60  Min_reward: -1687.15  Max_reward: -1687.15  Mean_reward: -1687.15  Average100 Score: -1346.71\n",
      "Episode 70  Min_reward: -1167.88  Max_reward: -1167.88  Mean_reward: -1167.88  Average100 Score: -1358.94\n",
      "Episode 80  Min_reward: -950.33  Max_reward: -950.33  Mean_reward: -950.33  Average100 Score: -1352.70.80\n",
      "Episode 90  Min_reward: -720.12  Max_reward: -720.12  Mean_reward: -720.12  Average100 Score: -1334.49.39\n",
      "Episode 98  Min_reward: -1366.84  Max_reward: -1366.84  Mean_reward: -1366.84  Average100 Score: -1332.62"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [267]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m scores, minmax_scores, average_100_scores\n\u001b[1;32m     48\u001b[0m agent \u001b[38;5;241m=\u001b[39m Agent(state_size\u001b[38;5;241m=\u001b[39mstate_size, action_size\u001b[38;5;241m=\u001b[39maction_size, n_step\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, per\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, munchausen\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m,distributional\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m     49\u001b[0m                  D2RL\u001b[38;5;241m=\u001b[39mD2RL, random_seed\u001b[38;5;241m=\u001b[39mseed, hidden_size\u001b[38;5;241m=\u001b[39mHIDDEN_SIZE, BATCH_SIZE\u001b[38;5;241m=\u001b[39mBATCH_SIZE, BUFFER_SIZE\u001b[38;5;241m=\u001b[39mBUFFER_SIZE, GAMMA\u001b[38;5;241m=\u001b[39mGAMMA,\n\u001b[1;32m     50\u001b[0m                  LR_ACTOR\u001b[38;5;241m=\u001b[39mLR_ACTOR, LR_CRITIC\u001b[38;5;241m=\u001b[39mLR_CRITIC, TAU\u001b[38;5;241m=\u001b[39mTAU, LEARN_EVERY\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, LEARN_NUMBER\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, device\u001b[38;5;241m=\u001b[39mdevice, frames\u001b[38;5;241m=\u001b[39mframes, worker\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 53\u001b[0m scores, minmax, average_100 \u001b[38;5;241m=\u001b[39m \u001b[43mddpg\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [267]\u001b[0m, in \u001b[0;36mddpg\u001b[0;34m(n_episodes, max_t, print_every)\u001b[0m\n\u001b[1;32m     17\u001b[0m action \u001b[38;5;241m=\u001b[39m agent\u001b[38;5;241m.\u001b[39mact(state)\n\u001b[1;32m     19\u001b[0m next_state, reward, done, _ \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(action) \n\u001b[0;32m---> 20\u001b[0m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreward\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnext_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdone\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtime_stamp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwriter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m state \u001b[38;5;241m=\u001b[39m next_state\n\u001b[1;32m     24\u001b[0m score \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m reward\n",
      "Input \u001b[0;32mIn [266]\u001b[0m, in \u001b[0;36mAgent.step\u001b[0;34m(self, state, action, reward, next_state, done, timestamp, writer)\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;124;03m\"\"\"Save experience in replay memory, and use random sample from buffer to learn.\"\"\"\u001b[39;00m\n\u001b[1;32m    101\u001b[0m \u001b[38;5;66;03m# Save experience / reward\u001b[39;00m\n\u001b[1;32m    102\u001b[0m \n\u001b[1;32m    103\u001b[0m \u001b[38;5;66;03m#rint('agent step reward:{}'.format(reward))\u001b[39;00m\n\u001b[0;32m--> 104\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmemory\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreward\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnext_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdone\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    108\u001b[0m \u001b[38;5;66;03m# Learn, if enough samples are available in memory\u001b[39;00m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmemory) \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mBATCH_SIZE \u001b[38;5;129;01mand\u001b[39;00m timestamp \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mLEARN_EVERY \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "Input \u001b[0;32mIn [262]\u001b[0m, in \u001b[0;36mPrioritizedReplay.add\u001b[0;34m(self, state, action, reward, next_state, done)\u001b[0m\n\u001b[1;32m     55\u001b[0m     e \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperience(state, action, reward, next_state, done)\n\u001b[1;32m     56\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmemory\u001b[38;5;241m.\u001b[39mappend(e)\n\u001b[0;32m---> 58\u001b[0m max_prio \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpriorities\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mfloat\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mmax() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuffer \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m1.0\u001b[39m \u001b[38;5;66;03m# gives max priority if buffer is not empty else 1\u001b[39;00m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuffer\u001b[38;5;241m.\u001b[39mappend((state, action, reward, next_state, done))\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpriorities\u001b[38;5;241m.\u001b[39mappend(max_prio)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "def ddpg(n_episodes=200, max_t=1000, print_every=10):\n",
    "    writer = SummaryWriter(\"\")\n",
    "    scores_deque = deque(maxlen=100)\n",
    "    scores = []\n",
    "    minmax_scores = []\n",
    "    average_100_scores = []\n",
    "    time_stamp = 0\n",
    "    for i_episode in range(1, n_episodes+1):\n",
    "\n",
    "        state = env.reset()                     \n",
    "        agent.reset()\n",
    "        score = 0\n",
    "        for t in range(max_t):\n",
    "            \n",
    "            action = agent.act(state)\n",
    "            \n",
    "            next_state, reward, done, _ = env.step(action) \n",
    "            agent.step(state, action, reward, next_state, done, time_stamp, writer)\n",
    "            \n",
    "            \n",
    "            state = next_state\n",
    "            score += reward\n",
    "            time_stamp += 1\n",
    "            \n",
    "            if done:\n",
    "                break \n",
    "        \n",
    "        scores_deque.append(np.mean(score))\n",
    "        scores.append(np.mean(score))\n",
    "        minmax_scores.append((np.min(score),np.max(score)))\n",
    "        average_100_scores.append(np.mean(scores_deque))\n",
    "        \n",
    "        print('\\rEpisode {}  Min_reward: {:.2f}  Max_reward: {:.2f}  Mean_reward: {:.2f}  Average100 Score: {:.2f}'.format(i_episode,np.min(score),np.max(score),np.mean(score), np.mean(scores_deque)), end=\"\")\n",
    "        if i_episode % 25 == 0:\n",
    "            torch.save(agent.actor_local.state_dict(), \"checkpoint_actor\"+str(i_episode)+\".pth\")\n",
    "            torch.save(agent.critic_local.state_dict(), \"checkpoint_critic\"+str(i_episode)+\".pth\")\n",
    "        if i_episode % print_every == 0:\n",
    "            print('\\rEpisode {}  Min_reward: {:.2f}  Max_reward: {:.2f}  Mean_reward: {:.2f}  Average100 Score: {:.2f}'.format(i_episode,np.min(score),np.max(score),np.mean(score), np.mean(scores_deque)))\n",
    "        if np.mean(scores_deque) >= 30:\n",
    "            print(\"\\nSolved Environment!\")\n",
    "            torch.save(agent.actor_local.state_dict(), 'final_actor.pth')\n",
    "            torch.save(agent.critic_local.state_dict(), 'final_critic.pth')\n",
    "            break\n",
    "    return scores, minmax_scores, average_100_scores\n",
    "\n",
    "agent = Agent(state_size=state_size, action_size=action_size, n_step=1, per=0, munchausen=0,distributional=1,\n",
    "                 D2RL=D2RL, random_seed=seed, hidden_size=HIDDEN_SIZE, BATCH_SIZE=BATCH_SIZE, BUFFER_SIZE=BUFFER_SIZE, GAMMA=GAMMA,\n",
    "                 LR_ACTOR=LR_ACTOR, LR_CRITIC=LR_CRITIC, TAU=TAU, LEARN_EVERY=1, LEARN_NUMBER=1, device=device, frames=frames, worker=1)\n",
    "\n",
    "\n",
    "scores, minmax, average_100 = ddpg()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
