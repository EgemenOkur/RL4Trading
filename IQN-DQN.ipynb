{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "import random\n",
    "import math\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from collections import deque, namedtuple\n",
    "import time\n",
    "import gym\n",
    "import os\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "def weight_init(layers):\n",
    "    for layer in layers:\n",
    "        torch.nn.init.kaiming_normal_(layer.weight, nonlinearity='relu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3.common.vec_env.base_vec_env import VecEnv, VecEnvStepReturn, VecEnvWrapper\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from stockstats import StockDataFrame as Sdf\n",
    "\n",
    "import datetime\n",
    "\n",
    "import datetime\n",
    "import os\n",
    "\n",
    "TRAINING_DATA_FILE = \"dataprocessing/Yfinance_Data.csv\"\n",
    "\n",
    "now = datetime.datetime.now()\n",
    "TRAINED_MODEL_DIR = f\"trained_models/{now}\"\n",
    "os.makedirs(TRAINED_MODEL_DIR)\n",
    "\n",
    "TESTING_DATA_FILE = \"test.csv\"\n",
    "\n",
    "def load_dataset(*, file_name: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    load csv dataset from path\n",
    "    :return: (df) pandas dataframe\n",
    "    \"\"\"\n",
    "    # _data = pd.read_csv(f\"{config.DATASET_DIR}/{file_name}\")\n",
    "    _data = pd.read_csv(file_name)\n",
    "\n",
    "    return _data\n",
    "\n",
    "\n",
    "def data_split(df, start, end):\n",
    "    \"\"\"\n",
    "    split the dataset into training or testing using date\n",
    "    :param data: (df) pandas dataframe, start, end\n",
    "    :return: (df) pandas dataframe\n",
    "    \"\"\"\n",
    "    data = df[(df.datadate >= start) & (df.datadate < end)]\n",
    "    data = data.sort_values(['datadate', 'tic'], ignore_index=True)\n",
    "\n",
    "\n",
    "    # data  = data[final_columns]\n",
    "    data.index = data.datadate.factorize()[0]\n",
    "\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def calculate_price(df):\n",
    "    \"\"\"\n",
    "    calcualte adjusted close price, open-high-low price and volume\n",
    "    :param data: (df) pandas dataframe\n",
    "    :return: (df) pandas dataframe\n",
    "    \"\"\"\n",
    "    data = df.copy()\n",
    "\n",
    "    data = data[['Date', 'tic', 'Close', 'Open', 'High', 'Low', 'Volume','datadate']]\n",
    "    data = data.sort_values(['tic', 'datadate'], ignore_index=True)\n",
    "    return data\n",
    "\n",
    "\n",
    "def add_technical_indicator(df):\n",
    "    \"\"\"\n",
    "    calcualte technical indicators\n",
    "    use stockstats package to add technical inidactors\n",
    "    :param data: (df) pandas dataframe\n",
    "    :return: (df) pandas dataframe\n",
    "    \"\"\"\n",
    "    stock = Sdf.retype(df.copy())\n",
    "\n",
    "    #print(stock)\n",
    "\n",
    "    unique_ticker = stock.tic.unique()\n",
    "\n",
    "    macd = pd.DataFrame()\n",
    "    rsi = pd.DataFrame()\n",
    "    cci = pd.DataFrame()\n",
    "    dx = pd.DataFrame()\n",
    "\n",
    "    # temp = stock[stock.tic == unique_ticker[0]]['macd']\n",
    "    for i in range(len(unique_ticker)):\n",
    "        ## macd\n",
    "        temp_macd = stock[stock.tic == unique_ticker[i]]['macd']\n",
    "        temp_macd = pd.DataFrame(temp_macd)\n",
    "        macd = macd.append(temp_macd, ignore_index=True)\n",
    "        ## rsi\n",
    "        temp_rsi = stock[stock.tic == unique_ticker[i]]['rsi_30']\n",
    "        temp_rsi = pd.DataFrame(temp_rsi)\n",
    "        rsi = rsi.append(temp_rsi, ignore_index=True)\n",
    "        ## cci\n",
    "        temp_cci = stock[stock.tic == unique_ticker[i]]['cci_30']\n",
    "        temp_cci = pd.DataFrame(temp_cci)\n",
    "        cci = cci.append(temp_cci, ignore_index=True)\n",
    "        ## adx\n",
    "        temp_dx = stock[stock.tic == unique_ticker[i]]['dx_30']\n",
    "        temp_dx = pd.DataFrame(temp_dx)\n",
    "        dx = dx.append(temp_dx, ignore_index=True)\n",
    "\n",
    "    df['macd'] = macd\n",
    "    df['rsi'] = rsi\n",
    "    df['cci'] = cci\n",
    "    df['adx'] = dx\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def preprocess_data():\n",
    "    \"\"\"data preprocessing pipeline\"\"\"\n",
    "    start = datetime.datetime(2010, 12, 1)\n",
    "    df = load_dataset(file_name=TRAINING_DATA_FILE)\n",
    "    # get data after 2010\n",
    "    # df = df[df.Date >= start]\n",
    "    # calcualte adjusted price\n",
    "    df_preprocess = calculate_price(df)\n",
    "    # add technical indicators using stockstats\n",
    "    df_final = add_technical_indicator(df_preprocess)\n",
    "    # fill the missing values at the beginning\n",
    "    df_final.fillna(method='bfill', inplace=True)\n",
    "    return df_final\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IQN(nn.Module):\n",
    "    def __init__(self, state_size, action_size,layer_size, n_step, seed, layer_type=\"ff\"):\n",
    "        super(IQN, self).__init__()\n",
    "        self.seed = torch.manual_seed(seed)\n",
    "        self.input_shape = state_size\n",
    "        self.action_size = action_size\n",
    "        self.K = 32\n",
    "        self.N = 8\n",
    "        self.n_cos = 64\n",
    "        self.layer_size = layer_size\n",
    "        self.pis = torch.FloatTensor([np.pi*i for i in range(self.n_cos)]).view(1,1,self.n_cos).to(device) # Starting from 0 as in the paper \n",
    "\n",
    "        self.head = nn.Linear(self.input_shape, layer_size) # cound be a cnn \n",
    "        self.cos_embedding = nn.Linear(self.n_cos, layer_size)\n",
    "        self.ff_1 = nn.Linear(layer_size, layer_size)\n",
    "        self.ff_2 = nn.Linear(layer_size, action_size)\n",
    "        #weight_init([self.head_1, self.ff_1])\n",
    "\n",
    "\n",
    "        \n",
    "    def calc_cos(self, batch_size, n_tau=8):\n",
    "        \"\"\"\n",
    "        Calculating the cosinus values depending on the number of tau samples\n",
    "        \"\"\"\n",
    "        taus = torch.rand(batch_size, n_tau).to(device).unsqueeze(-1) #(batch_size, n_tau, 1)\n",
    "        cos = torch.cos(taus*self.pis)\n",
    "\n",
    "        assert cos.shape == (batch_size,n_tau,self.n_cos), \"cos shape is incorrect\"\n",
    "        return cos, taus\n",
    "    \n",
    "    def forward(self, input, num_tau=8):\n",
    "        \"\"\"\n",
    "        Quantile Calculation depending on the number of tau\n",
    "        \n",
    "        Return:\n",
    "        quantiles [ shape of (batch_size, num_tau, action_size)]\n",
    "        taus [shape of ((batch_size, num_tau, 1))]\n",
    "        \n",
    "        \"\"\"\n",
    "        batch_size = input.shape[0]\n",
    "        \n",
    "        #print(\"self.head(input):{}\".format(self.head(input).shape))\n",
    "        \n",
    "        x = torch.relu(self.head(input))\n",
    "    \n",
    "        #print(\"batch_size:{}\".format(batch_size))\n",
    "        #print(\"X:{}\".format(x.shape))\n",
    "        \n",
    "        cos, taus = self.calc_cos(batch_size, num_tau) # cos shape (batch, num_tau, layer_size)\n",
    "        \n",
    "        #print(\"cos:{}\".format(cos.shape))\n",
    "        #print(\"taus:{}\".format(taus.shape))\n",
    "        \n",
    "        \n",
    "        cos = cos.view(batch_size*num_tau, self.n_cos)\n",
    "        cos_x = torch.relu(self.cos_embedding(cos)).view(batch_size, num_tau, self.layer_size) # (batch, n_tau, layer)\n",
    "        \n",
    "        # x has shape (batch, layer_size) for multiplication –> reshape to (batch, 1, layer)\n",
    "        #x = (x.unsqueeze(1)*cos_x).view(batch_size*num_tau, self.layer_size)\n",
    "        #print(\"x:{},cos_x Shape:{},batch_size:{},layer_size:{}\".format(x.shape,cos_x.shape,batch_size,self.layer_size))\n",
    "        x = (x.unsqueeze(1) * cos_x).view(batch_size * num_tau, self.layer_size)\n",
    "        #print(\"---------°°°°°°°°°------X----------°°°°°°°°°°-------:{}\".format(x.shape))\n",
    "        \n",
    "        x = torch.relu(self.ff_1(x))\n",
    "        out = self.ff_2(x)\n",
    "        #print(\"---------°°°°°°°°°------out----------°°°°°°°°°°-------:{}\".format(out.shape))\n",
    "        \n",
    "        return out.view(batch_size, num_tau, self.action_size), taus\n",
    "    \n",
    "    def get_action(self, inputs):\n",
    "        quantiles, _ = self.forward(inputs, self.K)\n",
    "        #print(\"quantiles:{}\".format(quantiles.shape))\n",
    "        actions = quantiles.mean(dim=1) #TODO: actions space= torch.Size([1, 32, 30])\n",
    "        #print(\"action space quantile:{}\".format(actions))\n",
    "        return actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    \"\"\"Fixed-size buffer to store experience tuples.\"\"\"\n",
    "\n",
    "    def __init__(self, buffer_size, batch_size, device, seed, gamma, n_step=1):\n",
    "        \"\"\"Initialize a ReplayBuffer object.\n",
    "        Params\n",
    "        ======\n",
    "            buffer_size (int): maximum size of buffer\n",
    "            batch_size (int): size of each training batch\n",
    "            seed (int): random seed\n",
    "        \"\"\"\n",
    "        self.device = device\n",
    "        self.memory = deque(maxlen=buffer_size)  \n",
    "        self.batch_size = batch_size\n",
    "        self.experience = namedtuple(\"Experience\", field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
    "        self.seed = random.seed(seed)\n",
    "        self.gamma = gamma\n",
    "        self.n_step = n_step\n",
    "        self.n_step_buffer = deque(maxlen=self.n_step)\n",
    "    \n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Add a new experience to memory.\"\"\"\n",
    "        #print(\"before:\", state,action,reward,next_state, done)\n",
    "        self.n_step_buffer.append((state, action, reward, next_state, done))\n",
    "        if len(self.n_step_buffer) == self.n_step:\n",
    "            state, action, reward, next_state, done = self.calc_multistep_return()\n",
    "            #print(\"after:\",state,action,reward,next_state, done)\n",
    "            e = self.experience(state, action, reward, next_state, done)\n",
    "            self.memory.append(e)\n",
    "    \n",
    "    def calc_multistep_return(self):\n",
    "        Return = 0\n",
    "        for idx in range(self.n_step):\n",
    "            Return += self.gamma**idx * self.n_step_buffer[idx][2]\n",
    "        \n",
    "        return self.n_step_buffer[0][0], self.n_step_buffer[0][1], Return, self.n_step_buffer[-1][3], self.n_step_buffer[-1][4]\n",
    "        \n",
    "    \n",
    "    \n",
    "    def sample(self):\n",
    "        \"\"\"Randomly sample a batch of experiences from memory.\"\"\"\n",
    "        experiences = random.sample(self.memory, k=self.batch_size)\n",
    "\n",
    "        states = torch.from_numpy(np.stack([e.state for e in experiences if e is not None])).float().to(self.device)\n",
    "        actions = torch.from_numpy(np.vstack([e.action for e in experiences if e is not None])).long().to(self.device)\n",
    "        rewards = torch.from_numpy(np.vstack([e.reward for e in experiences if e is not None])).float().to(self.device)\n",
    "        next_states = torch.from_numpy(np.stack([e.next_state for e in experiences if e is not None])).float().to(self.device)\n",
    "        dones = torch.from_numpy(np.vstack([e.done for e in experiences if e is not None]).astype(np.uint8)).float().to(self.device)\n",
    "  \n",
    "        return (states, actions, rewards, next_states, dones)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Return the current size of internal memory.\"\"\"\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class DQN_Agent():\n",
    "    \"\"\"Interacts with and learns from the environment.\"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 state_size,\n",
    "                 action_size,\n",
    "                 layer_size,\n",
    "                 n_step,\n",
    "                 BATCH_SIZE,\n",
    "                 BUFFER_SIZE,\n",
    "                 LR,\n",
    "                 TAU,\n",
    "                 GAMMA,\n",
    "                 UPDATE_EVERY,\n",
    "                 device,\n",
    "                 seed):\n",
    "        \"\"\"Initialize an Agent object.\n",
    "        \n",
    "        Params\n",
    "        ======\n",
    "            state_size (int): dimension of each state\n",
    "            action_size (int): dimension of each action\n",
    "            layer_size (int): size of the hidden layer\n",
    "            BATCH_SIZE (int): size of the training batch\n",
    "            BUFFER_SIZE (int): size of the replay memory\n",
    "            LR (float): learning rate\n",
    "            TAU (float): tau for soft updating the network weights\n",
    "            GAMMA (float): discount factor\n",
    "            UPDATE_EVERY (int): update frequency\n",
    "            device (str): device that is used for the compute\n",
    "            seed (int): random seed\n",
    "        \"\"\"\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.seed = random.seed(seed)\n",
    "        self.device = device\n",
    "        self.TAU = TAU\n",
    "        self.GAMMA = GAMMA\n",
    "        self.UPDATE_EVERY = UPDATE_EVERY\n",
    "        self.BATCH_SIZE = BATCH_SIZE\n",
    "        self.Q_updates = 0\n",
    "        self.n_step = n_step\n",
    "        self.action = []\n",
    "\n",
    "        self.action_step = 30\n",
    "\n",
    "        # IQN-Network\n",
    "        self.qnetwork_local = IQN(state_size, action_size,layer_size, n_step, seed).to(device)\n",
    "        self.qnetwork_target = IQN(state_size, action_size,layer_size, n_step, seed).to(device)\n",
    "\n",
    "        self.optimizer = optim.Adam(self.qnetwork_local.parameters(), lr=LR)\n",
    "        #print(self.qnetwork_local)\n",
    "        \n",
    "        # Replay memory\n",
    "        self.memory = ReplayBuffer(BUFFER_SIZE, BATCH_SIZE, self.device, seed, self.GAMMA, n_step)\n",
    "        \n",
    "        # Initialize time step (for updating every UPDATE_EVERY steps)\n",
    "        self.t_step = 0\n",
    "    \n",
    "    def step(self, state, action, reward, next_state, done, writer):\n",
    "        # Save experience in replay memory\n",
    "        #print(\"to memory action:{},state:{},next_state\".format(action,state.shape,next_state.shape))\n",
    "        self.memory.add(state, action, reward, next_state, done)\n",
    "        \n",
    "        # Learn every UPDATE_EVERY time steps.\n",
    "        self.t_step = (self.t_step + 1) % self.UPDATE_EVERY\n",
    "        if self.t_step == 0:\n",
    "            # If enough samples are available in memory, get random subset and learn\n",
    "            if len(self.memory) > self.BATCH_SIZE:\n",
    "                \n",
    "                experiences = self.memory.sample()\n",
    "                #print(\"experiences:{}\".format(experiences))\n",
    "                loss = self.learn(experiences)\n",
    "                self.Q_updates += 1\n",
    "                writer.add_scalar(\"Q_loss\", loss, self.Q_updates)\n",
    "\n",
    "    def act(self, state, eps=0.):\n",
    "        \"\"\"Returns actions for given state as per current policy. Acting only every 4 frames!\n",
    "        \n",
    "        Params\n",
    "        ======\n",
    "            frame: to adjust epsilon\n",
    "            state (array_like): current state\n",
    "            \n",
    "        \"\"\"\n",
    "        #print(\"without np.array:{}\".format(state.shape))\n",
    "\n",
    "        state = np.array(state)\n",
    "\n",
    "        #print(\"this is the state space before torch.from_numpy:{}\".format(state.shape))\n",
    "        \n",
    "        state = torch.from_numpy(state).float().unsqueeze(0).to(self.device) #WHY?\n",
    "        \n",
    "        #print(\"this is the state space after torch.from_numpy:{}\".format(state.shape))\n",
    "        \n",
    "        \n",
    "        self.qnetwork_local.eval() #WHY?\n",
    "        \n",
    "        \n",
    "        with torch.no_grad():\n",
    "            #print(\"this is the state space:{}\".format(state.shape))\n",
    "            action_values = self.qnetwork_local.get_action(state) # 30 dimensions are coming back.\n",
    "            #print('action_value:{}'.format(action_values))\n",
    "        self.qnetwork_local.train()\n",
    "\n",
    "        # Epsilon-greedy action selection\n",
    "        if random.random() > eps: # select greedy action if random number is higher than epsilon or noisy network is used!\n",
    "            action = np.argmax(action_values.cpu().data.numpy())\n",
    "            self.last_action = action\n",
    "            return action\n",
    "        else:\n",
    "            action = random.choice(np.arange(self.action_size))\n",
    "            self.last_action = action \n",
    "            return action\n",
    "\n",
    "\n",
    "\n",
    "    def learn(self, experiences):\n",
    "        \"\"\"Update value parameters using given batch of experience tuples.\n",
    "        Params\n",
    "        ======\n",
    "            experiences (Tuple[torch.Tensor]): tuple of (s, a, r, s', done) tuples \n",
    "            gamma (float): discount factor\n",
    "        \"\"\"\n",
    "        self.optimizer.zero_grad()\n",
    "        states, actions, rewards, next_states, dones = experiences\n",
    "        #print(\"learning states:{}, next_states:{}\".format(states.shape, next_states.shape))\n",
    "        # Get max predicted Q values (for next states) from target model\n",
    "        Q_targets_next, _ = self.qnetwork_target(next_states)\n",
    "        \n",
    "        #print(\"--Q_targets_next :{}\".format(Q_targets_next.shape))\n",
    "        \n",
    "        #print(\"---->Q_targets_next:{}\".format(Q_targets_next))\n",
    "        #print('------------------------------------')\n",
    "        #print(\"--Q_targets_next detach max:{}\".format(Q_targets_next.detach().max(2)))\n",
    "        \n",
    "        \n",
    "        \n",
    "        #print(\"--Q_targets_next.detach().max(2)[0].unsqueeze(1):{}\".format(Q_targets_next.detach().max(2)[0].unsqueeze(1)))\n",
    "        Q_targets_next = Q_targets_next.detach().max(2)[0].unsqueeze(1) # (batch_size, 1, N)\n",
    "        \n",
    "        # Compute Q targets for current states \n",
    "        Q_targets = rewards.unsqueeze(-1) + (self.GAMMA**self.n_step * Q_targets_next * (1. - dones.unsqueeze(-1)))\n",
    "        # Get expected Q values from local model\n",
    "        Q_expected, taus = self.qnetwork_local(states)\n",
    "        \n",
    "        #print(\"rewards:{}\".format(rewards.shape))\n",
    "        #print(\"Q_targets_Shape:{}\".format(Q_targets.shape))\n",
    "        #print(\"actions shape:{}\".format(actions.shape))\n",
    "        #print(\"Q_expected shape:{}\".format(Q_expected.shape))\n",
    "        #print(\"actions.unsqueeze(-1).shape:{}\".format(actions.unsqueeze(-1).shape))\n",
    "        #print(\"actions:{}\".format(actions))\n",
    "        #print(\"Q_expected:{}\".format(Q_expected))\n",
    "        #print(\"actions.unsqueeze(-1){}\".format(actions.unsqueeze(-1)))\n",
    "        Q_expected_2 = Q_expected.gather(2, actions.unsqueeze(-1))\n",
    "\n",
    "        #print(\"Q_expected.gather(2, actions.unsqueeze(-1):{}\".format(Q_expected_2.shape))\n",
    "        \n",
    "        Q_expected = Q_expected.gather(2, actions[0].unsqueeze(-1).expand(self.BATCH_SIZE, 8, 1))\n",
    "        #print(\"Final what we need Q_expected-----:{}\".format(Q_expected.shape))\n",
    "\n",
    "        # Quantile Huber loss\n",
    "        td_error = Q_targets - Q_expected\n",
    "        #print(\"td_error.shape:{}\".format(td_error.shape))\n",
    "        #print(\"Q_expected.shape:{}\".format(Q_expected.shape))\n",
    "        #print(\"td_error:{}\".format(td_error.shape))\n",
    "        assert td_error.shape == (self.BATCH_SIZE, 8, 8), \"wrong td error shape\"\n",
    "        huber_l = calculate_huber_loss(td_error, 1.0)\n",
    "        quantil_l = abs(taus -(td_error.detach() < 0).float()) * huber_l / 1.0\n",
    "        \n",
    "        loss = quantil_l.sum(dim=1).mean(dim=1) # , keepdim=True if per weights get multipl\n",
    "        loss = loss.mean()\n",
    "\n",
    "\n",
    "        # Minimize the loss\n",
    "        loss.backward()\n",
    "        #clip_grad_norm_(self.qnetwork_local.parameters(),1)\n",
    "        self.optimizer.step()\n",
    "\n",
    "        # ------------------- update target network ------------------- #\n",
    "        self.soft_update(self.qnetwork_local, self.qnetwork_target)\n",
    "        return loss.detach().cpu().numpy()            \n",
    "\n",
    "    def soft_update(self, local_model, target_model):\n",
    "        \"\"\"Soft update model parameters.\n",
    "        θ_target = τ*θ_local + (1 - τ)*θ_target\n",
    "        Params\n",
    "        ======\n",
    "            local_model (PyTorch model): weights will be copied from\n",
    "            target_model (PyTorch model): weights will be copied to\n",
    "            tau (float): interpolation parameter \n",
    "        \"\"\"\n",
    "        for target_param, local_param in zip(target_model.parameters(), local_model.parameters()):\n",
    "            target_param.data.copy_(self.TAU*local_param.data + (1.0-self.TAU)*target_param.data)\n",
    "\n",
    "\n",
    "def calculate_huber_loss(td_errors, k=1.0):\n",
    "    \"\"\"\n",
    "    Calculate huber loss element-wisely depending on kappa k.\n",
    "    \"\"\"\n",
    "    loss = torch.where(td_errors.abs() <= k, 0.5 * td_errors.pow(2), k * (td_errors.abs() - 0.5 * k))\n",
    "    #print('this is huber loss: {}'.format(loss.shape))\n",
    "    assert loss.shape == (td_errors.shape[0], 8, 8), \"huber loss has wrong shape\"\n",
    "    return loss\n",
    "    \n",
    "def eval_runs(eps, frame):\n",
    "    \"\"\"\n",
    "    Makes an evaluation run with the current epsilon\n",
    "    \"\"\"\n",
    "    print(\"-----------------------------------------evaluating-----------------------------------------\")\n",
    "    env = gym.make(\"Acrobot-v1\") # TODO:\n",
    "    reward_batch = []\n",
    "    for i in range(5):\n",
    "        state = env.reset()\n",
    "        rewards = 0\n",
    "        while True:\n",
    "            action = agent.act(state, eps)\n",
    "            state, reward, done, _ = env.step(action)\n",
    "            rewards += reward\n",
    "            if done:\n",
    "                break\n",
    "        reward_batch.append(rewards)\n",
    "        \n",
    "    writer.add_scalar(\"Reward\", np.mean(reward_batch), frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from IPython import display\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from gym.utils import seeding\n",
    "import gym\n",
    "import os\n",
    "from gym import spaces\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "\n",
    "# shares normalization factor\n",
    "# 100 shares per trade\n",
    "HMAX_NORMALIZE = 100\n",
    "# initial amount of money we have in our account\n",
    "INITIAL_ACCOUNT_BALANCE= 1000\n",
    "# total number of stocks in our portfolio\n",
    "STOCK_DIM = 3\n",
    "# transaction fee: 1/1000 reasonable percentage\n",
    "TRANSACTION_FEE_PERCENT = 0.001\n",
    "REWARD_SCALING = 1e-4\n",
    "\n",
    "class StockEnvTrain(gym.Env):\n",
    "    \"\"\"A stock trading environment for OpenAI gym\"\"\"\n",
    "    metadata = {'render.modes': ['human']}\n",
    "\n",
    "    def __init__(self, df,day = 0):\n",
    "        #super(StockEnv, self).__init__()\n",
    "        #money = 10 , scope = 1\n",
    "        self.day = day\n",
    "        self.df = df\n",
    "        self.agent_stock_iteration_index = 0\n",
    "\n",
    "        # action_space normalization and shape is STOCK_DIM\n",
    "        self.action_space = spaces.Box(low = -1, high = 1,shape = (STOCK_DIM,)) \n",
    "        # Shape = 181: [Current Balance]+[prices 1-30]+[owned shares 1-30] \n",
    "        # +[macd 1-30]+ [rsi 1-30] + [cci 1-30] + [adx 1-30]\n",
    "        self.observation_space = spaces.Box(low=0, high=np.inf, shape = (19,))\n",
    "        # load data from a pandas dataframe\n",
    "        #print('df: {}'.format(self.df))\n",
    "        #print('day: {}'.format(self.day))\n",
    "        self.data = self.df.loc[self.day,:]\n",
    "        #print(self.data.Close)\n",
    "        self.terminal = False\n",
    "\n",
    "\n",
    "\n",
    "        # initalize state\n",
    "        self.state = [INITIAL_ACCOUNT_BALANCE] + \\\n",
    "                      self.data.Close.values.tolist() + \\\n",
    "                      [0]*STOCK_DIM + \\\n",
    "                      self.data.macd.values.tolist() + \\\n",
    "                      self.data.rsi.values.tolist() + \\\n",
    "                      self.data.cci.values.tolist() + \\\n",
    "                      self.data.adx.values.tolist()\n",
    "        # initialize reward\n",
    "        self.reward = 0\n",
    "        self.cost = 0\n",
    "        # memorize all the total balance change\n",
    "        self.asset_memory = [INITIAL_ACCOUNT_BALANCE]\n",
    "        self.rewards_memory = []\n",
    "        self.final_asset_value = 0\n",
    "        self.trades = 0\n",
    "        #self.reset()\n",
    "        self._seed()\n",
    "\n",
    "\n",
    "    def _sell_stock(self, index, action):\n",
    "        # perform sell action based on the sign of the action\n",
    "        if self.state[index+STOCK_DIM+1] > 0:\n",
    "            #update balance\n",
    "            self.state[0] += \\\n",
    "            self.state[index+1]*min(abs(action),self.state[index+STOCK_DIM+1]) * \\\n",
    "             (1- TRANSACTION_FEE_PERCENT)\n",
    "\n",
    "            self.state[index+STOCK_DIM+1] -= min(abs(action), self.state[index+STOCK_DIM+1])\n",
    "            self.cost +=self.state[index+1]*min(abs(action),self.state[index+STOCK_DIM+1]) * \\\n",
    "             TRANSACTION_FEE_PERCENT\n",
    "            self.trades+=1\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "    \n",
    "    def _buy_stock(self, index, action):\n",
    "        # perform buy action based on the sign of the action\n",
    "        available_amount = self.state[0] // self.state[index+1]\n",
    "        # print('available_amount:{}'.format(available_amount))\n",
    "\n",
    "        #update balance\n",
    "        self.state[0] -= self.state[index+1]*min(available_amount, action)* \\\n",
    "                          (1+ TRANSACTION_FEE_PERCENT)\n",
    "\n",
    "        self.state[index+STOCK_DIM+1] += min(available_amount, action)\n",
    "\n",
    "        self.cost+=self.state[index+1]*min(available_amount, action)* \\\n",
    "                          TRANSACTION_FEE_PERCENT\n",
    "        self.trades+=1\n",
    "        \n",
    "    def step(self, actions):\n",
    "        # print(self.day)\n",
    "        self.terminal = self.day >= len(self.df.index.unique())-1\n",
    "        #print(actions)\n",
    "        self.actions = actions\n",
    "        if self.terminal:\n",
    "            print(\"Finished\")\n",
    "            print(self.state)\n",
    "            end_total_asset = self.state[0]+ \\\n",
    "            sum(np.array(self.state[1:(STOCK_DIM+1)])*np.array(self.state[(STOCK_DIM+1):(STOCK_DIM*2+1)]))\n",
    "\n",
    "            print(\"end_total_asset:{}\".format(end_total_asset))\n",
    "            df_total_value = pd.DataFrame(self.asset_memory)\n",
    "            #df_total_value.to_csv('results/account_value_train.csv')\n",
    "            #print(\"total_reward:{}\".format(self.state[0]+sum(np.array(self.state[1:(STOCK_DIM+1)])*np.array(self.state[(STOCK_DIM+1):61]))- INITIAL_ACCOUNT_BALANCE ))\n",
    "            #print(\"total_cost: \", self.cost)\n",
    "            #print(\"total_trades: \", self.trades)\n",
    "            df_total_value.columns = ['account_value']\n",
    "            df_total_value['daily_return']=df_total_value.pct_change(1)\n",
    "            sharpe = (252**0.5)*df_total_value['daily_return'].mean()/ \\\n",
    "                  df_total_value['daily_return'].std()\n",
    "            print(\"Sharpe: \",sharpe)\n",
    "            #print(\"=================================\")\n",
    "            df_rewards = pd.DataFrame(self.rewards_memory)\n",
    "            #df_rewards.to_csv('results/account_rewards_train.csv')\n",
    "\n",
    "            # print('total asset: {}'.format(self.state[0]+ sum(np.array(self.state[1:29])*np.array(self.state[29:]))))\n",
    "            #with open('obs.pkl', 'wb') as f:  \n",
    "            #    pickle.dump(self.state, f)\n",
    "\n",
    "            return self.state, self.reward, self.terminal,{}\n",
    "\n",
    "        else:\n",
    "            # print(np.array(self.state[1:29]))\n",
    "            #print(\"The actions is: {}\".format(self.actions))\n",
    "\n",
    "            #action = np.array([4,4,5])\n",
    "            #actions = np.array([4,0,0,0,0,0,0,0,4,0,4,0,-3,0,0,0,0,0,0,0,4,0,0,0,0,0,0,0,0,0])\n",
    "\n",
    "            #actions = self.actions * HMAX_NORMALIZE #WHY??\n",
    "            #print(\"actions-index------:{}\".format(actions))\n",
    "            #actions = (actions.astype(int))\n",
    "\n",
    "            begin_total_asset = self.state[0]+ \\\n",
    "            sum(np.array(self.state[1:(STOCK_DIM+1)])*np.array(self.state[(STOCK_DIM+1):(STOCK_DIM*2+1)]))\n",
    "            #print(\"begin_total_asset:{}\".format(begin_total_asset))\n",
    "\n",
    "            argsort_actions = np.argsort(actions) #TODO: this may not be touched.\n",
    "            #print(\"The actions is: {}\".format(actions))\n",
    "\n",
    "            sell_index = argsort_actions[:np.where(actions == 0)[0].shape[0]]\n",
    "            #sell_index = argsort_actions[4,0,0,0,0,0,0,0,4,0,4,0,-3,0,0,0,0,0,0,0,4,0,0,0,0,0,0,0,0,0]\n",
    "            #print(\"sell-index------:{}\".format(sell_index))\n",
    "            buy_index = argsort_actions[::-1][:np.where(actions == 2)[0].shape[0]]\n",
    "            #buy_index = argsort_actions[::-1][4,0,0,0,0,0,0,0,4,0,4,0,-3,0,0,0,0,0,0,0,4,0,0,0,0,0,0,0,0,0]\n",
    "            #print(\"buy-index------:{}\".format(buy_index))\n",
    "\n",
    "            for index in sell_index:\n",
    "            # print('take sell action'.format(actions[index]))\n",
    "                #print(\"--------Action Shape:{}\".format(actions.shape))\n",
    "                self._sell_stock(index+ self.agent_stock_iteration_index, 1)\n",
    "\n",
    "            for index in buy_index:\n",
    "                #print(\"--------Action Shape:{}\".format(actions.shape))\n",
    "            # print('take buy action: {}'.format(actions[index]))\n",
    "                self._buy_stock(index+ self.agent_stock_iteration_index, 1)\n",
    "                \n",
    "            \n",
    "            #print(\"self.day:{}\".format(self.day))\n",
    "            \n",
    "            \n",
    "\n",
    "                \n",
    "            #load next state\n",
    "            # print(\"stock_shares:{}\".format(self.state[29:]))\n",
    "            self.state =  [self.state[0]] + \\\n",
    "                self.data.Close.values.tolist() + \\\n",
    "                list(self.state[(STOCK_DIM+1):(STOCK_DIM*2+1)]) + \\\n",
    "                self.data.macd.values.tolist() + \\\n",
    "                self.data.rsi.values.tolist() + \\\n",
    "                self.data.cci.values.tolist() + \\\n",
    "                self.data.adx.values.tolist()\n",
    "\n",
    "            end_total_asset = self.state[0]+ \\\n",
    "            sum(np.array(self.state[1:(STOCK_DIM+1)])*np.array(self.state[(STOCK_DIM+1):(STOCK_DIM*2+1)]))\n",
    "            self.asset_memory.append(end_total_asset)\n",
    "            #print(\"end_total_asset:{}\".format(end_total_asset))\n",
    "\n",
    "            self.reward = end_total_asset - begin_total_asset            \n",
    "            # print(\"step_reward:{}\".format(self.reward))\n",
    "            self.rewards_memory.append(self.reward)\n",
    "\n",
    "            self.reward = self.reward*REWARD_SCALING\n",
    "            \n",
    "            self.agent_stock_iteration_index += 1 \n",
    "            if self.agent_stock_iteration_index ==3:\n",
    "                self.day += 1\n",
    "                self.data = self.df.loc[self.day,:]\n",
    "                self.agent_stock_iteration_index = 0\n",
    "            \n",
    "            \n",
    "\n",
    "        return self.state, self.reward, self.terminal, {}\n",
    "\n",
    "    def reset(self):\n",
    "        self.final_asset_value = 0\n",
    "        self.trades = 0\n",
    "        self.asset_memory = [INITIAL_ACCOUNT_BALANCE]\n",
    "        self.day = 0\n",
    "        self.data = self.df.loc[self.day,:]\n",
    "        self.cost = 0\n",
    "        self.trades = 0\n",
    "        self.terminal = False \n",
    "        self.rewards_memory = []\n",
    "        self.agent_stock_iteration_index = 0\n",
    "        #initiate state\n",
    "        self.state = [INITIAL_ACCOUNT_BALANCE] + \\\n",
    "                      self.data.Close.values.tolist() + \\\n",
    "                      [0]*STOCK_DIM + \\\n",
    "                      self.data.macd.values.tolist() + \\\n",
    "                      self.data.rsi.values.tolist() + \\\n",
    "                      self.data.cci.values.tolist() + \\\n",
    "                      self.data.adx.values.tolist() \n",
    "        # iteration += 1 \n",
    "        #print(\"[0]*STOCK_DIM:{}\".format([0]*STOCK_DIM))\n",
    "        #print(\"self.state:{}\".format(len(self.state)))\n",
    "        print(np.array(self.state))\n",
    "        return np.array(self.state)\n",
    "    \n",
    "    def render(self, mode='human'):\n",
    "        return self.state\n",
    "\n",
    "    def _seed(self, seed=None):\n",
    "        self.np_random, seed = seeding.np_random(seed)\n",
    "        return [seed]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def run(env,frames=1000, eps_fixed=False, eps_frames=1e6, min_eps=0.01):\n",
    "    \"\"\"Deep Q-Learning.\n",
    "    \n",
    "    Params\n",
    "    ======\n",
    "        n_episodes (int): maximum number of training episodes\n",
    "        max_t (int): maximum number of timesteps per episode\n",
    "        eps_start (float): starting value of epsilon, for epsilon-greedy action selection\n",
    "        eps_end (float): minimum value of epsilon\n",
    "        eps_decay (float): multiplicative factor (per episode) for decreasing epsilon\n",
    "    \"\"\"\n",
    "    scores = []                        # list containing scores from each episode\n",
    "    scores_window = deque(maxlen=100)  # last 100 scores\n",
    "    output_history = []\n",
    "    frame = 0\n",
    "    if eps_fixed:\n",
    "        eps = 0\n",
    "    else:\n",
    "        eps = 1\n",
    "    eps_start = 1\n",
    "    i_episode = 1\n",
    "    state = env.reset()\n",
    "    state = state[0,:]\n",
    "    #print(\"state space:{}\".format(state[0,:].shape))\n",
    "    score = 0                  \n",
    "    for frame in range(1, frames+1):\n",
    "        \n",
    "        if frame  == 0:\n",
    "            # inital state\n",
    "\n",
    "            initial = True\n",
    "        else:\n",
    "            # previous state\n",
    "            initial = False\n",
    "        #print('initial state:{}'.format(initial))\n",
    "\n",
    "        action = agent.act(state, eps) #TODO: getting one dimension back.\n",
    "        action = np.array([action])\n",
    "        next_state, reward, done, info = env_train.step([action]) #TODO: Wants a list of actions of size a\n",
    "\n",
    "        #print(\"env_trainNext State: {}\".format(next_state.shape))\n",
    "\n",
    "        next_state = next_state[0,:]\n",
    "        \n",
    "        agent.step(state, action, reward, next_state, done, writer)\n",
    "\n",
    "        \n",
    "        state = next_state\n",
    "        score += reward\n",
    "        # linear annealing to the min epsilon value until eps_frames and from there slowly decease epsilon to 0 until the end of training\n",
    "        if eps_fixed == False:\n",
    "            if frame < eps_frames:\n",
    "                eps = max(eps_start - (frame*(1/eps_frames)), min_eps)\n",
    "            else:\n",
    "                eps = max(min_eps - min_eps*((frame-eps_frames)/(frames-eps_frames)), 0.001)\n",
    "\n",
    "        # evaluation runs\n",
    "        if frame % 100000 == 0:\n",
    "            print(\"score: {}\".format(state))\n",
    "            print(\"score: {}\".format(score))\n",
    "            #print(\"state: {}\".format(state))\n",
    "            print(\"action:{}, Number:{}\".format(action,frame))\n",
    "            print(\"-------------------------\")\n",
    "        \n",
    "        if done:\n",
    "            \n",
    "            for k, v in agent.qnetwork_local.named_parameters():\n",
    "                print(k, v)\n",
    "                    \n",
    "            scores_window.append(score)       # save most recent score\n",
    "            scores.append(score)              # save most recent score\n",
    "            writer.add_scalar(\"Average100\", np.mean(scores_window), frame)\n",
    "            output_history.append(np.mean(scores_window))\n",
    "            print('\\rEpisode {}\\tFrame {} \\tAverage Score: {:.2f}'.format(i_episode, frame, np.mean(scores_window)), end=\"\")\n",
    "            if i_episode % 100 == 0:\n",
    "                print('\\rEpisode {}\\tFrame {}\\tAverage Score: {:.2f}'.format(i_episode,frame, np.mean(scores_window)))\n",
    "            i_episode +=1 \n",
    "\n",
    "            state = env.reset()\n",
    "            state = state[0,:]\n",
    "            score = 0              \n",
    "\n",
    "    return output_history\n",
    "\n",
    "class DQN_Agent():\n",
    "    \"\"\"Interacts with and learns from the environment.\"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 state_size,\n",
    "                 action_size,\n",
    "                 layer_size,\n",
    "                 n_step,\n",
    "                 BATCH_SIZE,\n",
    "                 BUFFER_SIZE,\n",
    "                 LR,\n",
    "                 TAU,\n",
    "                 GAMMA,\n",
    "                 UPDATE_EVERY,\n",
    "                 device,\n",
    "                 seed):\n",
    "        \"\"\"Initialize an Agent object.\n",
    "        \n",
    "        Params\n",
    "        ======\n",
    "            state_size (int): dimension of each state\n",
    "            action_size (int): dimension of each action\n",
    "            layer_size (int): size of the hidden layer\n",
    "            BATCH_SIZE (int): size of the training batch\n",
    "            BUFFER_SIZE (int): size of the replay memory\n",
    "            LR (float): learning rate\n",
    "            TAU (float): tau for soft updating the network weights\n",
    "            GAMMA (float): discount factor\n",
    "            UPDATE_EVERY (int): update frequency\n",
    "            device (str): device that is used for the compute\n",
    "            seed (int): random seed\n",
    "        \"\"\"\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.seed = random.seed(seed)\n",
    "        self.device = device\n",
    "        self.TAU = TAU\n",
    "        self.GAMMA = GAMMA\n",
    "        self.UPDATE_EVERY = UPDATE_EVERY\n",
    "        self.BATCH_SIZE = BATCH_SIZE\n",
    "        self.Q_updates = 0\n",
    "        self.n_step = n_step\n",
    "        self.action = []\n",
    "\n",
    "        self.action_step = 30\n",
    "\n",
    "        # IQN-Network\n",
    "        self.qnetwork_local = IQN(state_size, action_size,layer_size, n_step, seed).to(device)\n",
    "        self.qnetwork_target = IQN(state_size, action_size,layer_size, n_step, seed).to(device)\n",
    "\n",
    "        \n",
    "        self.qnetwork_local.load_state_dict(torch.load('IQN-IQN.pth'))\n",
    "        self.qnetwork_target.load_state_dict(torch.load('IQN-IQN.pth'))\n",
    "        \n",
    "        print('self.qnetwork_local.parameters():{}'.format(self.qnetwork_local.parameters()))\n",
    "        \n",
    "        self.optimizer = optim.Adam(self.qnetwork_local.parameters(), lr=LR)\n",
    "        #print(self.qnetwork_local)\n",
    "        \n",
    "        # Replay memory\n",
    "        self.memory = ReplayBuffer(BUFFER_SIZE, BATCH_SIZE, self.device, seed, self.GAMMA, n_step)\n",
    "        \n",
    "        # Initialize time step (for updating every UPDATE_EVERY steps)\n",
    "        self.t_step = 0\n",
    "\n",
    "\n",
    "    def act(self, state, eps=0.):\n",
    "        \"\"\"Returns actions for given state as per current policy. Acting only every 4 frames!\n",
    "        \n",
    "        Params\n",
    "        ======\n",
    "            frame: to adjust epsilon\n",
    "            state (array_like): current state\n",
    "            \n",
    "        \"\"\"\n",
    "        #print(\"without np.array:{}\".format(state.shape))\n",
    "\n",
    "        state = np.array(state)\n",
    "\n",
    "        #print(\"this is the state space before torch.from_numpy:{}\".format(state.shape))\n",
    "        \n",
    "        state = torch.from_numpy(state).float().unsqueeze(0).to(self.device) #WHY?\n",
    "        \n",
    "        #print(\"this is the state space after torch.from_numpy:{}\".format(state.shape))\n",
    "        \n",
    "        \n",
    "        self.qnetwork_local.eval() #WHY?\n",
    "        \n",
    "        \n",
    "        with torch.no_grad():\n",
    "            #print(\"this is the state space:{}\".format(state.shape))\n",
    "            action_values = self.qnetwork_local.get_action(state) # 30 dimensions are coming back.\n",
    "            #print('action_value:{}'.format(action_values))\n",
    "            \n",
    "        #self.qnetwork_local.train()\n",
    "\n",
    "        # Epsilon-greedy action selection\n",
    "        action = np.argmax(action_values.cpu().data.numpy())\n",
    "        self.last_action = action\n",
    "        return action\n",
    "\n",
    "    def step(self, state, action, reward, next_state, done, writer):\n",
    "        # Save experience in replay memory\n",
    "        #print(\"to memory action:{},state:{},next_state\".format(action,state.shape,next_state.shape))\n",
    "        self.memory.add(state, action, reward, next_state, done)\n",
    "        \n",
    "        # Learn every UPDATE_EVERY time steps.\n",
    "        self.t_step = (self.t_step + 1) % self.UPDATE_EVERY\n",
    "        if self.t_step == 0:\n",
    "            # If enough samples are available in memory, get random subset and learn\n",
    "            if len(self.memory) > self.BATCH_SIZE:\n",
    "                \n",
    "                experiences = self.memory.sample()\n",
    "                #print(\"experiences:{}\".format(experiences))\n",
    "                loss = self.learn(experiences)\n",
    "                self.Q_updates += 1\n",
    "                writer.add_scalar(\"Q_loss\", loss, self.Q_updates)\n",
    "    \n",
    "\n",
    "    def learn(self, experiences):\n",
    "        \"\"\"Update value parameters using given batch of experience tuples.\n",
    "        Params\n",
    "        ======\n",
    "            experiences (Tuple[torch.Tensor]): tuple of (s, a, r, s', done) tuples \n",
    "            gamma (float): discount factor\n",
    "        \"\"\"\n",
    "        self.optimizer.zero_grad()\n",
    "        states, actions, rewards, next_states, dones = experiences\n",
    "        #print(\"learning states:{}, next_states:{}\".format(states.shape, next_states.shape))\n",
    "        # Get max predicted Q values (for next states) from target model\n",
    "        Q_targets_next, _ = self.qnetwork_target(next_states)\n",
    "        \n",
    "        #print(\"--Q_targets_next :{}\".format(Q_targets_next.shape))\n",
    "        \n",
    "        #print(\"---->Q_targets_next:{}\".format(Q_targets_next))\n",
    "        #print('------------------------------------')\n",
    "        #print(\"--Q_targets_next detach max:{}\".format(Q_targets_next.detach().max(2)))\n",
    "        \n",
    "        \n",
    "        \n",
    "        #print(\"--Q_targets_next.detach().max(2)[0].unsqueeze(1):{}\".format(Q_targets_next.detach().max(2)[0].unsqueeze(1)))\n",
    "        Q_targets_next = Q_targets_next.detach().max(2)[0].unsqueeze(1) # (batch_size, 1, N)\n",
    "        \n",
    "        # Compute Q targets for current states \n",
    "        Q_targets = rewards.unsqueeze(-1) + (self.GAMMA**self.n_step * Q_targets_next * (1. - dones.unsqueeze(-1)))\n",
    "        # Get expected Q values from local model\n",
    "        Q_expected, taus = self.qnetwork_local(states)\n",
    "        \n",
    "        #print(\"rewards:{}\".format(rewards.shape))\n",
    "        #print(\"Q_targets_Shape:{}\".format(Q_targets.shape))\n",
    "        #print(\"actions shape:{}\".format(actions.shape))\n",
    "        #print(\"Q_expected shape:{}\".format(Q_expected.shape))\n",
    "        #print(\"actions.unsqueeze(-1).shape:{}\".format(actions.unsqueeze(-1).shape))\n",
    "        #print(\"actions:{}\".format(actions))\n",
    "        #print(\"Q_expected:{}\".format(Q_expected))\n",
    "        #print(\"actions.unsqueeze(-1){}\".format(actions.unsqueeze(-1)))\n",
    "        Q_expected_2 = Q_expected.gather(2, actions.unsqueeze(-1))\n",
    "\n",
    "        #print(\"Q_expected.gather(2, actions.unsqueeze(-1):{}\".format(Q_expected_2.shape))\n",
    "        \n",
    "        Q_expected = Q_expected.gather(2, actions[0].unsqueeze(-1).expand(self.BATCH_SIZE, 8, 1))\n",
    "        #print(\"Final what we need Q_expected-----:{}\".format(Q_expected.shape))\n",
    "\n",
    "        # Quantile Huber loss\n",
    "        td_error = Q_targets - Q_expected\n",
    "        #print(\"td_error.shape:{}\".format(td_error.shape))\n",
    "        #print(\"Q_expected.shape:{}\".format(Q_expected.shape))\n",
    "        #print(\"td_error:{}\".format(td_error.shape))\n",
    "        assert td_error.shape == (self.BATCH_SIZE, 8, 8), \"wrong td error shape\"\n",
    "        huber_l = calculate_huber_loss(td_error, 1.0)\n",
    "        quantil_l = abs(taus -(td_error.detach() < 0).float()) * huber_l / 1.0\n",
    "        \n",
    "        loss = quantil_l.sum(dim=1).mean(dim=1) # , keepdim=True if per weights get multipl\n",
    "        loss = loss.mean()\n",
    "\n",
    "\n",
    "        # Minimize the loss\n",
    "        loss.backward()\n",
    "        #clip_grad_norm_(self.qnetwork_local.parameters(),1)\n",
    "        self.optimizer.step()\n",
    "\n",
    "        # ------------------- update target network ------------------- #\n",
    "        self.soft_update(self.qnetwork_local, self.qnetwork_target)\n",
    "        return loss.detach().cpu().numpy()            \n",
    "\n",
    "    def soft_update(self, local_model, target_model):\n",
    "        \"\"\"Soft update model parameters.\n",
    "        θ_target = τ*θ_local + (1 - τ)*θ_target\n",
    "        Params\n",
    "        ======\n",
    "            local_model (PyTorch model): weights will be copied from\n",
    "            target_model (PyTorch model): weights will be copied to\n",
    "            tau (float): interpolation parameter \n",
    "        \"\"\"\n",
    "        for target_param, local_param in zip(target_model.parameters(), local_model.parameters()):\n",
    "            target_param.data.copy_(self.TAU*local_param.data + (1.0-self.TAU)*target_param.data)\n",
    "\n",
    "\n",
    "def calculate_huber_loss(td_errors, k=1.0):\n",
    "    \"\"\"\n",
    "    Calculate huber loss element-wisely depending on kappa k.\n",
    "    \"\"\"\n",
    "    loss = torch.where(td_errors.abs() <= k, 0.5 * td_errors.pow(2), k * (td_errors.abs() - 0.5 * k))\n",
    "    #print('this is huber loss: {}'.format(loss.shape))\n",
    "    assert loss.shape == (td_errors.shape[0], 8, 8), \"huber loss has wrong shape\"\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(env,frames=1000, eps_fixed=False, eps_frames=1e6, min_eps=0.01):\n",
    "    \"\"\"Deep Q-Learning.\n",
    "    \n",
    "    Params\n",
    "    ======\n",
    "        n_episodes (int): maximum number of training episodes\n",
    "        max_t (int): maximum number of timesteps per episode\n",
    "        eps_start (float): starting value of epsilon, for epsilon-greedy action selection\n",
    "        eps_end (float): minimum value of epsilon\n",
    "        eps_decay (float): multiplicative factor (per episode) for decreasing epsilon\n",
    "    \"\"\"\n",
    "    scores = []                        # list containing scores from each episode\n",
    "    scores_window = deque(maxlen=100)  # last 100 scores\n",
    "    output_history = []\n",
    "    frame = 0\n",
    "    if eps_fixed:\n",
    "        eps = 0\n",
    "    else:\n",
    "        eps = 1\n",
    "    eps_start = 1\n",
    "    i_episode = 1\n",
    "    state = env.reset()\n",
    "    state = state[0,:]\n",
    "    #print(\"state space:{}\".format(state[0,:].shape))\n",
    "    score = 0                  \n",
    "    for frame in range(1, frames+1):\n",
    "        \n",
    "        if frame  == 0:\n",
    "            # inital state\n",
    "\n",
    "            initial = True\n",
    "        else:\n",
    "            # previous state\n",
    "            initial = False\n",
    "        #print('initial state:{}'.format(initial))\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        action = agent.act(state, eps) #TODO: getting one dimension back.\n",
    "        \n",
    "        \n",
    "        action = np.array([action])\n",
    "        \n",
    "        \n",
    "        next_state, reward, done, info = env_train.step([action]) #TODO: Wants a list of actions of size a\n",
    "\n",
    "        #print(\"env_trainNext State: {}\".format(next_state.shape))\n",
    "\n",
    "        next_state = next_state[0,:]\n",
    "        \n",
    "        agent.step(state, action, reward, next_state, done, writer)\n",
    "        \n",
    "\n",
    "        state = next_state\n",
    "        score += reward\n",
    "        # linear annealing to the min epsilon value until eps_frames and from there slowly decease epsilon to 0 until the end of training\n",
    "        if eps_fixed == False:\n",
    "            if frame < eps_frames:\n",
    "                eps = max(eps_start - (frame*(1/eps_frames)), min_eps)\n",
    "            else:\n",
    "                eps = max(min_eps - min_eps*((frame-eps_frames)/(frames-eps_frames)), 0.001)\n",
    "\n",
    "        # evaluation runs\n",
    "        if frame % 100000 == 0:\n",
    "            print(\"score: {}\".format(state))\n",
    "            print(\"score: {}\".format(score))\n",
    "            #print(\"state: {}\".format(state))\n",
    "            print(\"action:{}, Number:{}\".format(action,frame))\n",
    "            print(\"-------------------------\")\n",
    "        \n",
    "        if done:\n",
    "            \n",
    "                        \n",
    "            for k, v in agent.qnetwork_local.named_parameters():\n",
    "                print(k, v)\n",
    "                    \n",
    "            scores_window.append(score)       # save most recent score\n",
    "            scores.append(score)              # save most recent score\n",
    "            writer.add_scalar(\"Average100\", np.mean(scores_window), frame)\n",
    "            output_history.append(np.mean(scores_window))\n",
    "            print('\\rEpisode {}\\tFrame {} \\tAverage Score: {:.2f}'.format(i_episode, frame, np.mean(scores_window)), end=\"\")\n",
    "            if i_episode % 100 == 0:\n",
    "                print('\\rEpisode {}\\tFrame {}\\tAverage Score: {:.2f}'.format(i_episode,frame, np.mean(scores_window)))\n",
    "            i_episode +=1 \n",
    "\n",
    "            state = env.reset()\n",
    "            state = state[0,:]\n",
    "            score = 0              \n",
    "\n",
    "    return output_history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using  cpu\n",
      "Action Space: 3\n",
      "State Space: 19\n",
      "self.qnetwork_local.parameters():<generator object Module.parameters at 0x12884f290>\n",
      "[1000.           12.96428571   19.33         45.25          0.\n",
      "    0.            0.            0.            0.            0.\n",
      "  100.          100.          100.           66.66666667   66.66666667\n",
      "   66.66666667  100.          100.          100.        ]\n",
      "Finished\n",
      "[1736.345883638546, 171.08, 99.7, 296.35, 1.0, 0.0, 0.0, 0.704458676, 0.9974521855, 7.430046544, 53.53658448, 64.10281906, 69.84104584, -29.59465816, 83.40596863, 85.13097907, 5.534176773, 21.16098526, 42.62075004]\n",
      "end_total_asset:1907.4258836385459\n",
      "Sharpe:  0.41354148742284513\n",
      "[1000.           12.96428571   19.33         45.25          0.\n",
      "    0.            0.            0.            0.            0.\n",
      "  100.          100.          100.           66.66666667   66.66666667\n",
      "   66.66666667  100.          100.          100.        ]\n",
      "head.weight Parameter containing:\n",
      "tensor([[-0.0621, -0.1281, -0.0797,  ..., -0.0045, -0.0591,  0.0453],\n",
      "        [-0.0433, -0.1199, -0.1780,  ..., -0.1416,  0.0394, -0.0871],\n",
      "        [ 0.0085, -0.2395,  0.1082,  ...,  0.1440,  0.0872, -0.0686],\n",
      "        ...,\n",
      "        [-0.0132, -0.0623,  0.1501,  ..., -0.1129, -0.3518, -0.2184],\n",
      "        [-0.0115,  0.1036, -0.0838,  ..., -0.2925, -0.3023,  0.0135],\n",
      "        [ 0.0073,  0.1218, -0.0873,  ..., -0.3161, -0.1986, -0.0524]],\n",
      "       requires_grad=True)\n",
      "head.bias Parameter containing:\n",
      "tensor([ 1.9485e-01, -1.1048e-01,  1.6141e-02, -3.3372e-02, -2.3382e-01,\n",
      "         3.3151e-02, -6.1631e-02, -1.1028e-01, -8.9282e-03, -1.2598e-01,\n",
      "        -1.0695e-01, -3.4957e-02, -2.2171e-01, -2.6015e-01, -1.3065e-01,\n",
      "         3.2564e-02, -9.9179e-02, -2.6988e-01, -1.8251e-01, -9.3595e-02,\n",
      "        -2.1191e-01, -1.5608e-01, -4.7216e-02,  7.1587e-02,  4.5909e-02,\n",
      "        -2.1711e-01, -2.5396e-01,  1.0730e-02, -8.0654e-02, -1.7905e-01,\n",
      "        -5.7478e-02, -3.1105e-01,  9.9572e-03, -8.0423e-02,  7.9885e-02,\n",
      "        -1.1785e-01, -1.9143e-01,  1.3611e-01, -1.8191e-01, -3.2589e-01,\n",
      "        -1.1483e-01,  9.5248e-02, -2.3933e-01, -9.1926e-02, -3.8313e-01,\n",
      "        -1.5724e-01, -1.3669e-01, -1.6276e-01, -1.7414e-01, -3.3998e-01,\n",
      "        -9.0344e-02,  1.2716e-01, -5.7718e-02, -1.2600e-02, -2.2146e-01,\n",
      "        -5.2327e-02, -1.6304e-02,  5.2906e-02, -1.5699e-02,  4.3462e-02,\n",
      "         3.1778e-02, -3.0031e-01,  1.0355e-01, -1.6585e-01, -1.5302e-01,\n",
      "        -3.4500e-03, -9.5500e-02, -1.6956e-01, -4.0769e-02, -5.0359e-02,\n",
      "         3.3294e-03, -2.2133e-01, -3.3313e-01, -3.4810e-01, -3.3909e-01,\n",
      "        -2.0751e-01, -2.5857e-01,  3.7478e-02, -2.3197e-01, -2.0731e-01,\n",
      "        -2.0512e-01, -2.1146e-01,  1.6700e-02,  1.8849e-01,  1.1731e-02,\n",
      "        -1.4507e-01, -1.5304e-01, -2.0179e-01,  1.9812e-02,  1.3203e-01,\n",
      "        -2.4571e-01,  7.5497e-02, -2.3558e-01, -1.5698e-01,  8.0497e-03,\n",
      "        -3.1967e-01, -2.1594e-01,  1.5914e-01, -2.9397e-01, -2.5457e-01,\n",
      "        -6.5071e-02,  1.3945e-01,  1.0289e-01,  5.7299e-03, -2.4109e-01,\n",
      "        -2.3398e-01,  8.9583e-02,  6.1126e-02,  9.6327e-02,  1.6243e-02,\n",
      "        -1.5730e-01, -4.7098e-02, -2.0106e-01, -2.0255e-01,  1.2705e-01,\n",
      "        -6.5886e-02,  3.5091e-02,  1.0045e-02,  2.0357e-01,  3.3907e-02,\n",
      "        -2.4082e-01, -2.2305e-01, -1.1535e-01,  1.0811e-01, -5.1609e-02,\n",
      "        -1.6054e-02, -8.6598e-02, -1.7460e-01, -4.1115e-01, -4.4271e-02,\n",
      "        -1.3865e-01, -1.0014e-02,  6.4500e-02, -2.2930e-01,  1.0721e-02,\n",
      "        -3.4468e-02, -1.6325e-02,  6.1788e-02, -2.6810e-01, -1.9701e-01,\n",
      "        -2.5933e-01,  7.5740e-02, -1.2825e-02, -8.2108e-02, -4.4841e-02,\n",
      "        -2.9285e-01,  8.2573e-02, -2.9709e-01, -1.7700e-01, -2.2632e-01,\n",
      "        -9.3678e-02, -2.8030e-01, -2.4648e-01, -1.3980e-01,  1.4831e-01,\n",
      "         1.9955e-01, -1.9322e-01,  1.4142e-02,  6.7788e-02,  1.3388e-01,\n",
      "        -2.6710e-02, -6.8673e-03,  1.3205e-01, -2.1566e-02, -2.4790e-01,\n",
      "         5.7587e-02,  1.0266e-01,  1.2324e-01, -2.9533e-01,  2.9806e-02,\n",
      "         1.3684e-01, -6.0305e-02,  2.9617e-02, -1.5348e-01,  1.4694e-01,\n",
      "        -2.7013e-01, -9.9583e-02, -3.3368e-01, -1.0232e-02, -1.2107e-01,\n",
      "        -1.1843e-01,  7.1182e-02, -1.6219e-01,  1.0814e-01,  3.4672e-02,\n",
      "        -2.6038e-01, -1.6022e-01,  1.7862e-02, -2.5278e-01, -1.3883e-02,\n",
      "        -3.1160e-01, -1.7031e-01,  1.2527e-01, -8.9977e-02, -2.5412e-01,\n",
      "        -2.6240e-01, -1.2733e-01, -1.2471e-01, -9.9120e-02, -1.8780e-02,\n",
      "         1.0583e-01, -6.4217e-02, -2.9791e-02,  1.1218e-01, -1.4983e-01,\n",
      "         1.5676e-02,  7.0057e-02, -2.2804e-01, -2.8627e-01,  1.1459e-01,\n",
      "         8.7190e-02,  1.6588e-01, -3.2739e-01,  1.2182e-01, -8.5906e-02,\n",
      "        -2.8378e-01, -1.9407e-01, -6.8018e-02,  7.8266e-02,  9.1704e-02,\n",
      "        -1.6085e-01, -2.5115e-01, -2.2749e-01, -2.0978e-01, -5.7947e-02,\n",
      "        -1.8828e-01,  1.1380e-01, -1.0642e-01, -1.0728e-01, -2.7607e-01,\n",
      "         4.3353e-02,  7.1647e-03, -1.3145e-01, -1.9121e-01,  1.1094e-02,\n",
      "        -1.1574e-01, -3.9035e-02, -1.6007e-01,  9.2450e-02, -2.2380e-01,\n",
      "        -2.2897e-01, -4.9547e-02, -1.9808e-01, -3.0409e-01,  1.6381e-01,\n",
      "        -8.7075e-03, -1.0058e-02, -1.3763e-01, -2.1827e-01, -2.9200e-02,\n",
      "        -2.9355e-01, -6.6331e-02, -3.2165e-01,  4.3629e-02, -3.3457e-01,\n",
      "        -2.5748e-01, -1.4467e-01,  2.0191e-01, -2.3301e-01, -2.2387e-01,\n",
      "         2.6357e-01,  3.0380e-02, -1.5627e-01, -1.3314e-01, -9.5478e-02,\n",
      "        -2.9804e-01, -3.5818e-01,  4.7257e-02,  1.9024e-02, -1.5811e-01,\n",
      "        -3.4101e-01,  2.7686e-02,  7.1943e-02, -6.6023e-02,  2.2592e-04,\n",
      "        -1.9619e-02, -1.8743e-01,  1.2750e-01, -1.0242e-01, -1.7667e-01,\n",
      "        -1.8888e-01,  2.3775e-02, -2.3629e-01,  3.1953e-02, -2.1020e-01,\n",
      "        -3.3238e-01, -2.4413e-01, -6.9393e-03,  1.0109e-01, -3.1399e-01,\n",
      "         1.2448e-01, -3.4954e-01,  5.8502e-02, -1.7958e-01, -2.7971e-01,\n",
      "        -2.9755e-01,  1.3689e-02, -8.9920e-02,  7.1208e-02, -3.2786e-01,\n",
      "        -1.5478e-01,  6.4934e-02,  1.0064e-01, -5.1911e-02, -2.0791e-01,\n",
      "        -3.2104e-01, -2.8661e-01,  4.6723e-03,  1.7494e-01, -1.0811e-01,\n",
      "        -1.4268e-01, -6.2468e-02, -3.0791e-01,  9.9514e-03, -3.0439e-01,\n",
      "        -2.6048e-01, -2.7915e-01, -8.4266e-02,  5.9831e-02,  1.2743e-01,\n",
      "        -2.9773e-01, -5.9114e-02, -2.6999e-01, -5.4270e-02,  8.3400e-02,\n",
      "        -1.6397e-02, -2.1425e-01, -1.6616e-01, -2.8966e-02,  8.7446e-02,\n",
      "        -3.0123e-01,  5.4558e-02, -1.7959e-01, -1.0455e-01, -2.4162e-02,\n",
      "         1.1309e-01, -2.5936e-01, -3.0245e-01,  5.6373e-02, -6.2402e-02,\n",
      "         2.0912e-02, -8.3542e-02, -5.5175e-02,  1.0637e-01,  1.1338e-01,\n",
      "        -2.7584e-02, -1.7972e-02,  3.3329e-03, -1.1423e-01, -2.3918e-01,\n",
      "        -6.1329e-02, -8.9691e-02,  4.2100e-03, -1.8084e-01,  8.7170e-02,\n",
      "        -3.5240e-01, -1.9142e-01, -1.5609e-01, -2.6168e-01, -1.5590e-01,\n",
      "         9.3592e-02,  1.4753e-01,  1.8155e-02, -4.7359e-02, -1.4850e-01,\n",
      "        -3.0604e-02, -1.9644e-01, -3.3617e-01, -1.3926e-01,  1.1867e-01,\n",
      "        -6.4732e-02, -7.3420e-02, -1.3732e-01,  1.0533e-01,  9.8553e-02,\n",
      "         2.5833e-03,  7.8684e-02, -1.0687e-01,  3.0189e-02, -1.3567e-01,\n",
      "        -1.8769e-01,  4.1804e-03,  8.2516e-03, -3.0714e-01, -1.5758e-01,\n",
      "         1.8035e-02, -1.0615e-01,  1.3078e-01, -1.3009e-01, -2.4304e-01,\n",
      "         6.5958e-02,  1.0093e-01, -2.4882e-01, -2.9836e-02,  1.3038e-01,\n",
      "        -1.7993e-02, -1.4988e-01,  1.0954e-01,  1.5142e-01, -1.0881e-01,\n",
      "        -2.3498e-01, -2.5829e-01, -8.0853e-02, -2.9304e-01, -3.1948e-01,\n",
      "        -6.2819e-02, -1.2230e-01, -4.8917e-02, -1.3797e-01, -1.2737e-01,\n",
      "         6.1071e-02, -2.0829e-02, -1.7353e-01, -8.0553e-03,  7.6871e-02,\n",
      "        -3.2183e-01,  1.3646e-01,  1.1018e-01,  5.7409e-02, -6.7540e-02,\n",
      "        -1.8404e-01, -3.1859e-01, -7.8125e-02,  1.2112e-01, -2.0856e-01,\n",
      "        -9.2339e-03, -2.4599e-01, -1.0852e-01, -1.0788e-01, -2.4121e-01,\n",
      "        -3.3778e-01, -2.8463e-01, -6.8629e-02, -5.8570e-03, -3.2799e-02,\n",
      "         1.1333e-01, -2.0716e-02, -3.3844e-01, -1.3657e-01, -2.1122e-01,\n",
      "        -2.8867e-01, -9.4543e-02, -1.4960e-01,  1.5759e-01, -5.5884e-02,\n",
      "         8.0545e-02, -1.7074e-01,  1.3028e-01, -2.3115e-01,  8.1770e-02,\n",
      "        -1.3444e-01, -1.3534e-01, -1.2327e-01, -3.4841e-01, -3.0704e-03,\n",
      "        -1.6050e-01, -2.0519e-01, -9.0622e-02, -1.8193e-01,  1.9584e-02,\n",
      "         1.4679e-01, -5.2345e-02, -1.6343e-01, -2.3688e-01,  5.7781e-02,\n",
      "        -7.9199e-02, -3.1054e-01,  6.6473e-02, -2.4969e-01, -9.2882e-02,\n",
      "        -1.4949e-01, -1.5925e-02, -2.1473e-01, -2.4744e-01, -2.3581e-01,\n",
      "        -1.0384e-01, -3.7293e-01, -3.4633e-03,  2.5474e-02, -7.9479e-03,\n",
      "         9.9739e-02, -2.5266e-01, -4.6785e-02, -9.9103e-02, -2.6305e-01,\n",
      "        -2.0455e-01, -1.4643e-01, -3.2220e-01, -1.1888e-01, -2.1109e-01,\n",
      "         5.6667e-02, -1.1376e-01, -1.0971e-02, -2.1071e-02,  1.9226e-02,\n",
      "         1.4824e-01, -3.6124e-01,  1.1800e-02, -7.2391e-02,  1.7496e-01,\n",
      "         5.9320e-02,  4.6219e-02, -5.8551e-02,  7.0033e-02, -2.0956e-01,\n",
      "        -2.3801e-01, -8.3808e-02, -2.1567e-01,  1.3744e-01, -2.1786e-01,\n",
      "        -2.6374e-01,  8.9746e-03], requires_grad=True)\n",
      "cos_embedding.weight Parameter containing:\n",
      "tensor([[-0.0795,  0.1664, -0.0029,  ..., -0.0325, -0.0006, -0.0277],\n",
      "        [-0.0514, -0.1351,  0.0274,  ...,  0.0703,  0.0387,  0.0112],\n",
      "        [ 0.0398, -0.0719,  0.0489,  ..., -0.0451, -0.0632, -0.0276],\n",
      "        ...,\n",
      "        [-0.0902, -0.0204,  0.0590,  ..., -0.0078,  0.0007, -0.0036],\n",
      "        [-0.1781, -0.0787,  0.0258,  ...,  0.0089, -0.0706, -0.0523],\n",
      "        [-0.1813, -0.0629,  0.0108,  ...,  0.0315, -0.0680,  0.0329]],\n",
      "       requires_grad=True)\n",
      "cos_embedding.bias Parameter containing:\n",
      "tensor([-1.1569e-01, -6.8529e-02, -1.5622e-01, -1.2770e-01, -5.1965e-02,\n",
      "        -2.9231e-01, -1.6529e-01,  1.4859e-01,  9.2422e-02, -1.1830e-01,\n",
      "        -1.4609e-01,  2.8645e-02, -4.3776e-02, -1.3911e-01,  7.4696e-04,\n",
      "        -8.0553e-02,  6.3612e-02, -5.7417e-02, -6.0545e-03, -1.5309e-01,\n",
      "        -2.1812e-01, -1.8653e-01, -1.4124e-02, -1.7989e-01,  8.7088e-02,\n",
      "        -1.0181e-01, -7.9987e-02, -1.0638e-01, -1.4393e-01, -8.9540e-02,\n",
      "        -9.7645e-04, -2.0640e-02, -5.1977e-02, -1.9855e-01, -1.5504e-01,\n",
      "        -2.6423e-02,  2.0975e-02,  4.7234e-03, -2.0705e-01, -1.1286e-01,\n",
      "         2.0770e-02,  1.2770e-02, -1.6238e-01, -3.9354e-02, -4.8741e-02,\n",
      "        -1.7540e-03, -9.5606e-02, -6.3964e-02, -1.1359e-01, -3.9409e-02,\n",
      "         5.0155e-03, -1.8362e-01, -1.6925e-01, -1.8942e-01,  5.1803e-02,\n",
      "        -5.4757e-02,  4.9448e-02, -1.1814e-01, -6.1997e-02, -2.1949e-01,\n",
      "        -1.0035e-01, -2.0884e-01, -2.4079e-01, -1.6036e-01, -1.7864e-01,\n",
      "         4.7006e-02, -1.1903e-01, -1.5833e-01, -1.6162e-01,  2.3202e-02,\n",
      "        -3.6038e-02, -8.7208e-02, -1.2468e-01, -1.3681e-01, -2.7759e-01,\n",
      "        -2.1554e-01, -2.3761e-02, -1.0237e-01, -1.7411e-01, -1.0489e-01,\n",
      "         4.9826e-02, -1.1824e-01, -1.0311e-01,  1.1653e-01, -5.9822e-02,\n",
      "         9.3057e-02,  4.9384e-02, -2.4465e-01,  5.5064e-02, -6.1816e-02,\n",
      "         9.3548e-02, -5.0311e-02, -1.4291e-01,  2.5256e-04, -3.2717e-02,\n",
      "        -1.8486e-01,  4.3349e-02,  3.2723e-02, -1.3065e-01, -9.4810e-02,\n",
      "        -1.1855e-01, -1.9680e-01, -4.0142e-02, -1.5384e-01, -1.8244e-01,\n",
      "        -9.3090e-02, -1.3936e-02, -1.6262e-01, -7.6478e-02,  2.7502e-02,\n",
      "        -5.6698e-02, -1.8260e-01, -1.2191e-01, -9.7610e-03, -1.4649e-01,\n",
      "         1.6560e-02, -1.1285e-01, -1.2730e-01, -4.4219e-02, -4.7200e-03,\n",
      "         2.4853e-02, -6.7783e-02, -1.4392e-02, -1.0938e-01, -1.1069e-01,\n",
      "        -8.0861e-02, -7.8625e-03, -1.8158e-02, -8.9387e-02, -5.2455e-02,\n",
      "         3.8890e-02, -1.5334e-01, -2.0684e-03, -5.5234e-02,  2.0315e-02,\n",
      "         4.0941e-02,  1.8572e-02, -5.5647e-02, -1.3292e-02, -8.0443e-02,\n",
      "         1.0950e-01, -1.9122e-01, -1.9229e-01, -1.2962e-01, -2.1963e-01,\n",
      "         7.6193e-02, -1.1430e-01,  2.1361e-02, -1.3630e-01, -5.5462e-02,\n",
      "        -3.6512e-03, -9.7632e-02, -2.5865e-02, -7.8671e-02, -3.9580e-02,\n",
      "        -9.2948e-02, -2.2185e-01,  1.2044e-01, -1.5534e-01, -3.1115e-02,\n",
      "        -1.8036e-01, -3.1189e-01,  4.2299e-02, -5.7552e-02,  7.6032e-03,\n",
      "        -2.8943e-02, -2.0826e-01, -6.6079e-02,  3.2265e-02, -8.1135e-02,\n",
      "        -2.5451e-01, -2.0748e-01, -6.1132e-02, -2.9702e-02, -2.1926e-01,\n",
      "         1.9975e-02, -5.8645e-02, -1.1482e-01, -1.5228e-01,  7.8902e-02,\n",
      "        -1.8275e-01,  1.1425e-02, -1.0980e-01, -6.7975e-02, -7.7232e-02,\n",
      "        -3.5656e-02, -7.9963e-02, -1.6917e-01, -6.4583e-02, -1.5600e-01,\n",
      "        -1.0298e-01,  3.9242e-02, -1.4346e-01, -1.5587e-01, -4.3627e-03,\n",
      "        -5.7963e-02, -3.6500e-03, -2.8820e-02, -1.0181e-01, -1.0290e-01,\n",
      "        -1.4007e-01, -1.2930e-01, -3.1805e-02, -1.5541e-01, -2.4068e-01,\n",
      "        -1.5120e-01,  4.5524e-02, -1.2330e-01, -8.8106e-02, -3.8798e-02,\n",
      "         9.6524e-02,  4.3881e-02, -6.2809e-02,  3.8371e-02,  2.5711e-02,\n",
      "         1.4084e-02, -1.1537e-01, -5.0373e-02, -4.7079e-02, -1.0368e-01,\n",
      "         1.9136e-02, -1.7679e-01, -3.0144e-02,  3.3658e-02, -2.1950e-01,\n",
      "        -8.8840e-02, -1.0467e-01, -9.3497e-03, -7.3569e-02, -1.8955e-01,\n",
      "        -1.6690e-01,  3.0113e-02, -8.7159e-02, -9.6443e-02, -3.6279e-02,\n",
      "         2.3639e-02, -3.1858e-03, -2.2829e-01,  6.3241e-02, -1.0163e-01,\n",
      "        -1.4345e-01, -8.5509e-02, -1.8805e-01, -1.2800e-01, -2.0190e-01,\n",
      "        -5.2704e-02, -1.8274e-01, -1.1115e-01,  1.2136e-02,  3.2133e-03,\n",
      "        -4.6610e-02, -7.3717e-02, -7.4817e-02, -1.8244e-01, -1.7114e-01,\n",
      "        -8.4524e-02, -5.0519e-02, -2.3157e-01, -8.9444e-02, -1.7333e-01,\n",
      "         2.4814e-01,  6.4662e-02, -8.7058e-02, -8.1152e-02, -9.4288e-02,\n",
      "        -8.2782e-02, -2.4687e-01, -8.8051e-02, -1.7229e-01, -1.2726e-01,\n",
      "        -1.3445e-01, -1.3653e-01,  4.0970e-02, -1.6563e-01,  8.1679e-02,\n",
      "        -1.4988e-01, -2.0409e-01, -2.8765e-02, -1.6034e-01, -2.1072e-01,\n",
      "        -3.7079e-02, -1.4283e-01, -3.8230e-02,  5.4818e-02, -8.6378e-02,\n",
      "        -1.3011e-01, -1.9759e-01, -7.0731e-02, -1.3130e-01, -1.7148e-01,\n",
      "         1.0974e-02, -6.3034e-02,  1.0715e-02, -1.8561e-02, -3.0854e-01,\n",
      "        -7.3605e-02, -9.2427e-02, -1.3929e-01, -2.7330e-02, -7.3296e-02,\n",
      "        -1.9107e-01,  2.3043e-02, -8.3214e-02, -9.7840e-02,  5.4270e-02,\n",
      "        -1.4807e-01, -2.0674e-01, -1.1427e-01, -1.3702e-01, -2.3268e-01,\n",
      "        -1.3886e-01,  4.7035e-02, -4.5802e-02, -1.7553e-01, -1.4021e-01,\n",
      "        -1.3810e-01, -2.0273e-01, -6.7414e-02, -1.8487e-01, -7.1816e-02,\n",
      "        -1.2979e-01, -8.9659e-02, -1.3804e-01, -7.2055e-02, -2.4745e-02,\n",
      "        -2.3740e-01, -1.3898e-01, -1.9215e-01, -2.4623e-01, -1.7717e-01,\n",
      "        -1.0021e-01, -1.5345e-01, -1.9507e-02, -7.9349e-02, -2.2588e-01,\n",
      "        -1.5245e-01, -6.0632e-02, -5.3705e-02,  2.2322e-02, -3.0178e-02,\n",
      "        -2.7085e-02, -9.8254e-02, -2.2161e-01, -1.1795e-01, -4.4188e-02,\n",
      "        -1.3832e-01, -1.7240e-02, -1.5625e-01, -5.3547e-02, -1.6605e-01,\n",
      "        -1.8683e-01,  6.8673e-02, -2.9828e-03, -1.7660e-01,  1.8653e-01,\n",
      "        -5.9102e-02, -9.7465e-02, -1.3532e-01,  1.4626e-01, -3.2110e-02,\n",
      "        -7.6306e-02,  8.7175e-02, -1.2231e-01, -4.6431e-02,  4.7338e-02,\n",
      "        -3.8011e-02, -9.1745e-02, -5.3961e-02, -5.3777e-02, -1.2233e-01,\n",
      "        -9.0404e-02, -2.6922e-01, -4.0364e-02,  5.4465e-02, -3.7192e-02,\n",
      "        -1.4890e-01, -8.2713e-02, -4.6608e-02, -9.8645e-02, -9.5383e-02,\n",
      "        -8.0782e-02, -6.5622e-02, -2.2027e-01, -1.0581e-01,  4.1645e-02,\n",
      "         2.5535e-02, -4.0823e-02,  2.6267e-02, -5.7916e-02, -1.4332e-01,\n",
      "        -1.5676e-01, -7.4620e-02, -1.5914e-01, -1.0249e-01, -6.3082e-02,\n",
      "        -6.3497e-02, -1.1978e-01, -1.5542e-01, -1.3166e-02, -1.7311e-03,\n",
      "        -1.5648e-01,  1.3987e-01,  9.3832e-02, -5.6052e-02, -2.3952e-01,\n",
      "        -1.4521e-01, -1.3922e-01, -2.2567e-01, -1.4949e-01, -1.8510e-01,\n",
      "         2.4903e-02, -8.4197e-02, -2.0159e-01, -2.2216e-02, -1.3207e-01,\n",
      "        -8.7653e-02, -1.8769e-01, -1.0480e-01, -5.8161e-02, -2.1764e-01,\n",
      "        -1.6250e-01, -6.3693e-02, -1.4072e-01,  6.5421e-02, -2.8354e-01,\n",
      "        -7.3004e-02, -1.0011e-04, -4.9917e-03, -7.9252e-02, -1.0967e-02,\n",
      "         6.7170e-03, -1.0084e-01, -3.1990e-02, -1.5787e-01, -1.7376e-01,\n",
      "         1.2470e-02, -1.2487e-02, -1.6903e-01, -1.9462e-02, -6.4136e-02,\n",
      "        -1.3653e-01, -3.5671e-02, -9.3926e-02, -2.8484e-02, -1.2269e-01,\n",
      "        -2.7283e-02, -7.0023e-02, -1.8862e-01,  2.6506e-02, -5.5247e-02,\n",
      "        -1.6038e-01,  2.8407e-01, -8.8103e-02, -5.9592e-02, -1.5023e-01,\n",
      "        -1.0845e-01,  6.6242e-03, -9.9156e-02, -3.0226e-02, -5.6623e-03,\n",
      "        -1.3758e-01, -1.4962e-01, -1.2148e-02, -5.5180e-02, -2.5326e-01,\n",
      "        -8.8309e-03, -1.5752e-01, -3.6037e-02, -7.7359e-02, -6.4692e-02,\n",
      "        -1.3687e-01,  1.1885e-02,  3.7043e-02, -1.7012e-01, -9.2201e-02,\n",
      "        -2.1716e-01, -1.2003e-01, -2.0598e-01, -1.7304e-01, -1.1946e-01,\n",
      "        -1.1159e-01, -1.0065e-01, -9.7807e-02, -2.2289e-03, -1.6870e-01,\n",
      "        -1.3257e-01,  7.0254e-02, -1.7219e-01, -2.8784e-02, -1.9219e-01,\n",
      "         1.1082e-01, -1.5919e-01,  3.1351e-01, -3.8534e-02,  4.5409e-02,\n",
      "         1.0609e-02, -1.8603e-02, -1.3365e-01, -1.3559e-01, -8.0603e-02,\n",
      "         1.6025e-02, -3.6988e-02, -2.8149e-01,  4.6573e-02, -2.1457e-02,\n",
      "        -1.6069e-01,  1.3292e-01, -7.7280e-02, -4.7010e-03, -1.0886e-01,\n",
      "        -2.0085e-01, -4.1796e-03], requires_grad=True)\n",
      "ff_1.weight Parameter containing:\n",
      "tensor([[-0.0111, -0.0439,  0.0167,  ..., -0.0024, -0.0547, -0.0023],\n",
      "        [-0.0123,  0.0179, -0.0188,  ...,  0.0105, -0.0466,  0.0201],\n",
      "        [-0.0316, -0.0395, -0.0227,  ..., -0.0447, -0.0021, -0.0065],\n",
      "        ...,\n",
      "        [ 0.0098, -0.0070,  0.0229,  ...,  0.0172,  0.0152,  0.0096],\n",
      "        [-0.0348, -0.0354,  0.0176,  ..., -0.0066, -0.0211,  0.0095],\n",
      "        [-0.0955, -0.0447, -0.0009,  ...,  0.0080, -0.0395, -0.0125]],\n",
      "       requires_grad=True)\n",
      "ff_1.bias Parameter containing:\n",
      "tensor([-4.9730e-02, -4.7281e-02, -8.8253e-02, -7.5452e-02, -4.9847e-02,\n",
      "        -9.2356e-02, -8.8763e-02, -4.4466e-03,  5.8943e-04, -4.1201e-02,\n",
      "        -6.5868e-02, -3.3090e-02, -1.0423e-01, -1.1392e-01, -1.1058e-01,\n",
      "        -8.3730e-02, -6.6694e-02, -5.7649e-04,  1.4256e-02, -1.1720e-01,\n",
      "        -4.5624e-02, -9.7564e-02, -2.2348e-02, -5.2470e-02, -6.5674e-02,\n",
      "        -9.0691e-02, -7.0226e-02, -6.7446e-02, -1.0207e-01, -7.3896e-02,\n",
      "        -9.8033e-02, -8.1015e-02, -3.2422e-02, -1.0306e-01, -6.5321e-02,\n",
      "        -6.2496e-02, -1.2154e-01, -2.1360e-02, -5.1726e-02, -7.5951e-02,\n",
      "        -5.1474e-02, -6.4854e-02, -1.0041e-02, -6.0550e-02, -5.7102e-02,\n",
      "         1.1759e-02, -5.9693e-02, -5.5161e-02, -6.5343e-02, -1.0517e-01,\n",
      "        -1.1984e-01, -1.4606e-02, -3.7967e-02, -5.9593e-02, -8.6352e-02,\n",
      "        -6.1844e-03, -4.3407e-02, -1.0230e-01, -1.3809e-02, -2.3353e-02,\n",
      "        -9.7271e-02, -1.3752e-01, -6.7916e-02, -8.1824e-02, -5.9069e-02,\n",
      "        -9.4381e-02, -1.0926e-01, -4.9680e-02, -7.4197e-02, -3.7931e-02,\n",
      "        -7.5026e-02, -5.3848e-02, -6.9606e-02, -4.8087e-03, -1.3911e-02,\n",
      "        -6.5377e-02, -4.1042e-02, -7.3419e-02, -3.9590e-02, -4.2007e-02,\n",
      "        -5.6734e-02, -5.8707e-02, -1.0365e-01, -3.9567e-02, -4.2732e-02,\n",
      "        -8.9803e-02, -3.0366e-02, -9.1460e-02, -6.9892e-02, -3.3322e-04,\n",
      "        -2.7431e-02, -7.5853e-02, -7.1066e-02, -5.6012e-02, -4.4923e-02,\n",
      "        -1.0784e-01, -1.1195e-02, -6.8582e-02, -3.8211e-02, -7.5153e-02,\n",
      "        -1.6720e-02, -2.8711e-02, -8.3005e-02, -9.6294e-02, -2.9333e-02,\n",
      "        -3.5750e-02, -8.7792e-02, -8.7527e-02, -9.2940e-02, -2.3110e-02,\n",
      "        -2.6232e-02, -7.0821e-02, -7.3930e-02, -6.4391e-02, -1.3147e-02,\n",
      "        -3.3142e-02, -1.1733e-01, -9.4237e-02, -1.2871e-01, -3.2566e-02,\n",
      "        -5.2310e-02, -1.3369e-01, -2.0894e-03, -3.4158e-02,  2.4776e-02,\n",
      "        -5.8472e-02, -1.0786e-01, -1.2079e-01,  6.1545e-03, -9.7759e-02,\n",
      "        -7.2647e-03, -6.3721e-02, -8.1800e-03, -4.1919e-02, -7.7188e-02,\n",
      "        -7.0248e-02, -8.4916e-02, -6.6009e-02, -4.8669e-02, -1.8305e-03,\n",
      "        -7.7926e-02, -7.3622e-02, -8.6682e-02, -8.1480e-02, -6.0722e-02,\n",
      "        -2.1055e-02, -1.4293e-01, -9.0514e-02, -4.8318e-02, -7.8942e-02,\n",
      "        -2.3981e-02, -1.5920e-02, -1.3872e-01, -8.7727e-03, -5.5322e-02,\n",
      "        -8.8069e-02, -1.3764e-01, -1.0214e-01, -1.8153e-02, -8.3851e-02,\n",
      "         2.7703e-02, -6.0074e-02, -4.4750e-02, -9.7228e-02, -6.6371e-02,\n",
      "        -9.7611e-03, -2.2129e-02, -1.4001e-02, -2.7313e-02, -2.4279e-02,\n",
      "        -8.5229e-02, -1.1019e-01, -2.2167e-02, -3.6578e-02, -8.1382e-02,\n",
      "        -5.3769e-02, -1.1116e-01, -8.4923e-02, -9.2939e-02,  1.5609e-02,\n",
      "         2.6569e-02, -1.0669e-01, -7.2565e-02, -1.2411e-01, -3.9094e-02,\n",
      "        -5.6155e-02, -5.0869e-02, -1.3681e-01, -8.6645e-02, -8.3700e-02,\n",
      "        -3.0106e-02, -9.1332e-02, -3.1931e-02, -1.1915e-02, -4.6834e-02,\n",
      "        -1.3868e-01, -2.5915e-02, -7.3056e-02, -5.8069e-02, -5.8650e-02,\n",
      "        -1.0733e-02, -7.1379e-02, -3.0507e-03, -5.2748e-02, -9.6890e-02,\n",
      "        -4.4654e-02, -2.1453e-02, -5.4882e-02, -9.5361e-02, -4.5626e-02,\n",
      "        -3.6807e-02, -6.8420e-02, -6.5413e-02, -7.4815e-02, -6.1727e-02,\n",
      "        -2.8028e-02, -3.5678e-02, -4.1944e-02,  2.5252e-02, -1.2391e-01,\n",
      "        -5.4688e-02, -6.1254e-02, -5.6500e-02, -7.5391e-02, -1.1244e-01,\n",
      "        -2.9993e-02, -3.6969e-02, -1.0955e-01, -6.9621e-02, -7.4441e-02,\n",
      "         3.8960e-03, -1.0118e-01, -1.4880e-01, -7.3674e-02, -5.8353e-02,\n",
      "        -7.5447e-02, -4.2217e-02, -9.3453e-02, -2.1039e-02, -1.7823e-02,\n",
      "         2.1460e-02, -6.9059e-02, -1.0310e-01, -6.9486e-02, -7.1635e-02,\n",
      "        -7.4597e-02, -7.6676e-02, -3.1382e-02, -5.3770e-02, -1.0150e-01,\n",
      "        -2.6335e-04, -1.1017e-02, -3.7435e-02, -7.9023e-03,  6.4193e-03,\n",
      "        -5.5044e-02, -3.3832e-02, -1.5470e-01, -6.6871e-02, -1.0037e-01,\n",
      "        -3.4851e-02, -7.1717e-02, -8.0892e-02, -4.6843e-02, -1.0445e-01,\n",
      "        -5.1153e-02, -4.5551e-02, -6.4295e-02, -8.9537e-02, -5.2991e-02,\n",
      "        -1.1966e-01, -7.8747e-02, -8.8091e-02, -1.2655e-03, -7.0432e-02,\n",
      "        -1.3981e-01, -1.4772e-01, -8.3395e-02, -9.3461e-02, -1.0295e-01,\n",
      "        -2.8328e-02, -5.3309e-02, -1.9920e-02, -3.2957e-02, -6.3867e-02,\n",
      "        -4.6606e-02, -5.0636e-02, -7.6311e-02, -1.0213e-02, -1.0758e-01,\n",
      "        -7.4966e-02, -5.6983e-02, -5.3532e-02, -1.4437e-02, -1.1636e-01,\n",
      "        -1.1328e-01, -8.9648e-02, -2.9984e-03, -7.0777e-02, -9.4368e-02,\n",
      "        -5.4963e-02, -9.2259e-02, -1.2516e-02, -5.0904e-02, -6.9229e-02,\n",
      "        -7.0682e-02, -1.0201e-01, -2.0265e-03, -3.6202e-02, -2.8805e-02,\n",
      "        -8.5300e-02, -5.5067e-02, -6.6248e-02, -1.0962e-01, -6.3131e-02,\n",
      "        -9.3447e-02, -1.3412e-01, -5.7369e-02, -1.0616e-01, -3.6016e-02,\n",
      "        -6.1172e-02, -4.6916e-02, -1.1191e-01, -4.7270e-02, -1.2512e-01,\n",
      "         1.9878e-02, -7.7593e-02, -2.9358e-02, -7.4108e-02, -1.4441e-01,\n",
      "        -8.1234e-02, -4.8259e-02, -2.6610e-02, -5.2713e-02, -1.1038e-01,\n",
      "        -7.8492e-02,  1.8667e-02, -5.5963e-02, -2.9011e-03, -6.9517e-02,\n",
      "        -1.4178e-02, -8.6119e-02, -8.4808e-02, -9.8407e-02, -7.9367e-02,\n",
      "        -4.9783e-02, -1.0881e-01, -3.6448e-02, -2.5957e-02, -1.0180e-01,\n",
      "        -1.9849e-02, -6.2833e-02, -6.4528e-02, -1.1729e-01, -8.4336e-02,\n",
      "        -3.0788e-04, -3.6531e-02, -9.0353e-02,  3.0143e-02, -1.1578e-01,\n",
      "        -1.1004e-01, -2.6858e-02, -5.6998e-02, -8.5879e-02, -4.6298e-02,\n",
      "        -5.2107e-02, -2.7640e-02, -8.7641e-02, -7.3840e-02, -9.9896e-02,\n",
      "        -1.2174e-01, -5.6385e-02, -4.9703e-02, -6.1322e-03, -6.8209e-02,\n",
      "        -9.3379e-02, -1.0825e-01, -8.2514e-02, -5.5571e-02, -1.0223e-01,\n",
      "        -9.7800e-02, -7.8374e-02, -6.4511e-02, -7.5661e-02, -1.5773e-02,\n",
      "        -5.6414e-02, -5.4910e-02, -4.1782e-02, -9.4877e-02, -1.0925e-01,\n",
      "        -1.1881e-01, -9.4369e-02, -3.4713e-02, -9.3728e-02, -5.7718e-02,\n",
      "        -4.7434e-02, -7.2100e-02, -3.6045e-02, -4.2829e-02, -7.9948e-02,\n",
      "        -1.0415e-01, -5.4262e-02, -6.8546e-02, -2.9479e-02, -6.1406e-02,\n",
      "        -2.8741e-02, -1.0465e-01, -5.9856e-02, -8.6037e-02,  9.8781e-03,\n",
      "        -1.0879e-01, -6.7978e-02, -5.2641e-02, -9.3707e-02, -1.2776e-01,\n",
      "        -5.4579e-02, -9.7660e-02, -4.8823e-02, -6.0936e-02, -1.1110e-01,\n",
      "        -2.0136e-03, -6.5965e-02, -1.3175e-02, -7.2792e-02, -6.4736e-02,\n",
      "        -8.2502e-02, -3.0606e-02, -9.4231e-02, -1.8043e-02, -1.1226e-01,\n",
      "        -4.6759e-02, -1.8502e-02, -8.3551e-02,  2.5647e-02, -4.3823e-02,\n",
      "        -1.4607e-01, -8.8639e-02, -6.4984e-02, -5.0067e-02, -1.0073e-01,\n",
      "        -2.6063e-02,  1.8059e-02, -3.9380e-03, -6.2056e-02, -5.0267e-02,\n",
      "        -5.8794e-02, -1.9403e-02, -6.9359e-02, -4.7010e-02,  1.1944e-02,\n",
      "         2.7380e-03, -1.1230e-01, -8.8373e-02, -9.2220e-02,  9.2673e-03,\n",
      "        -8.1982e-02, -9.9717e-02, -3.3061e-02, -3.9354e-02, -6.6404e-02,\n",
      "        -4.6010e-02, -9.5264e-02, -8.9320e-02, -9.6405e-02, -3.8096e-02,\n",
      "        -5.7505e-02, -5.0955e-02, -4.9666e-02, -1.7452e-01, -8.2577e-02,\n",
      "        -3.3688e-02, -5.4527e-02, -5.7603e-02, -9.9968e-02, -1.3542e-01,\n",
      "        -2.9958e-02, -7.6050e-02, -7.5882e-02,  2.8902e-01, -2.2292e-02,\n",
      "        -1.1601e-01,  2.9651e-02, -5.4982e-02, -8.0087e-02, -9.1269e-02,\n",
      "        -9.1825e-02, -2.6451e-02, -8.7603e-02, -2.4578e-02, -1.0710e-01,\n",
      "        -4.5958e-02, -1.0401e-01, -1.3284e-01, -8.4913e-02, -2.7860e-02,\n",
      "        -7.6902e-02, -1.1959e-01, -5.5176e-02, -5.2642e-02, -7.7102e-02,\n",
      "         5.6578e-03, -1.0394e-01, -9.0094e-02, -6.9083e-02, -9.2223e-02,\n",
      "        -3.2458e-02, -2.4637e-02, -4.5780e-02, -3.5477e-04, -9.1103e-02,\n",
      "        -9.0020e-02,  6.5961e-02], requires_grad=True)\n",
      "ff_2.weight Parameter containing:\n",
      "tensor([[-0.0146,  0.0274,  0.0080,  ..., -0.0005, -0.0237, -0.0147],\n",
      "        [-0.0090, -0.0311,  0.0173,  ..., -0.0242, -0.0141,  0.0055],\n",
      "        [-0.0065,  0.0012, -0.0062,  ...,  0.0316, -0.0151,  0.0211]],\n",
      "       requires_grad=True)\n",
      "ff_2.bias Parameter containing:\n",
      "tensor([0.0081, 0.0083, 0.0081], requires_grad=True)\n",
      "Episode 1\tFrame 6793 \tAverage Score: 0.09[1000.           12.96428571   19.33         45.25          0.\n",
      "    0.            0.            0.            0.            0.\n",
      "  100.          100.          100.           66.66666667   66.66666667\n",
      "   66.66666667  100.          100.          100.        ]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished\n",
      "[656.0041158257853, 171.08, 99.7, 296.35, 5.0, 3.0, 4.0, 0.704458676, 0.9974521855, 7.430046544, 53.53658448, 64.10281906, 69.84104584, -29.59465816, 83.40596863, 85.13097907, 5.534176773, 21.16098526, 42.62075004]\n",
      "end_total_asset:2995.9041158257855\n",
      "Sharpe:  0.5138147833036429\n",
      "[1000.           12.96428571   19.33         45.25          0.\n",
      "    0.            0.            0.            0.            0.\n",
      "  100.          100.          100.           66.66666667   66.66666667\n",
      "   66.66666667  100.          100.          100.        ]\n",
      "head.weight Parameter containing:\n",
      "tensor([[-0.0621, -0.1281, -0.0797,  ..., -0.0045, -0.0591,  0.0453],\n",
      "        [-0.0433, -0.1199, -0.1780,  ..., -0.1416,  0.0394, -0.0871],\n",
      "        [ 0.0085, -0.2395,  0.1082,  ...,  0.1440,  0.0872, -0.0686],\n",
      "        ...,\n",
      "        [-0.0132, -0.0623,  0.1501,  ..., -0.1129, -0.3518, -0.2184],\n",
      "        [-0.0115,  0.1036, -0.0838,  ..., -0.2925, -0.3023,  0.0135],\n",
      "        [ 0.0073,  0.1218, -0.0873,  ..., -0.3161, -0.1986, -0.0524]],\n",
      "       requires_grad=True)\n",
      "head.bias Parameter containing:\n",
      "tensor([ 1.9485e-01, -1.1048e-01,  1.6141e-02, -3.3372e-02, -2.3382e-01,\n",
      "         3.3151e-02, -6.1631e-02, -1.1028e-01, -8.9282e-03, -1.2598e-01,\n",
      "        -1.0695e-01, -3.4957e-02, -2.2171e-01, -2.6015e-01, -1.3065e-01,\n",
      "         3.2564e-02, -9.9179e-02, -2.6988e-01, -1.8251e-01, -9.3595e-02,\n",
      "        -2.1191e-01, -1.5608e-01, -4.7216e-02,  7.1587e-02,  4.5909e-02,\n",
      "        -2.1711e-01, -2.5396e-01,  1.0730e-02, -8.0654e-02, -1.7905e-01,\n",
      "        -5.7478e-02, -3.1105e-01,  9.9572e-03, -8.0423e-02,  7.9885e-02,\n",
      "        -1.1785e-01, -1.9143e-01,  1.3611e-01, -1.8191e-01, -3.2589e-01,\n",
      "        -1.1483e-01,  9.5248e-02, -2.3933e-01, -9.1926e-02, -3.8313e-01,\n",
      "        -1.5724e-01, -1.3669e-01, -1.6276e-01, -1.7414e-01, -3.3998e-01,\n",
      "        -9.0344e-02,  1.2716e-01, -5.7718e-02, -1.2600e-02, -2.2146e-01,\n",
      "        -5.2327e-02, -1.6304e-02,  5.2906e-02, -1.5699e-02,  4.3462e-02,\n",
      "         3.1778e-02, -3.0031e-01,  1.0355e-01, -1.6585e-01, -1.5302e-01,\n",
      "        -3.4500e-03, -9.5500e-02, -1.6956e-01, -4.0769e-02, -5.0359e-02,\n",
      "         3.3294e-03, -2.2133e-01, -3.3313e-01, -3.4810e-01, -3.3909e-01,\n",
      "        -2.0751e-01, -2.5857e-01,  3.7478e-02, -2.3197e-01, -2.0731e-01,\n",
      "        -2.0512e-01, -2.1146e-01,  1.6700e-02,  1.8849e-01,  1.1731e-02,\n",
      "        -1.4507e-01, -1.5304e-01, -2.0179e-01,  1.9812e-02,  1.3203e-01,\n",
      "        -2.4571e-01,  7.5497e-02, -2.3558e-01, -1.5698e-01,  8.0497e-03,\n",
      "        -3.1967e-01, -2.1594e-01,  1.5914e-01, -2.9397e-01, -2.5457e-01,\n",
      "        -6.5071e-02,  1.3945e-01,  1.0289e-01,  5.7299e-03, -2.4109e-01,\n",
      "        -2.3398e-01,  8.9583e-02,  6.1126e-02,  9.6327e-02,  1.6243e-02,\n",
      "        -1.5730e-01, -4.7098e-02, -2.0106e-01, -2.0255e-01,  1.2705e-01,\n",
      "        -6.5886e-02,  3.5091e-02,  1.0045e-02,  2.0357e-01,  3.3907e-02,\n",
      "        -2.4082e-01, -2.2305e-01, -1.1535e-01,  1.0811e-01, -5.1609e-02,\n",
      "        -1.6054e-02, -8.6598e-02, -1.7460e-01, -4.1115e-01, -4.4271e-02,\n",
      "        -1.3865e-01, -1.0014e-02,  6.4500e-02, -2.2930e-01,  1.0721e-02,\n",
      "        -3.4468e-02, -1.6325e-02,  6.1788e-02, -2.6810e-01, -1.9701e-01,\n",
      "        -2.5933e-01,  7.5740e-02, -1.2825e-02, -8.2108e-02, -4.4841e-02,\n",
      "        -2.9285e-01,  8.2573e-02, -2.9709e-01, -1.7700e-01, -2.2632e-01,\n",
      "        -9.3678e-02, -2.8030e-01, -2.4648e-01, -1.3980e-01,  1.4831e-01,\n",
      "         1.9955e-01, -1.9322e-01,  1.4142e-02,  6.7788e-02,  1.3388e-01,\n",
      "        -2.6710e-02, -6.8673e-03,  1.3205e-01, -2.1566e-02, -2.4790e-01,\n",
      "         5.7587e-02,  1.0266e-01,  1.2324e-01, -2.9533e-01,  2.9806e-02,\n",
      "         1.3684e-01, -6.0305e-02,  2.9617e-02, -1.5348e-01,  1.4694e-01,\n",
      "        -2.7013e-01, -9.9583e-02, -3.3368e-01, -1.0232e-02, -1.2107e-01,\n",
      "        -1.1843e-01,  7.1182e-02, -1.6219e-01,  1.0814e-01,  3.4672e-02,\n",
      "        -2.6038e-01, -1.6022e-01,  1.7862e-02, -2.5278e-01, -1.3883e-02,\n",
      "        -3.1160e-01, -1.7031e-01,  1.2527e-01, -8.9977e-02, -2.5412e-01,\n",
      "        -2.6240e-01, -1.2733e-01, -1.2471e-01, -9.9120e-02, -1.8780e-02,\n",
      "         1.0583e-01, -6.4217e-02, -2.9791e-02,  1.1218e-01, -1.4983e-01,\n",
      "         1.5676e-02,  7.0057e-02, -2.2804e-01, -2.8627e-01,  1.1459e-01,\n",
      "         8.7190e-02,  1.6588e-01, -3.2739e-01,  1.2182e-01, -8.5906e-02,\n",
      "        -2.8378e-01, -1.9407e-01, -6.8018e-02,  7.8266e-02,  9.1704e-02,\n",
      "        -1.6085e-01, -2.5115e-01, -2.2749e-01, -2.0978e-01, -5.7947e-02,\n",
      "        -1.8828e-01,  1.1380e-01, -1.0642e-01, -1.0728e-01, -2.7607e-01,\n",
      "         4.3353e-02,  7.1647e-03, -1.3145e-01, -1.9121e-01,  1.1094e-02,\n",
      "        -1.1574e-01, -3.9035e-02, -1.6007e-01,  9.2450e-02, -2.2380e-01,\n",
      "        -2.2897e-01, -4.9547e-02, -1.9808e-01, -3.0409e-01,  1.6381e-01,\n",
      "        -8.7075e-03, -1.0058e-02, -1.3763e-01, -2.1827e-01, -2.9200e-02,\n",
      "        -2.9355e-01, -6.6331e-02, -3.2165e-01,  4.3629e-02, -3.3457e-01,\n",
      "        -2.5748e-01, -1.4467e-01,  2.0191e-01, -2.3301e-01, -2.2387e-01,\n",
      "         2.6357e-01,  3.0380e-02, -1.5627e-01, -1.3314e-01, -9.5478e-02,\n",
      "        -2.9804e-01, -3.5818e-01,  4.7257e-02,  1.9024e-02, -1.5811e-01,\n",
      "        -3.4101e-01,  2.7686e-02,  7.1943e-02, -6.6023e-02,  2.2592e-04,\n",
      "        -1.9619e-02, -1.8743e-01,  1.2750e-01, -1.0242e-01, -1.7667e-01,\n",
      "        -1.8888e-01,  2.3775e-02, -2.3629e-01,  3.1953e-02, -2.1020e-01,\n",
      "        -3.3238e-01, -2.4413e-01, -6.9393e-03,  1.0109e-01, -3.1399e-01,\n",
      "         1.2448e-01, -3.4954e-01,  5.8502e-02, -1.7958e-01, -2.7971e-01,\n",
      "        -2.9755e-01,  1.3689e-02, -8.9920e-02,  7.1208e-02, -3.2786e-01,\n",
      "        -1.5478e-01,  6.4934e-02,  1.0064e-01, -5.1911e-02, -2.0791e-01,\n",
      "        -3.2104e-01, -2.8661e-01,  4.6723e-03,  1.7494e-01, -1.0811e-01,\n",
      "        -1.4268e-01, -6.2468e-02, -3.0791e-01,  9.9514e-03, -3.0439e-01,\n",
      "        -2.6048e-01, -2.7915e-01, -8.4266e-02,  5.9831e-02,  1.2743e-01,\n",
      "        -2.9773e-01, -5.9114e-02, -2.6999e-01, -5.4270e-02,  8.3400e-02,\n",
      "        -1.6397e-02, -2.1425e-01, -1.6616e-01, -2.8966e-02,  8.7446e-02,\n",
      "        -3.0123e-01,  5.4558e-02, -1.7959e-01, -1.0455e-01, -2.4162e-02,\n",
      "         1.1309e-01, -2.5936e-01, -3.0245e-01,  5.6373e-02, -6.2402e-02,\n",
      "         2.0912e-02, -8.3542e-02, -5.5175e-02,  1.0637e-01,  1.1338e-01,\n",
      "        -2.7584e-02, -1.7972e-02,  3.3329e-03, -1.1423e-01, -2.3918e-01,\n",
      "        -6.1329e-02, -8.9691e-02,  4.2100e-03, -1.8084e-01,  8.7170e-02,\n",
      "        -3.5240e-01, -1.9142e-01, -1.5609e-01, -2.6168e-01, -1.5590e-01,\n",
      "         9.3592e-02,  1.4753e-01,  1.8155e-02, -4.7359e-02, -1.4850e-01,\n",
      "        -3.0604e-02, -1.9644e-01, -3.3617e-01, -1.3926e-01,  1.1867e-01,\n",
      "        -6.4732e-02, -7.3420e-02, -1.3732e-01,  1.0533e-01,  9.8553e-02,\n",
      "         2.5833e-03,  7.8684e-02, -1.0687e-01,  3.0189e-02, -1.3567e-01,\n",
      "        -1.8769e-01,  4.1804e-03,  8.2516e-03, -3.0714e-01, -1.5758e-01,\n",
      "         1.8035e-02, -1.0615e-01,  1.3078e-01, -1.3009e-01, -2.4304e-01,\n",
      "         6.5958e-02,  1.0093e-01, -2.4882e-01, -2.9836e-02,  1.3038e-01,\n",
      "        -1.7993e-02, -1.4988e-01,  1.0954e-01,  1.5142e-01, -1.0881e-01,\n",
      "        -2.3498e-01, -2.5829e-01, -8.0853e-02, -2.9304e-01, -3.1948e-01,\n",
      "        -6.2819e-02, -1.2230e-01, -4.8917e-02, -1.3797e-01, -1.2737e-01,\n",
      "         6.1071e-02, -2.0829e-02, -1.7353e-01, -8.0553e-03,  7.6871e-02,\n",
      "        -3.2183e-01,  1.3646e-01,  1.1018e-01,  5.7409e-02, -6.7540e-02,\n",
      "        -1.8404e-01, -3.1859e-01, -7.8125e-02,  1.2112e-01, -2.0856e-01,\n",
      "        -9.2339e-03, -2.4599e-01, -1.0852e-01, -1.0788e-01, -2.4121e-01,\n",
      "        -3.3778e-01, -2.8463e-01, -6.8629e-02, -5.8570e-03, -3.2799e-02,\n",
      "         1.1333e-01, -2.0716e-02, -3.3844e-01, -1.3657e-01, -2.1122e-01,\n",
      "        -2.8867e-01, -9.4543e-02, -1.4960e-01,  1.5759e-01, -5.5884e-02,\n",
      "         8.0545e-02, -1.7074e-01,  1.3028e-01, -2.3115e-01,  8.1770e-02,\n",
      "        -1.3444e-01, -1.3534e-01, -1.2327e-01, -3.4841e-01, -3.0704e-03,\n",
      "        -1.6050e-01, -2.0519e-01, -9.0622e-02, -1.8193e-01,  1.9584e-02,\n",
      "         1.4679e-01, -5.2345e-02, -1.6343e-01, -2.3688e-01,  5.7781e-02,\n",
      "        -7.9199e-02, -3.1054e-01,  6.6473e-02, -2.4969e-01, -9.2882e-02,\n",
      "        -1.4949e-01, -1.5925e-02, -2.1473e-01, -2.4744e-01, -2.3581e-01,\n",
      "        -1.0384e-01, -3.7293e-01, -3.4633e-03,  2.5474e-02, -7.9479e-03,\n",
      "         9.9739e-02, -2.5266e-01, -4.6785e-02, -9.9103e-02, -2.6305e-01,\n",
      "        -2.0455e-01, -1.4643e-01, -3.2220e-01, -1.1888e-01, -2.1109e-01,\n",
      "         5.6667e-02, -1.1376e-01, -1.0971e-02, -2.1071e-02,  1.9226e-02,\n",
      "         1.4824e-01, -3.6124e-01,  1.1800e-02, -7.2391e-02,  1.7496e-01,\n",
      "         5.9320e-02,  4.6219e-02, -5.8551e-02,  7.0033e-02, -2.0956e-01,\n",
      "        -2.3801e-01, -8.3808e-02, -2.1567e-01,  1.3744e-01, -2.1786e-01,\n",
      "        -2.6374e-01,  8.9746e-03], requires_grad=True)\n",
      "cos_embedding.weight Parameter containing:\n",
      "tensor([[-0.0795,  0.1664, -0.0029,  ..., -0.0325, -0.0006, -0.0277],\n",
      "        [-0.0514, -0.1351,  0.0274,  ...,  0.0703,  0.0387,  0.0112],\n",
      "        [ 0.0398, -0.0719,  0.0489,  ..., -0.0451, -0.0632, -0.0276],\n",
      "        ...,\n",
      "        [-0.0902, -0.0204,  0.0590,  ..., -0.0078,  0.0007, -0.0036],\n",
      "        [-0.1781, -0.0787,  0.0258,  ...,  0.0089, -0.0706, -0.0523],\n",
      "        [-0.1813, -0.0629,  0.0108,  ...,  0.0315, -0.0680,  0.0329]],\n",
      "       requires_grad=True)\n",
      "cos_embedding.bias Parameter containing:\n",
      "tensor([-1.1569e-01, -6.8529e-02, -1.5622e-01, -1.2770e-01, -5.1965e-02,\n",
      "        -2.9231e-01, -1.6529e-01,  1.4859e-01,  9.2422e-02, -1.1830e-01,\n",
      "        -1.4609e-01,  2.8645e-02, -4.3776e-02, -1.3911e-01,  7.4696e-04,\n",
      "        -8.0553e-02,  6.3612e-02, -5.7417e-02, -6.0545e-03, -1.5309e-01,\n",
      "        -2.1812e-01, -1.8653e-01, -1.4124e-02, -1.7989e-01,  8.7088e-02,\n",
      "        -1.0181e-01, -7.9987e-02, -1.0638e-01, -1.4393e-01, -8.9540e-02,\n",
      "        -9.7645e-04, -2.0640e-02, -5.1977e-02, -1.9855e-01, -1.5504e-01,\n",
      "        -2.6423e-02,  2.0975e-02,  4.7234e-03, -2.0705e-01, -1.1286e-01,\n",
      "         2.0770e-02,  1.2770e-02, -1.6238e-01, -3.9354e-02, -4.8741e-02,\n",
      "        -1.7540e-03, -9.5606e-02, -6.3964e-02, -1.1359e-01, -3.9409e-02,\n",
      "         5.0155e-03, -1.8362e-01, -1.6925e-01, -1.8942e-01,  5.1803e-02,\n",
      "        -5.4757e-02,  4.9448e-02, -1.1814e-01, -6.1997e-02, -2.1949e-01,\n",
      "        -1.0035e-01, -2.0884e-01, -2.4079e-01, -1.6036e-01, -1.7864e-01,\n",
      "         4.7006e-02, -1.1903e-01, -1.5833e-01, -1.6162e-01,  2.3202e-02,\n",
      "        -3.6038e-02, -8.7208e-02, -1.2468e-01, -1.3681e-01, -2.7759e-01,\n",
      "        -2.1554e-01, -2.3761e-02, -1.0237e-01, -1.7411e-01, -1.0489e-01,\n",
      "         4.9826e-02, -1.1824e-01, -1.0311e-01,  1.1653e-01, -5.9822e-02,\n",
      "         9.3057e-02,  4.9384e-02, -2.4465e-01,  5.5064e-02, -6.1816e-02,\n",
      "         9.3548e-02, -5.0311e-02, -1.4291e-01,  2.5256e-04, -3.2717e-02,\n",
      "        -1.8486e-01,  4.3349e-02,  3.2723e-02, -1.3065e-01, -9.4810e-02,\n",
      "        -1.1855e-01, -1.9680e-01, -4.0142e-02, -1.5384e-01, -1.8244e-01,\n",
      "        -9.3090e-02, -1.3936e-02, -1.6262e-01, -7.6478e-02,  2.7502e-02,\n",
      "        -5.6698e-02, -1.8260e-01, -1.2191e-01, -9.7610e-03, -1.4649e-01,\n",
      "         1.6560e-02, -1.1285e-01, -1.2730e-01, -4.4219e-02, -4.7200e-03,\n",
      "         2.4853e-02, -6.7783e-02, -1.4392e-02, -1.0938e-01, -1.1069e-01,\n",
      "        -8.0861e-02, -7.8625e-03, -1.8158e-02, -8.9387e-02, -5.2455e-02,\n",
      "         3.8890e-02, -1.5334e-01, -2.0684e-03, -5.5234e-02,  2.0315e-02,\n",
      "         4.0941e-02,  1.8572e-02, -5.5647e-02, -1.3292e-02, -8.0443e-02,\n",
      "         1.0950e-01, -1.9122e-01, -1.9229e-01, -1.2962e-01, -2.1963e-01,\n",
      "         7.6193e-02, -1.1430e-01,  2.1361e-02, -1.3630e-01, -5.5462e-02,\n",
      "        -3.6512e-03, -9.7632e-02, -2.5865e-02, -7.8671e-02, -3.9580e-02,\n",
      "        -9.2948e-02, -2.2185e-01,  1.2044e-01, -1.5534e-01, -3.1115e-02,\n",
      "        -1.8036e-01, -3.1189e-01,  4.2299e-02, -5.7552e-02,  7.6032e-03,\n",
      "        -2.8943e-02, -2.0826e-01, -6.6079e-02,  3.2265e-02, -8.1135e-02,\n",
      "        -2.5451e-01, -2.0748e-01, -6.1132e-02, -2.9702e-02, -2.1926e-01,\n",
      "         1.9975e-02, -5.8645e-02, -1.1482e-01, -1.5228e-01,  7.8902e-02,\n",
      "        -1.8275e-01,  1.1425e-02, -1.0980e-01, -6.7975e-02, -7.7232e-02,\n",
      "        -3.5656e-02, -7.9963e-02, -1.6917e-01, -6.4583e-02, -1.5600e-01,\n",
      "        -1.0298e-01,  3.9242e-02, -1.4346e-01, -1.5587e-01, -4.3627e-03,\n",
      "        -5.7963e-02, -3.6500e-03, -2.8820e-02, -1.0181e-01, -1.0290e-01,\n",
      "        -1.4007e-01, -1.2930e-01, -3.1805e-02, -1.5541e-01, -2.4068e-01,\n",
      "        -1.5120e-01,  4.5524e-02, -1.2330e-01, -8.8106e-02, -3.8798e-02,\n",
      "         9.6524e-02,  4.3881e-02, -6.2809e-02,  3.8371e-02,  2.5711e-02,\n",
      "         1.4084e-02, -1.1537e-01, -5.0373e-02, -4.7079e-02, -1.0368e-01,\n",
      "         1.9136e-02, -1.7679e-01, -3.0144e-02,  3.3658e-02, -2.1950e-01,\n",
      "        -8.8840e-02, -1.0467e-01, -9.3497e-03, -7.3569e-02, -1.8955e-01,\n",
      "        -1.6690e-01,  3.0113e-02, -8.7159e-02, -9.6443e-02, -3.6279e-02,\n",
      "         2.3639e-02, -3.1858e-03, -2.2829e-01,  6.3241e-02, -1.0163e-01,\n",
      "        -1.4345e-01, -8.5509e-02, -1.8805e-01, -1.2800e-01, -2.0190e-01,\n",
      "        -5.2704e-02, -1.8274e-01, -1.1115e-01,  1.2136e-02,  3.2133e-03,\n",
      "        -4.6610e-02, -7.3717e-02, -7.4817e-02, -1.8244e-01, -1.7114e-01,\n",
      "        -8.4524e-02, -5.0519e-02, -2.3157e-01, -8.9444e-02, -1.7333e-01,\n",
      "         2.4814e-01,  6.4662e-02, -8.7058e-02, -8.1152e-02, -9.4288e-02,\n",
      "        -8.2782e-02, -2.4687e-01, -8.8051e-02, -1.7229e-01, -1.2726e-01,\n",
      "        -1.3445e-01, -1.3653e-01,  4.0970e-02, -1.6563e-01,  8.1679e-02,\n",
      "        -1.4988e-01, -2.0409e-01, -2.8765e-02, -1.6034e-01, -2.1072e-01,\n",
      "        -3.7079e-02, -1.4283e-01, -3.8230e-02,  5.4818e-02, -8.6378e-02,\n",
      "        -1.3011e-01, -1.9759e-01, -7.0731e-02, -1.3130e-01, -1.7148e-01,\n",
      "         1.0974e-02, -6.3034e-02,  1.0715e-02, -1.8561e-02, -3.0854e-01,\n",
      "        -7.3605e-02, -9.2427e-02, -1.3929e-01, -2.7330e-02, -7.3296e-02,\n",
      "        -1.9107e-01,  2.3043e-02, -8.3214e-02, -9.7840e-02,  5.4270e-02,\n",
      "        -1.4807e-01, -2.0674e-01, -1.1427e-01, -1.3702e-01, -2.3268e-01,\n",
      "        -1.3886e-01,  4.7035e-02, -4.5802e-02, -1.7553e-01, -1.4021e-01,\n",
      "        -1.3810e-01, -2.0273e-01, -6.7414e-02, -1.8487e-01, -7.1816e-02,\n",
      "        -1.2979e-01, -8.9659e-02, -1.3804e-01, -7.2055e-02, -2.4745e-02,\n",
      "        -2.3740e-01, -1.3898e-01, -1.9215e-01, -2.4623e-01, -1.7717e-01,\n",
      "        -1.0021e-01, -1.5345e-01, -1.9507e-02, -7.9349e-02, -2.2588e-01,\n",
      "        -1.5245e-01, -6.0632e-02, -5.3705e-02,  2.2322e-02, -3.0178e-02,\n",
      "        -2.7085e-02, -9.8254e-02, -2.2161e-01, -1.1795e-01, -4.4188e-02,\n",
      "        -1.3832e-01, -1.7240e-02, -1.5625e-01, -5.3547e-02, -1.6605e-01,\n",
      "        -1.8683e-01,  6.8673e-02, -2.9828e-03, -1.7660e-01,  1.8653e-01,\n",
      "        -5.9102e-02, -9.7465e-02, -1.3532e-01,  1.4626e-01, -3.2110e-02,\n",
      "        -7.6306e-02,  8.7175e-02, -1.2231e-01, -4.6431e-02,  4.7338e-02,\n",
      "        -3.8011e-02, -9.1745e-02, -5.3961e-02, -5.3777e-02, -1.2233e-01,\n",
      "        -9.0404e-02, -2.6922e-01, -4.0364e-02,  5.4465e-02, -3.7192e-02,\n",
      "        -1.4890e-01, -8.2713e-02, -4.6608e-02, -9.8645e-02, -9.5383e-02,\n",
      "        -8.0782e-02, -6.5622e-02, -2.2027e-01, -1.0581e-01,  4.1645e-02,\n",
      "         2.5535e-02, -4.0823e-02,  2.6267e-02, -5.7916e-02, -1.4332e-01,\n",
      "        -1.5676e-01, -7.4620e-02, -1.5914e-01, -1.0249e-01, -6.3082e-02,\n",
      "        -6.3497e-02, -1.1978e-01, -1.5542e-01, -1.3166e-02, -1.7311e-03,\n",
      "        -1.5648e-01,  1.3987e-01,  9.3832e-02, -5.6052e-02, -2.3952e-01,\n",
      "        -1.4521e-01, -1.3922e-01, -2.2567e-01, -1.4949e-01, -1.8510e-01,\n",
      "         2.4903e-02, -8.4197e-02, -2.0159e-01, -2.2216e-02, -1.3207e-01,\n",
      "        -8.7653e-02, -1.8769e-01, -1.0480e-01, -5.8161e-02, -2.1764e-01,\n",
      "        -1.6250e-01, -6.3693e-02, -1.4072e-01,  6.5421e-02, -2.8354e-01,\n",
      "        -7.3004e-02, -1.0011e-04, -4.9917e-03, -7.9252e-02, -1.0967e-02,\n",
      "         6.7170e-03, -1.0084e-01, -3.1990e-02, -1.5787e-01, -1.7376e-01,\n",
      "         1.2470e-02, -1.2487e-02, -1.6903e-01, -1.9462e-02, -6.4136e-02,\n",
      "        -1.3653e-01, -3.5671e-02, -9.3926e-02, -2.8484e-02, -1.2269e-01,\n",
      "        -2.7283e-02, -7.0023e-02, -1.8862e-01,  2.6506e-02, -5.5247e-02,\n",
      "        -1.6038e-01,  2.8407e-01, -8.8103e-02, -5.9592e-02, -1.5023e-01,\n",
      "        -1.0845e-01,  6.6242e-03, -9.9156e-02, -3.0226e-02, -5.6623e-03,\n",
      "        -1.3758e-01, -1.4962e-01, -1.2148e-02, -5.5180e-02, -2.5326e-01,\n",
      "        -8.8309e-03, -1.5752e-01, -3.6037e-02, -7.7359e-02, -6.4692e-02,\n",
      "        -1.3687e-01,  1.1885e-02,  3.7043e-02, -1.7012e-01, -9.2201e-02,\n",
      "        -2.1716e-01, -1.2003e-01, -2.0598e-01, -1.7304e-01, -1.1946e-01,\n",
      "        -1.1159e-01, -1.0065e-01, -9.7807e-02, -2.2289e-03, -1.6870e-01,\n",
      "        -1.3257e-01,  7.0254e-02, -1.7219e-01, -2.8784e-02, -1.9219e-01,\n",
      "         1.1082e-01, -1.5919e-01,  3.1351e-01, -3.8534e-02,  4.5409e-02,\n",
      "         1.0609e-02, -1.8603e-02, -1.3365e-01, -1.3559e-01, -8.0603e-02,\n",
      "         1.6025e-02, -3.6988e-02, -2.8149e-01,  4.6573e-02, -2.1457e-02,\n",
      "        -1.6069e-01,  1.3292e-01, -7.7280e-02, -4.7010e-03, -1.0886e-01,\n",
      "        -2.0085e-01, -4.1796e-03], requires_grad=True)\n",
      "ff_1.weight Parameter containing:\n",
      "tensor([[-0.0111, -0.0439,  0.0167,  ..., -0.0024, -0.0547, -0.0023],\n",
      "        [-0.0123,  0.0179, -0.0188,  ...,  0.0105, -0.0466,  0.0201],\n",
      "        [-0.0316, -0.0395, -0.0227,  ..., -0.0447, -0.0021, -0.0065],\n",
      "        ...,\n",
      "        [ 0.0098, -0.0070,  0.0229,  ...,  0.0172,  0.0152,  0.0096],\n",
      "        [-0.0348, -0.0354,  0.0176,  ..., -0.0066, -0.0211,  0.0095],\n",
      "        [-0.0955, -0.0447, -0.0009,  ...,  0.0080, -0.0395, -0.0125]],\n",
      "       requires_grad=True)\n",
      "ff_1.bias Parameter containing:\n",
      "tensor([-4.9730e-02, -4.7281e-02, -8.8253e-02, -7.5452e-02, -4.9847e-02,\n",
      "        -9.2356e-02, -8.8763e-02, -4.4466e-03,  5.8943e-04, -4.1201e-02,\n",
      "        -6.5868e-02, -3.3090e-02, -1.0423e-01, -1.1392e-01, -1.1058e-01,\n",
      "        -8.3730e-02, -6.6694e-02, -5.7649e-04,  1.4256e-02, -1.1720e-01,\n",
      "        -4.5624e-02, -9.7564e-02, -2.2348e-02, -5.2470e-02, -6.5674e-02,\n",
      "        -9.0691e-02, -7.0226e-02, -6.7446e-02, -1.0207e-01, -7.3896e-02,\n",
      "        -9.8033e-02, -8.1015e-02, -3.2422e-02, -1.0306e-01, -6.5321e-02,\n",
      "        -6.2496e-02, -1.2154e-01, -2.1360e-02, -5.1726e-02, -7.5951e-02,\n",
      "        -5.1474e-02, -6.4854e-02, -1.0041e-02, -6.0550e-02, -5.7102e-02,\n",
      "         1.1759e-02, -5.9693e-02, -5.5161e-02, -6.5343e-02, -1.0517e-01,\n",
      "        -1.1984e-01, -1.4606e-02, -3.7967e-02, -5.9593e-02, -8.6352e-02,\n",
      "        -6.1844e-03, -4.3407e-02, -1.0230e-01, -1.3809e-02, -2.3353e-02,\n",
      "        -9.7271e-02, -1.3752e-01, -6.7916e-02, -8.1824e-02, -5.9069e-02,\n",
      "        -9.4381e-02, -1.0926e-01, -4.9680e-02, -7.4197e-02, -3.7931e-02,\n",
      "        -7.5026e-02, -5.3848e-02, -6.9606e-02, -4.8087e-03, -1.3911e-02,\n",
      "        -6.5377e-02, -4.1042e-02, -7.3419e-02, -3.9590e-02, -4.2007e-02,\n",
      "        -5.6734e-02, -5.8707e-02, -1.0365e-01, -3.9567e-02, -4.2732e-02,\n",
      "        -8.9803e-02, -3.0366e-02, -9.1460e-02, -6.9892e-02, -3.3322e-04,\n",
      "        -2.7431e-02, -7.5853e-02, -7.1066e-02, -5.6012e-02, -4.4923e-02,\n",
      "        -1.0784e-01, -1.1195e-02, -6.8582e-02, -3.8211e-02, -7.5153e-02,\n",
      "        -1.6720e-02, -2.8711e-02, -8.3005e-02, -9.6294e-02, -2.9333e-02,\n",
      "        -3.5750e-02, -8.7792e-02, -8.7527e-02, -9.2940e-02, -2.3110e-02,\n",
      "        -2.6232e-02, -7.0821e-02, -7.3930e-02, -6.4391e-02, -1.3147e-02,\n",
      "        -3.3142e-02, -1.1733e-01, -9.4237e-02, -1.2871e-01, -3.2566e-02,\n",
      "        -5.2310e-02, -1.3369e-01, -2.0894e-03, -3.4158e-02,  2.4776e-02,\n",
      "        -5.8472e-02, -1.0786e-01, -1.2079e-01,  6.1545e-03, -9.7759e-02,\n",
      "        -7.2647e-03, -6.3721e-02, -8.1800e-03, -4.1919e-02, -7.7188e-02,\n",
      "        -7.0248e-02, -8.4916e-02, -6.6009e-02, -4.8669e-02, -1.8305e-03,\n",
      "        -7.7926e-02, -7.3622e-02, -8.6682e-02, -8.1480e-02, -6.0722e-02,\n",
      "        -2.1055e-02, -1.4293e-01, -9.0514e-02, -4.8318e-02, -7.8942e-02,\n",
      "        -2.3981e-02, -1.5920e-02, -1.3872e-01, -8.7727e-03, -5.5322e-02,\n",
      "        -8.8069e-02, -1.3764e-01, -1.0214e-01, -1.8153e-02, -8.3851e-02,\n",
      "         2.7703e-02, -6.0074e-02, -4.4750e-02, -9.7228e-02, -6.6371e-02,\n",
      "        -9.7611e-03, -2.2129e-02, -1.4001e-02, -2.7313e-02, -2.4279e-02,\n",
      "        -8.5229e-02, -1.1019e-01, -2.2167e-02, -3.6578e-02, -8.1382e-02,\n",
      "        -5.3769e-02, -1.1116e-01, -8.4923e-02, -9.2939e-02,  1.5609e-02,\n",
      "         2.6569e-02, -1.0669e-01, -7.2565e-02, -1.2411e-01, -3.9094e-02,\n",
      "        -5.6155e-02, -5.0869e-02, -1.3681e-01, -8.6645e-02, -8.3700e-02,\n",
      "        -3.0106e-02, -9.1332e-02, -3.1931e-02, -1.1915e-02, -4.6834e-02,\n",
      "        -1.3868e-01, -2.5915e-02, -7.3056e-02, -5.8069e-02, -5.8650e-02,\n",
      "        -1.0733e-02, -7.1379e-02, -3.0507e-03, -5.2748e-02, -9.6890e-02,\n",
      "        -4.4654e-02, -2.1453e-02, -5.4882e-02, -9.5361e-02, -4.5626e-02,\n",
      "        -3.6807e-02, -6.8420e-02, -6.5413e-02, -7.4815e-02, -6.1727e-02,\n",
      "        -2.8028e-02, -3.5678e-02, -4.1944e-02,  2.5252e-02, -1.2391e-01,\n",
      "        -5.4688e-02, -6.1254e-02, -5.6500e-02, -7.5391e-02, -1.1244e-01,\n",
      "        -2.9993e-02, -3.6969e-02, -1.0955e-01, -6.9621e-02, -7.4441e-02,\n",
      "         3.8960e-03, -1.0118e-01, -1.4880e-01, -7.3674e-02, -5.8353e-02,\n",
      "        -7.5447e-02, -4.2217e-02, -9.3453e-02, -2.1039e-02, -1.7823e-02,\n",
      "         2.1460e-02, -6.9059e-02, -1.0310e-01, -6.9486e-02, -7.1635e-02,\n",
      "        -7.4597e-02, -7.6676e-02, -3.1382e-02, -5.3770e-02, -1.0150e-01,\n",
      "        -2.6335e-04, -1.1017e-02, -3.7435e-02, -7.9023e-03,  6.4193e-03,\n",
      "        -5.5044e-02, -3.3832e-02, -1.5470e-01, -6.6871e-02, -1.0037e-01,\n",
      "        -3.4851e-02, -7.1717e-02, -8.0892e-02, -4.6843e-02, -1.0445e-01,\n",
      "        -5.1153e-02, -4.5551e-02, -6.4295e-02, -8.9537e-02, -5.2991e-02,\n",
      "        -1.1966e-01, -7.8747e-02, -8.8091e-02, -1.2655e-03, -7.0432e-02,\n",
      "        -1.3981e-01, -1.4772e-01, -8.3395e-02, -9.3461e-02, -1.0295e-01,\n",
      "        -2.8328e-02, -5.3309e-02, -1.9920e-02, -3.2957e-02, -6.3867e-02,\n",
      "        -4.6606e-02, -5.0636e-02, -7.6311e-02, -1.0213e-02, -1.0758e-01,\n",
      "        -7.4966e-02, -5.6983e-02, -5.3532e-02, -1.4437e-02, -1.1636e-01,\n",
      "        -1.1328e-01, -8.9648e-02, -2.9984e-03, -7.0777e-02, -9.4368e-02,\n",
      "        -5.4963e-02, -9.2259e-02, -1.2516e-02, -5.0904e-02, -6.9229e-02,\n",
      "        -7.0682e-02, -1.0201e-01, -2.0265e-03, -3.6202e-02, -2.8805e-02,\n",
      "        -8.5300e-02, -5.5067e-02, -6.6248e-02, -1.0962e-01, -6.3131e-02,\n",
      "        -9.3447e-02, -1.3412e-01, -5.7369e-02, -1.0616e-01, -3.6016e-02,\n",
      "        -6.1172e-02, -4.6916e-02, -1.1191e-01, -4.7270e-02, -1.2512e-01,\n",
      "         1.9878e-02, -7.7593e-02, -2.9358e-02, -7.4108e-02, -1.4441e-01,\n",
      "        -8.1234e-02, -4.8259e-02, -2.6610e-02, -5.2713e-02, -1.1038e-01,\n",
      "        -7.8492e-02,  1.8667e-02, -5.5963e-02, -2.9011e-03, -6.9517e-02,\n",
      "        -1.4178e-02, -8.6119e-02, -8.4808e-02, -9.8407e-02, -7.9367e-02,\n",
      "        -4.9783e-02, -1.0881e-01, -3.6448e-02, -2.5957e-02, -1.0180e-01,\n",
      "        -1.9849e-02, -6.2833e-02, -6.4528e-02, -1.1729e-01, -8.4336e-02,\n",
      "        -3.0788e-04, -3.6531e-02, -9.0353e-02,  3.0143e-02, -1.1578e-01,\n",
      "        -1.1004e-01, -2.6858e-02, -5.6998e-02, -8.5879e-02, -4.6298e-02,\n",
      "        -5.2107e-02, -2.7640e-02, -8.7641e-02, -7.3840e-02, -9.9896e-02,\n",
      "        -1.2174e-01, -5.6385e-02, -4.9703e-02, -6.1322e-03, -6.8209e-02,\n",
      "        -9.3379e-02, -1.0825e-01, -8.2514e-02, -5.5571e-02, -1.0223e-01,\n",
      "        -9.7800e-02, -7.8374e-02, -6.4511e-02, -7.5661e-02, -1.5773e-02,\n",
      "        -5.6414e-02, -5.4910e-02, -4.1782e-02, -9.4877e-02, -1.0925e-01,\n",
      "        -1.1881e-01, -9.4369e-02, -3.4713e-02, -9.3728e-02, -5.7718e-02,\n",
      "        -4.7434e-02, -7.2100e-02, -3.6045e-02, -4.2829e-02, -7.9948e-02,\n",
      "        -1.0415e-01, -5.4262e-02, -6.8546e-02, -2.9479e-02, -6.1406e-02,\n",
      "        -2.8741e-02, -1.0465e-01, -5.9856e-02, -8.6037e-02,  9.8781e-03,\n",
      "        -1.0879e-01, -6.7978e-02, -5.2641e-02, -9.3707e-02, -1.2776e-01,\n",
      "        -5.4579e-02, -9.7660e-02, -4.8823e-02, -6.0936e-02, -1.1110e-01,\n",
      "        -2.0136e-03, -6.5965e-02, -1.3175e-02, -7.2792e-02, -6.4736e-02,\n",
      "        -8.2502e-02, -3.0606e-02, -9.4231e-02, -1.8043e-02, -1.1226e-01,\n",
      "        -4.6759e-02, -1.8502e-02, -8.3551e-02,  2.5647e-02, -4.3823e-02,\n",
      "        -1.4607e-01, -8.8639e-02, -6.4984e-02, -5.0067e-02, -1.0073e-01,\n",
      "        -2.6063e-02,  1.8059e-02, -3.9380e-03, -6.2056e-02, -5.0267e-02,\n",
      "        -5.8794e-02, -1.9403e-02, -6.9359e-02, -4.7010e-02,  1.1944e-02,\n",
      "         2.7380e-03, -1.1230e-01, -8.8373e-02, -9.2220e-02,  9.2673e-03,\n",
      "        -8.1982e-02, -9.9717e-02, -3.3061e-02, -3.9354e-02, -6.6404e-02,\n",
      "        -4.6010e-02, -9.5264e-02, -8.9320e-02, -9.6405e-02, -3.8096e-02,\n",
      "        -5.7505e-02, -5.0955e-02, -4.9666e-02, -1.7452e-01, -8.2577e-02,\n",
      "        -3.3688e-02, -5.4527e-02, -5.7603e-02, -9.9968e-02, -1.3542e-01,\n",
      "        -2.9958e-02, -7.6050e-02, -7.5882e-02,  2.8902e-01, -2.2292e-02,\n",
      "        -1.1601e-01,  2.9651e-02, -5.4982e-02, -8.0087e-02, -9.1269e-02,\n",
      "        -9.1825e-02, -2.6451e-02, -8.7603e-02, -2.4578e-02, -1.0710e-01,\n",
      "        -4.5958e-02, -1.0401e-01, -1.3284e-01, -8.4913e-02, -2.7860e-02,\n",
      "        -7.6902e-02, -1.1959e-01, -5.5176e-02, -5.2642e-02, -7.7102e-02,\n",
      "         5.6578e-03, -1.0394e-01, -9.0094e-02, -6.9083e-02, -9.2223e-02,\n",
      "        -3.2458e-02, -2.4637e-02, -4.5780e-02, -3.5477e-04, -9.1103e-02,\n",
      "        -9.0020e-02,  6.5961e-02], requires_grad=True)\n",
      "ff_2.weight Parameter containing:\n",
      "tensor([[-0.0146,  0.0274,  0.0080,  ..., -0.0005, -0.0237, -0.0147],\n",
      "        [-0.0090, -0.0311,  0.0173,  ..., -0.0242, -0.0141,  0.0055],\n",
      "        [-0.0065,  0.0012, -0.0062,  ...,  0.0316, -0.0151,  0.0211]],\n",
      "       requires_grad=True)\n",
      "ff_2.bias Parameter containing:\n",
      "tensor([0.0079, 0.0085, 0.0081], requires_grad=True)\n",
      "\r",
      "Episode 2\tFrame 13586 \tAverage Score: 0.15[1000.           12.96428571   19.33         45.25          0.\n",
      "    0.            0.            0.            0.            0.\n",
      "  100.          100.          100.           66.66666667   66.66666667\n",
      "   66.66666667  100.          100.          100.        ]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [85]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     53\u001b[0m eps_fixed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m     54\u001b[0m t0 \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m---> 55\u001b[0m final_average100 \u001b[38;5;241m=\u001b[39m \u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menv_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframes\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m20000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meps_fixed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meps_fixed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meps_frames\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmin_eps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.025\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     56\u001b[0m t1 \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n",
      "Input \u001b[0;32mIn [84]\u001b[0m, in \u001b[0;36mrun\u001b[0;34m(env, frames, eps_fixed, eps_frames, min_eps)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;66;03m#print(\"env_trainNext State: {}\".format(next_state.shape))\u001b[39;00m\n\u001b[1;32m     50\u001b[0m next_state \u001b[38;5;241m=\u001b[39m next_state[\u001b[38;5;241m0\u001b[39m,:]\n\u001b[0;32m---> 52\u001b[0m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreward\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnext_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdone\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwriter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m state \u001b[38;5;241m=\u001b[39m next_state\n\u001b[1;32m     56\u001b[0m score \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m reward\n",
      "Input \u001b[0;32mIn [83]\u001b[0m, in \u001b[0;36mDQN_Agent.step\u001b[0;34m(self, state, action, reward, next_state, done, writer)\u001b[0m\n\u001b[1;32m    197\u001b[0m experiences \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmemory\u001b[38;5;241m.\u001b[39msample()\n\u001b[1;32m    198\u001b[0m \u001b[38;5;66;03m#print(\"experiences:{}\".format(experiences))\u001b[39;00m\n\u001b[0;32m--> 199\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexperiences\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    200\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mQ_updates \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    201\u001b[0m writer\u001b[38;5;241m.\u001b[39madd_scalar(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mQ_loss\u001b[39m\u001b[38;5;124m\"\u001b[39m, loss, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mQ_updates)\n",
      "Input \u001b[0;32mIn [83]\u001b[0m, in \u001b[0;36mDQN_Agent.learn\u001b[0;34m(self, experiences)\u001b[0m\n\u001b[1;32m    262\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m    263\u001b[0m \u001b[38;5;66;03m#clip_grad_norm_(self.qnetwork_local.parameters(),1)\u001b[39;00m\n\u001b[0;32m--> 264\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    266\u001b[0m \u001b[38;5;66;03m# ------------------- update target network ------------------- #\u001b[39;00m\n\u001b[1;32m    267\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msoft_update(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mqnetwork_local, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mqnetwork_target)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/optim/optimizer.py:88\u001b[0m, in \u001b[0;36mOptimizer._hook_for_profile.<locals>.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m profile_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOptimizer.step#\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m.step\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(obj\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mrecord_function(profile_name):\n\u001b[0;32m---> 88\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/autograd/grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclone():\n\u001b[0;32m---> 27\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/optim/adam.py:141\u001b[0m, in \u001b[0;36mAdam.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    138\u001b[0m             \u001b[38;5;66;03m# record the step after step update\u001b[39;00m\n\u001b[1;32m    139\u001b[0m             state_steps\u001b[38;5;241m.\u001b[39mappend(state[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstep\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m--> 141\u001b[0m     \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madam\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    142\u001b[0m \u001b[43m           \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    143\u001b[0m \u001b[43m           \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    144\u001b[0m \u001b[43m           \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    145\u001b[0m \u001b[43m           \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    146\u001b[0m \u001b[43m           \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    147\u001b[0m \u001b[43m           \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mamsgrad\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    148\u001b[0m \u001b[43m           \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    149\u001b[0m \u001b[43m           \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    150\u001b[0m \u001b[43m           \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[43m           \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    152\u001b[0m \u001b[43m           \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43meps\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    153\u001b[0m \u001b[43m           \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmaximize\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/optim/_functional.py:105\u001b[0m, in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    103\u001b[0m     denom \u001b[38;5;241m=\u001b[39m (max_exp_avg_sqs[i]\u001b[38;5;241m.\u001b[39msqrt() \u001b[38;5;241m/\u001b[39m math\u001b[38;5;241m.\u001b[39msqrt(bias_correction2))\u001b[38;5;241m.\u001b[39madd_(eps)\n\u001b[1;32m    104\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 105\u001b[0m     denom \u001b[38;5;241m=\u001b[39m (\u001b[43mexp_avg_sq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msqrt\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m/\u001b[39m math\u001b[38;5;241m.\u001b[39msqrt(bias_correction2))\u001b[38;5;241m.\u001b[39madd_(eps)\n\u001b[1;32m    109\u001b[0m step_size \u001b[38;5;241m=\u001b[39m lr \u001b[38;5;241m/\u001b[39m bias_correction1\n\u001b[1;32m    110\u001b[0m param\u001b[38;5;241m.\u001b[39maddcdiv_(exp_avg, denom, value\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39mstep_size)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "        # read and preprocess data\n",
    "    preprocessed_path = \"done_3stocks.csv\"\n",
    "    if os.path.exists(preprocessed_path):\n",
    "        data = pd.read_csv(preprocessed_path, index_col=0)\n",
    "\n",
    "    unique_trade_date = data[(data.datadate > 20151001)&(data.datadate <= 20200707)].datadate.unique()\n",
    "    #print(unique_trade_date)\n",
    "\n",
    "    \n",
    "    train = data_split(data, start=2016101, end=20180101)\n",
    "    \n",
    "    env_train = DummyVecEnv([lambda: StockEnvTrain(train)])\n",
    "    \n",
    "    writer = SummaryWriter(\"runs/\"+\"IQN_CP_5\")\n",
    "    seed = 1\n",
    "    BUFFER_SIZE = 10000\n",
    "    BATCH_SIZE = 8\n",
    "    GAMMA = 0.99\n",
    "    TAU = 1e-2\n",
    "    LR = 1e-3\n",
    "    UPDATE_EVERY = 1\n",
    "    n_step = 1\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(\"Using \", device)\n",
    "\n",
    "\n",
    "    action_size     = env_train.action_space.shape[0]\n",
    "\n",
    "    \n",
    "    \n",
    "    print('Action Space: {}'.format(action_size))\n",
    "    state_size = env_train.observation_space.shape[0]\n",
    "    print('State Space: {}'.format(state_size))\n",
    "\n",
    "    agent = DQN_Agent(state_size=19,    #181 #4\n",
    "                        action_size=3, #30 #7\n",
    "                        layer_size=512, #512, #512\n",
    "                        n_step=n_step,\n",
    "                        BATCH_SIZE=BATCH_SIZE, \n",
    "                        BUFFER_SIZE=BUFFER_SIZE, \n",
    "                        LR=LR, \n",
    "                        TAU=TAU, \n",
    "                        GAMMA=GAMMA, \n",
    "                        UPDATE_EVERY=UPDATE_EVERY, \n",
    "                        device=device, \n",
    "                        seed=seed)\n",
    "\n",
    "\n",
    "\n",
    "    # set epsilon frames to 0 so no epsilon exploration\n",
    "    eps_fixed = False\n",
    "    t0 = time.time()\n",
    "    final_average100 = run(env=env_train, frames = 20000, eps_fixed=eps_fixed, eps_frames=5000, min_eps=0.025)\n",
    "    t1 = time.time()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training\n",
    "def run(env,frames=1000, eps_fixed=False, eps_frames=1e6, min_eps=0.01):\n",
    "    \"\"\"Deep Q-Learning.\n",
    "    \n",
    "    Params\n",
    "    ======\n",
    "        n_episodes (int): maximum number of training episodes\n",
    "        max_t (int): maximum number of timesteps per episode\n",
    "        eps_start (float): starting value of epsilon, for epsilon-greedy action selection\n",
    "        eps_end (float): minimum value of epsilon\n",
    "        eps_decay (float): multiplicative factor (per episode) for decreasing epsilon\n",
    "    \"\"\"\n",
    "    scores = []                        # list containing scores from each episode\n",
    "    scores_window = deque(maxlen=100)  # last 100 scores\n",
    "    output_history = []\n",
    "    frame = 0\n",
    "    if eps_fixed:\n",
    "        eps = 0\n",
    "    else:\n",
    "        eps = 1\n",
    "    eps_start = 1\n",
    "    i_episode = 1\n",
    "    state = env.reset()\n",
    "    state = state[0,:]\n",
    "    #print(\"state space:{}\".format(state[0,:].shape))\n",
    "    score = 0                  \n",
    "    for frame in range(1, frames+1):\n",
    "        \n",
    "        if frame  == 0:\n",
    "            # inital state\n",
    "\n",
    "            initial = True\n",
    "        else:\n",
    "            # previous state\n",
    "            initial = False\n",
    "        #print('initial state:{}'.format(initial))\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        action = agent.act(state, eps) #TODO: getting one dimension back.\n",
    "        \n",
    "        \n",
    "        action = np.array([action])\n",
    "        \n",
    "        \n",
    "        next_state, reward, done, info = env_train.step([action]) #TODO: Wants a list of actions of size a\n",
    "\n",
    "        #print(\"env_trainNext State: {}\".format(next_state.shape))\n",
    "\n",
    "        next_state = next_state[0,:]\n",
    "        \n",
    "        agent.step(state, action, reward, next_state, done, writer)\n",
    "        \n",
    "\n",
    "        state = next_state\n",
    "        score += reward\n",
    "        # linear annealing to the min epsilon value until eps_frames and from there slowly decease epsilon to 0 until the end of training\n",
    "        if eps_fixed == False:\n",
    "            if frame < eps_frames:\n",
    "                eps = max(eps_start - (frame*(1/eps_frames)), min_eps)\n",
    "            else:\n",
    "                eps = max(min_eps - min_eps*((frame-eps_frames)/(frames-eps_frames)), 0.001)\n",
    "\n",
    "        # evaluation runs\n",
    "        if frame % 100000 == 0:\n",
    "            print(\"score: {}\".format(state))\n",
    "            print(\"score: {}\".format(score))\n",
    "            #print(\"state: {}\".format(state))\n",
    "            print(\"action:{}, Number:{}\".format(action,frame))\n",
    "            print(\"-------------------------\")\n",
    "        \n",
    "        if done:\n",
    "            \n",
    "                        \n",
    "            for k, v in agent.qnetwork_local.named_parameters():\n",
    "                if k=='ff_2.bias':\n",
    "                    print(k, v)\n",
    "                    \n",
    "            scores_window.append(score)       # save most recent score\n",
    "            scores.append(score)              # save most recent score\n",
    "            writer.add_scalar(\"Average100\", np.mean(scores_window), frame)\n",
    "            output_history.append(np.mean(scores_window))\n",
    "            print('\\rEpisode {}\\tFrame {} \\tAverage Score: {:.2f}'.format(i_episode, frame, np.mean(scores_window)), end=\"\")\n",
    "            if i_episode % 100 == 0:\n",
    "                print('\\rEpisode {}\\tFrame {}\\tAverage Score: {:.2f}'.format(i_episode,frame, np.mean(scores_window)))\n",
    "            i_episode +=1 \n",
    "\n",
    "            state = env.reset()\n",
    "            state = state[0,:]\n",
    "            score = 0              \n",
    "\n",
    "    return output_history\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "        # read and preprocess data\n",
    "    preprocessed_path = \"done_3stocks.csv\"\n",
    "    if os.path.exists(preprocessed_path):\n",
    "        data = pd.read_csv(preprocessed_path, index_col=0)\n",
    "\n",
    "    unique_trade_date = data[(data.datadate > 20151001)&(data.datadate <= 20200707)].datadate.unique()\n",
    "    #print(unique_trade_date)\n",
    "\n",
    "    \n",
    "    train = data_split(data, start=20100101, end=20160101)\n",
    "    \n",
    "    env_train = DummyVecEnv([lambda: StockEnvTrain(train)])\n",
    "    \n",
    "    writer = SummaryWriter(\"runs/\"+\"IQN_CP_5\")\n",
    "    seed = 1\n",
    "    BUFFER_SIZE = 10000\n",
    "    BATCH_SIZE = 8\n",
    "    GAMMA = 0.99\n",
    "    TAU = 1e-2\n",
    "    LR = 1e-3\n",
    "    UPDATE_EVERY = 1\n",
    "    n_step = 1\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(\"Using \", device)\n",
    "\n",
    "\n",
    "    action_size     = env_train.action_space.shape[0]\n",
    "\n",
    "    \n",
    "    \n",
    "    print('Action Space: {}'.format(action_size))\n",
    "    state_size = env_train.observation_space.shape[0]\n",
    "    print('State Space: {}'.format(state_size))\n",
    "\n",
    "    agent = DQN_Agent(state_size=19,    #181 #4\n",
    "                        action_size=3, #30 #7\n",
    "                        layer_size=512, #512, #512\n",
    "                        n_step=n_step,\n",
    "                        BATCH_SIZE=BATCH_SIZE, \n",
    "                        BUFFER_SIZE=BUFFER_SIZE, \n",
    "                        LR=LR, \n",
    "                        TAU=TAU, \n",
    "                        GAMMA=GAMMA, \n",
    "                        UPDATE_EVERY=UPDATE_EVERY, \n",
    "                        device=device, \n",
    "                        seed=seed)\n",
    "\n",
    "\n",
    "\n",
    "    # set epsilon frames to 0 so no epsilon exploration\n",
    "    eps_fixed = False\n",
    "    t0 = time.time()\n",
    "    final_average100 = run(env=env_train, frames = 600000, eps_fixed=eps_fixed, eps_frames=5000, min_eps=0.025)\n",
    "    t1 = time.time()\n",
    "    \n",
    "    print(\"Training time: {}min\".format(round((t1-t0)/60,2)))\n",
    "    torch.save(agent.qnetwork_local.state_dict(), \"IQN-IQN\"+\".pth\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
