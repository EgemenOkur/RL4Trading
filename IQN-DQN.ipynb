{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "import random\n",
    "import math\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from collections import deque, namedtuple\n",
    "import time\n",
    "import gym\n",
    "import os\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "def weight_init(layers):\n",
    "    for layer in layers:\n",
    "        torch.nn.init.kaiming_normal_(layer.weight, nonlinearity='relu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3.common.vec_env.base_vec_env import VecEnv, VecEnvStepReturn, VecEnvWrapper\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from stockstats import StockDataFrame as Sdf\n",
    "\n",
    "import datetime\n",
    "\n",
    "import datetime\n",
    "import os\n",
    "\n",
    "TRAINING_DATA_FILE = \"dataprocessing/Yfinance_Data.csv\"\n",
    "\n",
    "now = datetime.datetime.now()\n",
    "TRAINED_MODEL_DIR = f\"trained_models/{now}\"\n",
    "os.makedirs(TRAINED_MODEL_DIR)\n",
    "\n",
    "TESTING_DATA_FILE = \"test.csv\"\n",
    "\n",
    "def load_dataset(*, file_name: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    load csv dataset from path\n",
    "    :return: (df) pandas dataframe\n",
    "    \"\"\"\n",
    "    # _data = pd.read_csv(f\"{config.DATASET_DIR}/{file_name}\")\n",
    "    _data = pd.read_csv(file_name)\n",
    "\n",
    "    return _data\n",
    "\n",
    "\n",
    "def data_split(df, start, end):\n",
    "    \"\"\"\n",
    "    split the dataset into training or testing using date\n",
    "    :param data: (df) pandas dataframe, start, end\n",
    "    :return: (df) pandas dataframe\n",
    "    \"\"\"\n",
    "    data = df[(df.datadate >= start) & (df.datadate < end)]\n",
    "    data = data.sort_values(['datadate', 'tic'], ignore_index=True)\n",
    "\n",
    "\n",
    "    # data  = data[final_columns]\n",
    "    data.index = data.datadate.factorize()[0]\n",
    "\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def calculate_price(df):\n",
    "    \"\"\"\n",
    "    calcualte adjusted close price, open-high-low price and volume\n",
    "    :param data: (df) pandas dataframe\n",
    "    :return: (df) pandas dataframe\n",
    "    \"\"\"\n",
    "    data = df.copy()\n",
    "\n",
    "    data = data[['Date', 'tic', 'Close', 'Open', 'High', 'Low', 'Volume','datadate']]\n",
    "    data = data.sort_values(['tic', 'datadate'], ignore_index=True)\n",
    "    return data\n",
    "\n",
    "\n",
    "def add_technical_indicator(df):\n",
    "    \"\"\"\n",
    "    calcualte technical indicators\n",
    "    use stockstats package to add technical inidactors\n",
    "    :param data: (df) pandas dataframe\n",
    "    :return: (df) pandas dataframe\n",
    "    \"\"\"\n",
    "    stock = Sdf.retype(df.copy())\n",
    "\n",
    "    #print(stock)\n",
    "\n",
    "    unique_ticker = stock.tic.unique()\n",
    "\n",
    "    macd = pd.DataFrame()\n",
    "    rsi = pd.DataFrame()\n",
    "    cci = pd.DataFrame()\n",
    "    dx = pd.DataFrame()\n",
    "\n",
    "    # temp = stock[stock.tic == unique_ticker[0]]['macd']\n",
    "    for i in range(len(unique_ticker)):\n",
    "        ## macd\n",
    "        temp_macd = stock[stock.tic == unique_ticker[i]]['macd']\n",
    "        temp_macd = pd.DataFrame(temp_macd)\n",
    "        macd = macd.append(temp_macd, ignore_index=True)\n",
    "        ## rsi\n",
    "        temp_rsi = stock[stock.tic == unique_ticker[i]]['rsi_30']\n",
    "        temp_rsi = pd.DataFrame(temp_rsi)\n",
    "        rsi = rsi.append(temp_rsi, ignore_index=True)\n",
    "        ## cci\n",
    "        temp_cci = stock[stock.tic == unique_ticker[i]]['cci_30']\n",
    "        temp_cci = pd.DataFrame(temp_cci)\n",
    "        cci = cci.append(temp_cci, ignore_index=True)\n",
    "        ## adx\n",
    "        temp_dx = stock[stock.tic == unique_ticker[i]]['dx_30']\n",
    "        temp_dx = pd.DataFrame(temp_dx)\n",
    "        dx = dx.append(temp_dx, ignore_index=True)\n",
    "\n",
    "    df['macd'] = macd\n",
    "    df['rsi'] = rsi\n",
    "    df['cci'] = cci\n",
    "    df['adx'] = dx\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def preprocess_data():\n",
    "    \"\"\"data preprocessing pipeline\"\"\"\n",
    "    start = datetime.datetime(2010, 12, 1)\n",
    "    df = load_dataset(file_name=TRAINING_DATA_FILE)\n",
    "    # get data after 2010\n",
    "    # df = df[df.Date >= start]\n",
    "    # calcualte adjusted price\n",
    "    df_preprocess = calculate_price(df)\n",
    "    # add technical indicators using stockstats\n",
    "    df_final = add_technical_indicator(df_preprocess)\n",
    "    # fill the missing values at the beginning\n",
    "    df_final.fillna(method='bfill', inplace=True)\n",
    "    return df_final\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IQN(nn.Module):\n",
    "    def __init__(self, state_size, action_size,layer_size, n_step, seed, layer_type=\"ff\"):\n",
    "        super(IQN, self).__init__()\n",
    "        self.seed = torch.manual_seed(seed)\n",
    "        self.input_shape = state_size\n",
    "        self.action_size = action_size\n",
    "        self.K = 32\n",
    "        self.N = 8\n",
    "        self.n_cos = 64\n",
    "        self.layer_size = layer_size\n",
    "        self.pis = torch.FloatTensor([np.pi*i for i in range(self.n_cos)]).view(1,1,self.n_cos).to(device) # Starting from 0 as in the paper \n",
    "\n",
    "        self.head = nn.Linear(self.input_shape, layer_size) # cound be a cnn \n",
    "        self.cos_embedding = nn.Linear(self.n_cos, layer_size)\n",
    "        self.ff_1 = nn.Linear(layer_size, layer_size)\n",
    "        self.ff_2 = nn.Linear(layer_size, action_size)\n",
    "        #weight_init([self.head_1, self.ff_1])\n",
    "\n",
    "\n",
    "        \n",
    "    def calc_cos(self, batch_size, n_tau=8):\n",
    "        \"\"\"\n",
    "        Calculating the cosinus values depending on the number of tau samples\n",
    "        \"\"\"\n",
    "        taus = torch.rand(batch_size, n_tau).to(device).unsqueeze(-1) #(batch_size, n_tau, 1)\n",
    "        cos = torch.cos(taus*self.pis)\n",
    "\n",
    "        assert cos.shape == (batch_size,n_tau,self.n_cos), \"cos shape is incorrect\"\n",
    "        return cos, taus\n",
    "    \n",
    "    def forward(self, input, num_tau=8):\n",
    "        \"\"\"\n",
    "        Quantile Calculation depending on the number of tau\n",
    "        \n",
    "        Return:\n",
    "        quantiles [ shape of (batch_size, num_tau, action_size)]\n",
    "        taus [shape of ((batch_size, num_tau, 1))]\n",
    "        \n",
    "        \"\"\"\n",
    "        batch_size = input.shape[0]\n",
    "        \n",
    "        #print(\"self.head(input):{}\".format(self.head(input).shape))\n",
    "        \n",
    "        x = torch.relu(self.head(input))\n",
    "    \n",
    "        #print(\"batch_size:{}\".format(batch_size))\n",
    "        #print(\"X:{}\".format(x.shape))\n",
    "        \n",
    "        cos, taus = self.calc_cos(batch_size, num_tau) # cos shape (batch, num_tau, layer_size)\n",
    "        \n",
    "        #print(\"cos:{}\".format(cos.shape))\n",
    "        #print(\"taus:{}\".format(taus.shape))\n",
    "        \n",
    "        \n",
    "        cos = cos.view(batch_size*num_tau, self.n_cos)\n",
    "        cos_x = torch.relu(self.cos_embedding(cos)).view(batch_size, num_tau, self.layer_size) # (batch, n_tau, layer)\n",
    "        \n",
    "        # x has shape (batch, layer_size) for multiplication –> reshape to (batch, 1, layer)\n",
    "        #x = (x.unsqueeze(1)*cos_x).view(batch_size*num_tau, self.layer_size)\n",
    "        #print(\"x:{},cos_x Shape:{},batch_size:{},layer_size:{}\".format(x.shape,cos_x.shape,batch_size,self.layer_size))\n",
    "        x = (x.unsqueeze(1) * cos_x).view(batch_size * num_tau, self.layer_size)\n",
    "        #print(\"---------°°°°°°°°°------X----------°°°°°°°°°°-------:{}\".format(x.shape))\n",
    "        \n",
    "        x = torch.relu(self.ff_1(x))\n",
    "        out = self.ff_2(x)\n",
    "        #print(\"---------°°°°°°°°°------out----------°°°°°°°°°°-------:{}\".format(out.shape))\n",
    "        \n",
    "        return out.view(batch_size, num_tau, self.action_size), taus\n",
    "    \n",
    "    def get_action(self, inputs):\n",
    "        quantiles, _ = self.forward(inputs, self.K)\n",
    "        #print(\"quantiles:{}\".format(quantiles.shape))\n",
    "        actions = quantiles.mean(dim=1) #TODO: actions space= torch.Size([1, 32, 30])\n",
    "        #print(\"action space quantile:{}\".format(actions))\n",
    "        return actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    \"\"\"Fixed-size buffer to store experience tuples.\"\"\"\n",
    "\n",
    "    def __init__(self, buffer_size, batch_size, device, seed, gamma, n_step=1):\n",
    "        \"\"\"Initialize a ReplayBuffer object.\n",
    "        Params\n",
    "        ======\n",
    "            buffer_size (int): maximum size of buffer\n",
    "            batch_size (int): size of each training batch\n",
    "            seed (int): random seed\n",
    "        \"\"\"\n",
    "        self.device = device\n",
    "        self.memory = deque(maxlen=buffer_size)  \n",
    "        self.batch_size = batch_size\n",
    "        self.experience = namedtuple(\"Experience\", field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
    "        self.seed = random.seed(seed)\n",
    "        self.gamma = gamma\n",
    "        self.n_step = n_step\n",
    "        self.n_step_buffer = deque(maxlen=self.n_step)\n",
    "    \n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Add a new experience to memory.\"\"\"\n",
    "        #print(\"before:\", state,action,reward,next_state, done)\n",
    "        self.n_step_buffer.append((state, action, reward, next_state, done))\n",
    "        if len(self.n_step_buffer) == self.n_step:\n",
    "            state, action, reward, next_state, done = self.calc_multistep_return()\n",
    "            #print(\"after:\",state,action,reward,next_state, done)\n",
    "            e = self.experience(state, action, reward, next_state, done)\n",
    "            self.memory.append(e)\n",
    "    \n",
    "    def calc_multistep_return(self):\n",
    "        Return = 0\n",
    "        for idx in range(self.n_step):\n",
    "            Return += self.gamma**idx * self.n_step_buffer[idx][2]\n",
    "        \n",
    "        return self.n_step_buffer[0][0], self.n_step_buffer[0][1], Return, self.n_step_buffer[-1][3], self.n_step_buffer[-1][4]\n",
    "        \n",
    "    \n",
    "    \n",
    "    def sample(self):\n",
    "        \"\"\"Randomly sample a batch of experiences from memory.\"\"\"\n",
    "        experiences = random.sample(self.memory, k=self.batch_size)\n",
    "\n",
    "        states = torch.from_numpy(np.stack([e.state for e in experiences if e is not None])).float().to(self.device)\n",
    "        actions = torch.from_numpy(np.vstack([e.action for e in experiences if e is not None])).long().to(self.device)\n",
    "        rewards = torch.from_numpy(np.vstack([e.reward for e in experiences if e is not None])).float().to(self.device)\n",
    "        next_states = torch.from_numpy(np.stack([e.next_state for e in experiences if e is not None])).float().to(self.device)\n",
    "        dones = torch.from_numpy(np.vstack([e.done for e in experiences if e is not None]).astype(np.uint8)).float().to(self.device)\n",
    "  \n",
    "        return (states, actions, rewards, next_states, dones)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Return the current size of internal memory.\"\"\"\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class DQN_Agent():\n",
    "    \"\"\"Interacts with and learns from the environment.\"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 state_size,\n",
    "                 action_size,\n",
    "                 layer_size,\n",
    "                 n_step,\n",
    "                 BATCH_SIZE,\n",
    "                 BUFFER_SIZE,\n",
    "                 LR,\n",
    "                 TAU,\n",
    "                 GAMMA,\n",
    "                 UPDATE_EVERY,\n",
    "                 device,\n",
    "                 seed):\n",
    "        \"\"\"Initialize an Agent object.\n",
    "        \n",
    "        Params\n",
    "        ======\n",
    "            state_size (int): dimension of each state\n",
    "            action_size (int): dimension of each action\n",
    "            layer_size (int): size of the hidden layer\n",
    "            BATCH_SIZE (int): size of the training batch\n",
    "            BUFFER_SIZE (int): size of the replay memory\n",
    "            LR (float): learning rate\n",
    "            TAU (float): tau for soft updating the network weights\n",
    "            GAMMA (float): discount factor\n",
    "            UPDATE_EVERY (int): update frequency\n",
    "            device (str): device that is used for the compute\n",
    "            seed (int): random seed\n",
    "        \"\"\"\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.seed = random.seed(seed)\n",
    "        self.device = device\n",
    "        self.TAU = TAU\n",
    "        self.GAMMA = GAMMA\n",
    "        self.UPDATE_EVERY = UPDATE_EVERY\n",
    "        self.BATCH_SIZE = BATCH_SIZE\n",
    "        self.Q_updates = 0\n",
    "        self.n_step = n_step\n",
    "        self.action = []\n",
    "\n",
    "        self.action_step = 30\n",
    "\n",
    "        # IQN-Network\n",
    "        self.qnetwork_local = IQN(state_size, action_size,layer_size, n_step, seed).to(device)\n",
    "        self.qnetwork_target = IQN(state_size, action_size,layer_size, n_step, seed).to(device)\n",
    "\n",
    "        self.optimizer = optim.Adam(self.qnetwork_local.parameters(), lr=LR)\n",
    "        #print(self.qnetwork_local)\n",
    "        \n",
    "        # Replay memory\n",
    "        self.memory = ReplayBuffer(BUFFER_SIZE, BATCH_SIZE, self.device, seed, self.GAMMA, n_step)\n",
    "        \n",
    "        # Initialize time step (for updating every UPDATE_EVERY steps)\n",
    "        self.t_step = 0\n",
    "    \n",
    "    def step(self, state, action, reward, next_state, done, writer):\n",
    "        # Save experience in replay memory\n",
    "        #print(\"to memory action:{},state:{},next_state\".format(action,state.shape,next_state.shape))\n",
    "        self.memory.add(state, action, reward, next_state, done)\n",
    "        \n",
    "        # Learn every UPDATE_EVERY time steps.\n",
    "        self.t_step = (self.t_step + 1) % self.UPDATE_EVERY\n",
    "        if self.t_step == 0:\n",
    "            # If enough samples are available in memory, get random subset and learn\n",
    "            if len(self.memory) > self.BATCH_SIZE:\n",
    "                \n",
    "                experiences = self.memory.sample()\n",
    "                #print(\"experiences:{}\".format(experiences))\n",
    "                loss = self.learn(experiences)\n",
    "                self.Q_updates += 1\n",
    "                writer.add_scalar(\"Q_loss\", loss, self.Q_updates)\n",
    "\n",
    "    def act(self, state, eps=0.):\n",
    "        \"\"\"Returns actions for given state as per current policy. Acting only every 4 frames!\n",
    "        \n",
    "        Params\n",
    "        ======\n",
    "            frame: to adjust epsilon\n",
    "            state (array_like): current state\n",
    "            \n",
    "        \"\"\"\n",
    "        #print(\"without np.array:{}\".format(state.shape))\n",
    "\n",
    "        state = np.array(state)\n",
    "\n",
    "        #print(\"this is the state space before torch.from_numpy:{}\".format(state.shape))\n",
    "        \n",
    "        state = torch.from_numpy(state).float().unsqueeze(0).to(self.device) #WHY?\n",
    "        \n",
    "        #print(\"this is the state space after torch.from_numpy:{}\".format(state.shape))\n",
    "        \n",
    "        \n",
    "        self.qnetwork_local.eval() #WHY?\n",
    "        \n",
    "        \n",
    "        with torch.no_grad():\n",
    "            #print(\"this is the state space:{}\".format(state.shape))\n",
    "            action_values = self.qnetwork_local.get_action(state) # 30 dimensions are coming back.\n",
    "            #print('action_value:{}'.format(action_values))\n",
    "        self.qnetwork_local.train()\n",
    "\n",
    "        # Epsilon-greedy action selection\n",
    "        if random.random() > eps: # select greedy action if random number is higher than epsilon or noisy network is used!\n",
    "            action = np.argmax(action_values.cpu().data.numpy())\n",
    "            self.last_action = action\n",
    "            return action\n",
    "        else:\n",
    "            action = random.choice(np.arange(self.action_size))\n",
    "            self.last_action = action \n",
    "            return action\n",
    "\n",
    "\n",
    "\n",
    "    def learn(self, experiences):\n",
    "        \"\"\"Update value parameters using given batch of experience tuples.\n",
    "        Params\n",
    "        ======\n",
    "            experiences (Tuple[torch.Tensor]): tuple of (s, a, r, s', done) tuples \n",
    "            gamma (float): discount factor\n",
    "        \"\"\"\n",
    "        self.optimizer.zero_grad()\n",
    "        states, actions, rewards, next_states, dones = experiences\n",
    "        #print(\"learning states:{}, next_states:{}\".format(states.shape, next_states.shape))\n",
    "        # Get max predicted Q values (for next states) from target model\n",
    "        Q_targets_next, _ = self.qnetwork_target(next_states)\n",
    "        \n",
    "        #print(\"--Q_targets_next :{}\".format(Q_targets_next.shape))\n",
    "        \n",
    "        #print(\"---->Q_targets_next:{}\".format(Q_targets_next))\n",
    "        #print('------------------------------------')\n",
    "        #print(\"--Q_targets_next detach max:{}\".format(Q_targets_next.detach().max(2)))\n",
    "        \n",
    "        \n",
    "        \n",
    "        #print(\"--Q_targets_next.detach().max(2)[0].unsqueeze(1):{}\".format(Q_targets_next.detach().max(2)[0].unsqueeze(1)))\n",
    "        Q_targets_next = Q_targets_next.detach().max(2)[0].unsqueeze(1) # (batch_size, 1, N)\n",
    "        \n",
    "        # Compute Q targets for current states \n",
    "        Q_targets = rewards.unsqueeze(-1) + (self.GAMMA**self.n_step * Q_targets_next * (1. - dones.unsqueeze(-1)))\n",
    "        # Get expected Q values from local model\n",
    "        Q_expected, taus = self.qnetwork_local(states)\n",
    "        \n",
    "        #print(\"rewards:{}\".format(rewards.shape))\n",
    "        #print(\"Q_targets_Shape:{}\".format(Q_targets.shape))\n",
    "        #print(\"actions shape:{}\".format(actions.shape))\n",
    "        #print(\"Q_expected shape:{}\".format(Q_expected.shape))\n",
    "        #print(\"actions.unsqueeze(-1).shape:{}\".format(actions.unsqueeze(-1).shape))\n",
    "        #print(\"actions:{}\".format(actions))\n",
    "        #print(\"Q_expected:{}\".format(Q_expected))\n",
    "        #print(\"actions.unsqueeze(-1){}\".format(actions.unsqueeze(-1)))\n",
    "        Q_expected_2 = Q_expected.gather(2, actions.unsqueeze(-1))\n",
    "\n",
    "        #print(\"Q_expected.gather(2, actions.unsqueeze(-1):{}\".format(Q_expected_2.shape))\n",
    "        \n",
    "        Q_expected = Q_expected.gather(2, actions[0].unsqueeze(-1).expand(self.BATCH_SIZE, 8, 1))\n",
    "        #print(\"Final what we need Q_expected-----:{}\".format(Q_expected.shape))\n",
    "\n",
    "        # Quantile Huber loss\n",
    "        td_error = Q_targets - Q_expected\n",
    "        #print(\"td_error.shape:{}\".format(td_error.shape))\n",
    "        #print(\"Q_expected.shape:{}\".format(Q_expected.shape))\n",
    "        #print(\"td_error:{}\".format(td_error.shape))\n",
    "        assert td_error.shape == (self.BATCH_SIZE, 8, 8), \"wrong td error shape\"\n",
    "        huber_l = calculate_huber_loss(td_error, 1.0)\n",
    "        quantil_l = abs(taus -(td_error.detach() < 0).float()) * huber_l / 1.0\n",
    "        \n",
    "        loss = quantil_l.sum(dim=1).mean(dim=1) # , keepdim=True if per weights get multipl\n",
    "        loss = loss.mean()\n",
    "\n",
    "\n",
    "        # Minimize the loss\n",
    "        loss.backward()\n",
    "        #clip_grad_norm_(self.qnetwork_local.parameters(),1)\n",
    "        self.optimizer.step()\n",
    "\n",
    "        # ------------------- update target network ------------------- #\n",
    "        self.soft_update(self.qnetwork_local, self.qnetwork_target)\n",
    "        return loss.detach().cpu().numpy()            \n",
    "\n",
    "    def soft_update(self, local_model, target_model):\n",
    "        \"\"\"Soft update model parameters.\n",
    "        θ_target = τ*θ_local + (1 - τ)*θ_target\n",
    "        Params\n",
    "        ======\n",
    "            local_model (PyTorch model): weights will be copied from\n",
    "            target_model (PyTorch model): weights will be copied to\n",
    "            tau (float): interpolation parameter \n",
    "        \"\"\"\n",
    "        for target_param, local_param in zip(target_model.parameters(), local_model.parameters()):\n",
    "            target_param.data.copy_(self.TAU*local_param.data + (1.0-self.TAU)*target_param.data)\n",
    "\n",
    "\n",
    "def calculate_huber_loss(td_errors, k=1.0):\n",
    "    \"\"\"\n",
    "    Calculate huber loss element-wisely depending on kappa k.\n",
    "    \"\"\"\n",
    "    loss = torch.where(td_errors.abs() <= k, 0.5 * td_errors.pow(2), k * (td_errors.abs() - 0.5 * k))\n",
    "    #print('this is huber loss: {}'.format(loss.shape))\n",
    "    assert loss.shape == (td_errors.shape[0], 8, 8), \"huber loss has wrong shape\"\n",
    "    return loss\n",
    "    \n",
    "def eval_runs(eps, frame):\n",
    "    \"\"\"\n",
    "    Makes an evaluation run with the current epsilon\n",
    "    \"\"\"\n",
    "    print(\"-----------------------------------------evaluating-----------------------------------------\")\n",
    "    env = gym.make(\"Acrobot-v1\") # TODO:\n",
    "    reward_batch = []\n",
    "    for i in range(5):\n",
    "        state = env.reset()\n",
    "        rewards = 0\n",
    "        while True:\n",
    "            action = agent.act(state, eps)\n",
    "            state, reward, done, _ = env.step(action)\n",
    "            rewards += reward\n",
    "            if done:\n",
    "                break\n",
    "        reward_batch.append(rewards)\n",
    "        \n",
    "    writer.add_scalar(\"Reward\", np.mean(reward_batch), frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using  cpu\n",
      "Action Space: 3\n",
      "State Space: 19\n",
      "[1.00000000e+03 3.05728571e+01 4.09200000e+01 5.61800000e+01\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 5.60928564e-01\n",
      " 3.58722215e-01 6.24669814e-01 6.21337044e+01 5.67090572e+01\n",
      " 5.85111632e+01 1.68824972e+02 2.09000651e+00 8.31548524e+01\n",
      " 3.37911464e+01 1.15370605e+01 1.08349581e+01]\n",
      "Finished\n",
      "[1315.2506514799938, 107.32, 70.16, 146.41, 1.0, 0.0, 1.0, -2.466788775, -0.5672052238, -0.1475155644, 42.30524752, 44.89929352, 52.58918069, -98.67248174, -15.74083296, 29.12710167, 29.43194077, 7.763541354, 9.869031232]\n",
      "end_total_asset:1568.9806514799939\n",
      "Sharpe:  0.38604505994930216\n",
      "[1.00000000e+03 3.05728571e+01 4.09200000e+01 5.61800000e+01\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 5.60928564e-01\n",
      " 3.58722215e-01 6.24669814e-01 6.21337044e+01 5.67090572e+01\n",
      " 5.85111632e+01 1.68824972e+02 2.09000651e+00 8.31548524e+01\n",
      " 3.37911464e+01 1.15370605e+01 1.08349581e+01]\n",
      "Episode 1\tFrame 4528 \tAverage Score: 0.06[1.00000000e+03 3.05728571e+01 4.09200000e+01 5.61800000e+01\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 5.60928564e-01\n",
      " 3.58722215e-01 6.24669814e-01 6.21337044e+01 5.67090572e+01\n",
      " 5.85111632e+01 1.68824972e+02 2.09000651e+00 8.31548524e+01\n",
      " 3.37911464e+01 1.15370605e+01 1.08349581e+01]\n",
      "Finished\n",
      "[1155.1671780942263, 107.32, 70.16, 146.41, 0.0, 0.0, 0.0, -2.466788775, -0.5672052238, -0.1475155644, 42.30524752, 44.89929352, 52.58918069, -98.67248174, -15.74083296, 29.12710167, 29.43194077, 7.763541354, 9.869031232]\n",
      "end_total_asset:1155.1671780942263\n",
      "Sharpe:  0.17748014135161636\n",
      "[1.00000000e+03 3.05728571e+01 4.09200000e+01 5.61800000e+01\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 5.60928564e-01\n",
      " 3.58722215e-01 6.24669814e-01 6.21337044e+01 5.67090572e+01\n",
      " 5.85111632e+01 1.68824972e+02 2.09000651e+00 8.31548524e+01\n",
      " 3.37911464e+01 1.15370605e+01 1.08349581e+01]\n",
      "Episode 2\tFrame 9056 \tAverage Score: 0.04[1.00000000e+03 3.05728571e+01 4.09200000e+01 5.61800000e+01\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 5.60928564e-01\n",
      " 3.58722215e-01 6.24669814e-01 6.21337044e+01 5.67090572e+01\n",
      " 5.85111632e+01 1.68824972e+02 2.09000651e+00 8.31548524e+01\n",
      " 3.37911464e+01 1.15370605e+01 1.08349581e+01]\n",
      "Finished\n",
      "[980.8064689413973, 107.32, 70.16, 146.41, 1.0, 1.0, 0.0, -2.466788775, -0.5672052238, -0.1475155644, 42.30524752, 44.89929352, 52.58918069, -98.67248174, -15.74083296, 29.12710167, 29.43194077, 7.763541354, 9.869031232]\n",
      "end_total_asset:1158.2864689413973\n",
      "Sharpe:  0.19597954444473276\n",
      "[1.00000000e+03 3.05728571e+01 4.09200000e+01 5.61800000e+01\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 5.60928564e-01\n",
      " 3.58722215e-01 6.24669814e-01 6.21337044e+01 5.67090572e+01\n",
      " 5.85111632e+01 1.68824972e+02 2.09000651e+00 8.31548524e+01\n",
      " 3.37911464e+01 1.15370605e+01 1.08349581e+01]\n",
      "Episode 3\tFrame 13584 \tAverage Score: 0.03[1.00000000e+03 3.05728571e+01 4.09200000e+01 5.61800000e+01\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 5.60928564e-01\n",
      " 3.58722215e-01 6.24669814e-01 6.21337044e+01 5.67090572e+01\n",
      " 5.85111632e+01 1.68824972e+02 2.09000651e+00 8.31548524e+01\n",
      " 3.37911464e+01 1.15370605e+01 1.08349581e+01]\n",
      "Finished\n",
      "[688.2798922214382, 107.32, 70.16, 146.41, 3.0, 3.0, 3.0, -2.466788775, -0.5672052238, -0.1475155644, 42.30524752, 44.89929352, 52.58918069, -98.67248174, -15.74083296, 29.12710167, 29.43194077, 7.763541354, 9.869031232]\n",
      "end_total_asset:1659.9498922214382\n",
      "Sharpe:  0.5652674359873382\n",
      "[1.00000000e+03 3.05728571e+01 4.09200000e+01 5.61800000e+01\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 5.60928564e-01\n",
      " 3.58722215e-01 6.24669814e-01 6.21337044e+01 5.67090572e+01\n",
      " 5.85111632e+01 1.68824972e+02 2.09000651e+00 8.31548524e+01\n",
      " 3.37911464e+01 1.15370605e+01 1.08349581e+01]\n",
      "Episode 4\tFrame 18112 \tAverage Score: 0.04[1.00000000e+03 3.05728571e+01 4.09200000e+01 5.61800000e+01\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 5.60928564e-01\n",
      " 3.58722215e-01 6.24669814e-01 6.21337044e+01 5.67090572e+01\n",
      " 5.85111632e+01 1.68824972e+02 2.09000651e+00 8.31548524e+01\n",
      " 3.37911464e+01 1.15370605e+01 1.08349581e+01]\n",
      "Finished\n",
      "[1292.2109885142686, 107.32, 70.16, 146.41, 0.0, 0.0, 1.0, -2.466788775, -0.5672052238, -0.1475155644, 42.30524752, 44.89929352, 52.58918069, -98.67248174, -15.74083296, 29.12710167, 29.43194077, 7.763541354, 9.869031232]\n",
      "end_total_asset:1438.6209885142687\n",
      "Sharpe:  0.3490636417411459\n",
      "[1.00000000e+03 3.05728571e+01 4.09200000e+01 5.61800000e+01\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 5.60928564e-01\n",
      " 3.58722215e-01 6.24669814e-01 6.21337044e+01 5.67090572e+01\n",
      " 5.85111632e+01 1.68824972e+02 2.09000651e+00 8.31548524e+01\n",
      " 3.37911464e+01 1.15370605e+01 1.08349581e+01]\n",
      "Episode 5\tFrame 22640 \tAverage Score: 0.04[1.00000000e+03 3.05728571e+01 4.09200000e+01 5.61800000e+01\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 5.60928564e-01\n",
      " 3.58722215e-01 6.24669814e-01 6.21337044e+01 5.67090572e+01\n",
      " 5.85111632e+01 1.68824972e+02 2.09000651e+00 8.31548524e+01\n",
      " 3.37911464e+01 1.15370605e+01 1.08349581e+01]\n",
      "Finished\n",
      "[1515.2360714627284, 107.32, 70.16, 146.41, 0.0, 0.0, 0.0, -2.466788775, -0.5672052238, -0.1475155644, 42.30524752, 44.89929352, 52.58918069, -98.67248174, -15.74083296, 29.12710167, 29.43194077, 7.763541354, 9.869031232]\n",
      "end_total_asset:1515.2360714627284\n",
      "Sharpe:  0.35196272831598524\n",
      "[1.00000000e+03 3.05728571e+01 4.09200000e+01 5.61800000e+01\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 5.60928564e-01\n",
      " 3.58722215e-01 6.24669814e-01 6.21337044e+01 5.67090572e+01\n",
      " 5.85111632e+01 1.68824972e+02 2.09000651e+00 8.31548524e+01\n",
      " 3.37911464e+01 1.15370605e+01 1.08349581e+01]\n",
      "Episode 6\tFrame 27168 \tAverage Score: 0.04[1.00000000e+03 3.05728571e+01 4.09200000e+01 5.61800000e+01\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 5.60928564e-01\n",
      " 3.58722215e-01 6.24669814e-01 6.21337044e+01 5.67090572e+01\n",
      " 5.85111632e+01 1.68824972e+02 2.09000651e+00 8.31548524e+01\n",
      " 3.37911464e+01 1.15370605e+01 1.08349581e+01]\n",
      "Finished\n",
      "[62.45939841275796, 107.32, 70.16, 146.41, 6.0, 9.0, 5.0, -2.466788775, -0.5672052238, -0.1475155644, 42.30524752, 44.89929352, 52.58918069, -98.67248174, -15.74083296, 29.12710167, 29.43194077, 7.763541354, 9.869031232]\n",
      "end_total_asset:2069.8693984127576\n",
      "Sharpe:  0.4962258118821621\n",
      "[1.00000000e+03 3.05728571e+01 4.09200000e+01 5.61800000e+01\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 5.60928564e-01\n",
      " 3.58722215e-01 6.24669814e-01 6.21337044e+01 5.67090572e+01\n",
      " 5.85111632e+01 1.68824972e+02 2.09000651e+00 8.31548524e+01\n",
      " 3.37911464e+01 1.15370605e+01 1.08349581e+01]\n",
      "Episode 7\tFrame 31696 \tAverage Score: 0.05[1.00000000e+03 3.05728571e+01 4.09200000e+01 5.61800000e+01\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 5.60928564e-01\n",
      " 3.58722215e-01 6.24669814e-01 6.21337044e+01 5.67090572e+01\n",
      " 5.85111632e+01 1.68824972e+02 2.09000651e+00 8.31548524e+01\n",
      " 3.37911464e+01 1.15370605e+01 1.08349581e+01]\n",
      "Finished\n",
      "[94.16456711129638, 107.32, 70.16, 146.41, 5.0, 6.0, 6.0, -2.466788775, -0.5672052238, -0.1475155644, 42.30524752, 44.89929352, 52.58918069, -98.67248174, -15.74083296, 29.12710167, 29.43194077, 7.763541354, 9.869031232]\n",
      "end_total_asset:1930.1845671112965\n",
      "Sharpe:  0.5348605941973322\n",
      "[1.00000000e+03 3.05728571e+01 4.09200000e+01 5.61800000e+01\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 5.60928564e-01\n",
      " 3.58722215e-01 6.24669814e-01 6.21337044e+01 5.67090572e+01\n",
      " 5.85111632e+01 1.68824972e+02 2.09000651e+00 8.31548524e+01\n",
      " 3.37911464e+01 1.15370605e+01 1.08349581e+01]\n",
      "Episode 8\tFrame 36224 \tAverage Score: 0.06[1.00000000e+03 3.05728571e+01 4.09200000e+01 5.61800000e+01\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 5.60928564e-01\n",
      " 3.58722215e-01 6.24669814e-01 6.21337044e+01 5.67090572e+01\n",
      " 5.85111632e+01 1.68824972e+02 2.09000651e+00 8.31548524e+01\n",
      " 3.37911464e+01 1.15370605e+01 1.08349581e+01]\n",
      "Finished\n",
      "[1130.7064022985207, 107.32, 70.16, 146.41, 1.0, 0.0, 0.0, -2.466788775, -0.5672052238, -0.1475155644, 42.30524752, 44.89929352, 52.58918069, -98.67248174, -15.74083296, 29.12710167, 29.43194077, 7.763541354, 9.869031232]\n",
      "end_total_asset:1238.0264022985207\n",
      "Sharpe:  0.19757305952962248\n",
      "[1.00000000e+03 3.05728571e+01 4.09200000e+01 5.61800000e+01\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 5.60928564e-01\n",
      " 3.58722215e-01 6.24669814e-01 6.21337044e+01 5.67090572e+01\n",
      " 5.85111632e+01 1.68824972e+02 2.09000651e+00 8.31548524e+01\n",
      " 3.37911464e+01 1.15370605e+01 1.08349581e+01]\n",
      "Episode 9\tFrame 40752 \tAverage Score: 0.05[1.00000000e+03 3.05728571e+01 4.09200000e+01 5.61800000e+01\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 5.60928564e-01\n",
      " 3.58722215e-01 6.24669814e-01 6.21337044e+01 5.67090572e+01\n",
      " 5.85111632e+01 1.68824972e+02 2.09000651e+00 8.31548524e+01\n",
      " 3.37911464e+01 1.15370605e+01 1.08349581e+01]\n",
      "Finished\n",
      "[1000.6423772142309, 107.32, 70.16, 146.41, 2.0, 4.0, 3.0, -2.466788775, -0.5672052238, -0.1475155644, 42.30524752, 44.89929352, 52.58918069, -98.67248174, -15.74083296, 29.12710167, 29.43194077, 7.763541354, 9.869031232]\n",
      "end_total_asset:1935.152377214231\n",
      "Sharpe:  0.5441779170434781\n",
      "[1.00000000e+03 3.05728571e+01 4.09200000e+01 5.61800000e+01\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 5.60928564e-01\n",
      " 3.58722215e-01 6.24669814e-01 6.21337044e+01 5.67090572e+01\n",
      " 5.85111632e+01 1.68824972e+02 2.09000651e+00 8.31548524e+01\n",
      " 3.37911464e+01 1.15370605e+01 1.08349581e+01]\n",
      "Episode 10\tFrame 45280 \tAverage Score: 0.06[1.00000000e+03 3.05728571e+01 4.09200000e+01 5.61800000e+01\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 5.60928564e-01\n",
      " 3.58722215e-01 6.24669814e-01 6.21337044e+01 5.67090572e+01\n",
      " 5.85111632e+01 1.68824972e+02 2.09000651e+00 8.31548524e+01\n",
      " 3.37911464e+01 1.15370605e+01 1.08349581e+01]\n",
      "Finished\n",
      "[1369.1845579656115, 107.32, 70.16, 146.41, 0.0, 0.0, 0.0, -2.466788775, -0.5672052238, -0.1475155644, 42.30524752, 44.89929352, 52.58918069, -98.67248174, -15.74083296, 29.12710167, 29.43194077, 7.763541354, 9.869031232]\n",
      "end_total_asset:1369.1845579656115\n",
      "Sharpe:  0.29509394026843505\n",
      "[1.00000000e+03 3.05728571e+01 4.09200000e+01 5.61800000e+01\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 5.60928564e-01\n",
      " 3.58722215e-01 6.24669814e-01 6.21337044e+01 5.67090572e+01\n",
      " 5.85111632e+01 1.68824972e+02 2.09000651e+00 8.31548524e+01\n",
      " 3.37911464e+01 1.15370605e+01 1.08349581e+01]\n",
      "Episode 11\tFrame 49808 \tAverage Score: 0.05[1.00000000e+03 3.05728571e+01 4.09200000e+01 5.61800000e+01\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 5.60928564e-01\n",
      " 3.58722215e-01 6.24669814e-01 6.21337044e+01 5.67090572e+01\n",
      " 5.85111632e+01 1.68824972e+02 2.09000651e+00 8.31548524e+01\n",
      " 3.37911464e+01 1.15370605e+01 1.08349581e+01]\n",
      "Finished\n",
      "[20.571208584221708, 107.32, 70.16, 146.41, 3.0, 8.0, 2.0, -2.466788775, -0.5672052238, -0.1475155644, 42.30524752, 44.89929352, 52.58918069, -98.67248174, -15.74083296, 29.12710167, 29.43194077, 7.763541354, 9.869031232]\n",
      "end_total_asset:1196.6312085842217\n",
      "Sharpe:  0.168583309541609\n",
      "[1.00000000e+03 3.05728571e+01 4.09200000e+01 5.61800000e+01\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 5.60928564e-01\n",
      " 3.58722215e-01 6.24669814e-01 6.21337044e+01 5.67090572e+01\n",
      " 5.85111632e+01 1.68824972e+02 2.09000651e+00 8.31548524e+01\n",
      " 3.37911464e+01 1.15370605e+01 1.08349581e+01]\n",
      "Episode 12\tFrame 54336 \tAverage Score: 0.05[1.00000000e+03 3.05728571e+01 4.09200000e+01 5.61800000e+01\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 5.60928564e-01\n",
      " 3.58722215e-01 6.24669814e-01 6.21337044e+01 5.67090572e+01\n",
      " 5.85111632e+01 1.68824972e+02 2.09000651e+00 8.31548524e+01\n",
      " 3.37911464e+01 1.15370605e+01 1.08349581e+01]\n",
      "Finished\n",
      "[633.5456894870388, 107.32, 70.16, 146.41, 2.0, 2.0, 2.0, -2.466788775, -0.5672052238, -0.1475155644, 42.30524752, 44.89929352, 52.58918069, -98.67248174, -15.74083296, 29.12710167, 29.43194077, 7.763541354, 9.869031232]\n",
      "end_total_asset:1281.3256894870387\n",
      "Sharpe:  0.2230291249796406\n",
      "[1.00000000e+03 3.05728571e+01 4.09200000e+01 5.61800000e+01\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 5.60928564e-01\n",
      " 3.58722215e-01 6.24669814e-01 6.21337044e+01 5.67090572e+01\n",
      " 5.85111632e+01 1.68824972e+02 2.09000651e+00 8.31548524e+01\n",
      " 3.37911464e+01 1.15370605e+01 1.08349581e+01]\n",
      "Episode 13\tFrame 58864 \tAverage Score: 0.05[1.00000000e+03 3.05728571e+01 4.09200000e+01 5.61800000e+01\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 5.60928564e-01\n",
      " 3.58722215e-01 6.24669814e-01 6.21337044e+01 5.67090572e+01\n",
      " 5.85111632e+01 1.68824972e+02 2.09000651e+00 8.31548524e+01\n",
      " 3.37911464e+01 1.15370605e+01 1.08349581e+01]\n",
      "Finished\n",
      "[360.5876117856126, 107.32, 70.16, 146.41, 4.0, 1.0, 5.0, -2.466788775, -0.5672052238, -0.1475155644, 42.30524752, 44.89929352, 52.58918069, -98.67248174, -15.74083296, 29.12710167, 29.43194077, 7.763541354, 9.869031232]\n",
      "end_total_asset:1592.0776117856124\n",
      "Sharpe:  0.42025313614434506\n",
      "[1.00000000e+03 3.05728571e+01 4.09200000e+01 5.61800000e+01\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 5.60928564e-01\n",
      " 3.58722215e-01 6.24669814e-01 6.21337044e+01 5.67090572e+01\n",
      " 5.85111632e+01 1.68824972e+02 2.09000651e+00 8.31548524e+01\n",
      " 3.37911464e+01 1.15370605e+01 1.08349581e+01]\n",
      "Episode 14\tFrame 63392 \tAverage Score: 0.05[1.00000000e+03 3.05728571e+01 4.09200000e+01 5.61800000e+01\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 5.60928564e-01\n",
      " 3.58722215e-01 6.24669814e-01 6.21337044e+01 5.67090572e+01\n",
      " 5.85111632e+01 1.68824972e+02 2.09000651e+00 8.31548524e+01\n",
      " 3.37911464e+01 1.15370605e+01 1.08349581e+01]\n",
      "Finished\n",
      "[865.341239152825, 107.32, 70.16, 146.41, 0.0, 3.0, 2.0, -2.466788775, -0.5672052238, -0.1475155644, 42.30524752, 44.89929352, 52.58918069, -98.67248174, -15.74083296, 29.12710167, 29.43194077, 7.763541354, 9.869031232]\n",
      "end_total_asset:1368.641239152825\n",
      "Sharpe:  0.28014171524382553\n",
      "[1.00000000e+03 3.05728571e+01 4.09200000e+01 5.61800000e+01\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 5.60928564e-01\n",
      " 3.58722215e-01 6.24669814e-01 6.21337044e+01 5.67090572e+01\n",
      " 5.85111632e+01 1.68824972e+02 2.09000651e+00 8.31548524e+01\n",
      " 3.37911464e+01 1.15370605e+01 1.08349581e+01]\n",
      "Episode 15\tFrame 67920 \tAverage Score: 0.05[1.00000000e+03 3.05728571e+01 4.09200000e+01 5.61800000e+01\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 5.60928564e-01\n",
      " 3.58722215e-01 6.24669814e-01 6.21337044e+01 5.67090572e+01\n",
      " 5.85111632e+01 1.68824972e+02 2.09000651e+00 8.31548524e+01\n",
      " 3.37911464e+01 1.15370605e+01 1.08349581e+01]\n",
      "Finished\n",
      "[698.0347818385399, 107.32, 70.16, 146.41, 2.0, 2.0, 3.0, -2.466788775, -0.5672052238, -0.1475155644, 42.30524752, 44.89929352, 52.58918069, -98.67248174, -15.74083296, 29.12710167, 29.43194077, 7.763541354, 9.869031232]\n",
      "end_total_asset:1492.22478183854\n",
      "Sharpe:  0.3755313416059055\n",
      "[1.00000000e+03 3.05728571e+01 4.09200000e+01 5.61800000e+01\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 5.60928564e-01\n",
      " 3.58722215e-01 6.24669814e-01 6.21337044e+01 5.67090572e+01\n",
      " 5.85111632e+01 1.68824972e+02 2.09000651e+00 8.31548524e+01\n",
      " 3.37911464e+01 1.15370605e+01 1.08349581e+01]\n",
      "Episode 16\tFrame 72448 \tAverage Score: 0.05[1.00000000e+03 3.05728571e+01 4.09200000e+01 5.61800000e+01\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 5.60928564e-01\n",
      " 3.58722215e-01 6.24669814e-01 6.21337044e+01 5.67090572e+01\n",
      " 5.85111632e+01 1.68824972e+02 2.09000651e+00 8.31548524e+01\n",
      " 3.37911464e+01 1.15370605e+01 1.08349581e+01]\n",
      "Finished\n",
      "[977.0210845427864, 107.32, 70.16, 146.41, 1.0, 1.0, 1.0, -2.466788775, -0.5672052238, -0.1475155644, 42.30524752, 44.89929352, 52.58918069, -98.67248174, -15.74083296, 29.12710167, 29.43194077, 7.763541354, 9.869031232]\n",
      "end_total_asset:1300.9110845427863\n",
      "Sharpe:  0.24332148824097707\n",
      "[1.00000000e+03 3.05728571e+01 4.09200000e+01 5.61800000e+01\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 5.60928564e-01\n",
      " 3.58722215e-01 6.24669814e-01 6.21337044e+01 5.67090572e+01\n",
      " 5.85111632e+01 1.68824972e+02 2.09000651e+00 8.31548524e+01\n",
      " 3.37911464e+01 1.15370605e+01 1.08349581e+01]\n",
      "Episode 17\tFrame 76976 \tAverage Score: 0.05[1.00000000e+03 3.05728571e+01 4.09200000e+01 5.61800000e+01\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 5.60928564e-01\n",
      " 3.58722215e-01 6.24669814e-01 6.21337044e+01 5.67090572e+01\n",
      " 5.85111632e+01 1.68824972e+02 2.09000651e+00 8.31548524e+01\n",
      " 3.37911464e+01 1.15370605e+01 1.08349581e+01]\n",
      "Finished\n",
      "[49.6629331385207, 107.32, 70.16, 146.41, 3.0, 7.0, 5.0, -2.466788775, -0.5672052238, -0.1475155644, 42.30524752, 44.89929352, 52.58918069, -98.67248174, -15.74083296, 29.12710167, 29.43194077, 7.763541354, 9.869031232]\n",
      "end_total_asset:1594.7929331385205\n",
      "Sharpe:  0.4037022410238967\n",
      "[1.00000000e+03 3.05728571e+01 4.09200000e+01 5.61800000e+01\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 5.60928564e-01\n",
      " 3.58722215e-01 6.24669814e-01 6.21337044e+01 5.67090572e+01\n",
      " 5.85111632e+01 1.68824972e+02 2.09000651e+00 8.31548524e+01\n",
      " 3.37911464e+01 1.15370605e+01 1.08349581e+01]\n",
      "Episode 18\tFrame 81504 \tAverage Score: 0.05[1.00000000e+03 3.05728571e+01 4.09200000e+01 5.61800000e+01\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 5.60928564e-01\n",
      " 3.58722215e-01 6.24669814e-01 6.21337044e+01 5.67090572e+01\n",
      " 5.85111632e+01 1.68824972e+02 2.09000651e+00 8.31548524e+01\n",
      " 3.37911464e+01 1.15370605e+01 1.08349581e+01]\n",
      "Finished\n",
      "[1068.5608120256875, 107.32, 70.16, 146.41, 0.0, 0.0, 2.0, -2.466788775, -0.5672052238, -0.1475155644, 42.30524752, 44.89929352, 52.58918069, -98.67248174, -15.74083296, 29.12710167, 29.43194077, 7.763541354, 9.869031232]\n",
      "end_total_asset:1361.3808120256874\n",
      "Sharpe:  0.2786308122786844\n",
      "[1.00000000e+03 3.05728571e+01 4.09200000e+01 5.61800000e+01\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 5.60928564e-01\n",
      " 3.58722215e-01 6.24669814e-01 6.21337044e+01 5.67090572e+01\n",
      " 5.85111632e+01 1.68824972e+02 2.09000651e+00 8.31548524e+01\n",
      " 3.37911464e+01 1.15370605e+01 1.08349581e+01]\n",
      "Episode 19\tFrame 86032 \tAverage Score: 0.05[1.00000000e+03 3.05728571e+01 4.09200000e+01 5.61800000e+01\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 5.60928564e-01\n",
      " 3.58722215e-01 6.24669814e-01 6.21337044e+01 5.67090572e+01\n",
      " 5.85111632e+01 1.68824972e+02 2.09000651e+00 8.31548524e+01\n",
      " 3.37911464e+01 1.15370605e+01 1.08349581e+01]\n",
      "Finished\n",
      "[321.69539773858173, 107.32, 70.16, 146.41, 3.0, 6.0, 3.0, -2.466788775, -0.5672052238, -0.1475155644, 42.30524752, 44.89929352, 52.58918069, -98.67248174, -15.74083296, 29.12710167, 29.43194077, 7.763541354, 9.869031232]\n",
      "end_total_asset:1503.8453977385818\n",
      "Sharpe:  0.3427788530672083\n",
      "[1.00000000e+03 3.05728571e+01 4.09200000e+01 5.61800000e+01\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 5.60928564e-01\n",
      " 3.58722215e-01 6.24669814e-01 6.21337044e+01 5.67090572e+01\n",
      " 5.85111632e+01 1.68824972e+02 2.09000651e+00 8.31548524e+01\n",
      " 3.37911464e+01 1.15370605e+01 1.08349581e+01]\n",
      "Episode 20\tFrame 90560 \tAverage Score: 0.05[1.00000000e+03 3.05728571e+01 4.09200000e+01 5.61800000e+01\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 5.60928564e-01\n",
      " 3.58722215e-01 6.24669814e-01 6.21337044e+01 5.67090572e+01\n",
      " 5.85111632e+01 1.68824972e+02 2.09000651e+00 8.31548524e+01\n",
      " 3.37911464e+01 1.15370605e+01 1.08349581e+01]\n",
      "Finished\n",
      "[1019.4781680898809, 107.32, 70.16, 146.41, 3.0, 8.0, 2.0, -2.466788775, -0.5672052238, -0.1475155644, 42.30524752, 44.89929352, 52.58918069, -98.67248174, -15.74083296, 29.12710167, 29.43194077, 7.763541354, 9.869031232]\n",
      "end_total_asset:2195.538168089881\n",
      "Sharpe:  0.6607775371120175\n",
      "[1.00000000e+03 3.05728571e+01 4.09200000e+01 5.61800000e+01\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 5.60928564e-01\n",
      " 3.58722215e-01 6.24669814e-01 6.21337044e+01 5.67090572e+01\n",
      " 5.85111632e+01 1.68824972e+02 2.09000651e+00 8.31548524e+01\n",
      " 3.37911464e+01 1.15370605e+01 1.08349581e+01]\n",
      "Episode 21\tFrame 95088 \tAverage Score: 0.05[1.00000000e+03 3.05728571e+01 4.09200000e+01 5.61800000e+01\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 5.60928564e-01\n",
      " 3.58722215e-01 6.24669814e-01 6.21337044e+01 5.67090572e+01\n",
      " 5.85111632e+01 1.68824972e+02 2.09000651e+00 8.31548524e+01\n",
      " 3.37911464e+01 1.15370605e+01 1.08349581e+01]\n",
      "Finished\n",
      "[838.230502812761, 107.32, 70.16, 146.41, 1.0, 1.0, 0.0, -2.466788775, -0.5672052238, -0.1475155644, 42.30524752, 44.89929352, 52.58918069, -98.67248174, -15.74083296, 29.12710167, 29.43194077, 7.763541354, 9.869031232]\n",
      "end_total_asset:1015.710502812761\n",
      "Sharpe:  0.05125700420268531\n",
      "[1.00000000e+03 3.05728571e+01 4.09200000e+01 5.61800000e+01\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 5.60928564e-01\n",
      " 3.58722215e-01 6.24669814e-01 6.21337044e+01 5.67090572e+01\n",
      " 5.85111632e+01 1.68824972e+02 2.09000651e+00 8.31548524e+01\n",
      " 3.37911464e+01 1.15370605e+01 1.08349581e+01]\n",
      "Episode 22\tFrame 99616 \tAverage Score: 0.05[1.00000000e+03 3.05728571e+01 4.09200000e+01 5.61800000e+01\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 5.60928564e-01\n",
      " 3.58722215e-01 6.24669814e-01 6.21337044e+01 5.67090572e+01\n",
      " 5.85111632e+01 1.68824972e+02 2.09000651e+00 8.31548524e+01\n",
      " 3.37911464e+01 1.15370605e+01 1.08349581e+01]\n",
      "score: [ 1.2708673e+02  3.6952145e+01  4.1150002e+01  6.3299999e+01\n",
      "  8.0000000e+00  6.0000000e+00  6.0000000e+00 -9.3986094e-02\n",
      " -2.3074219e-01 -9.4289541e-01  5.2600868e+01  4.9987862e+01\n",
      "  4.5862316e+01 -2.2120813e+01  8.6065245e+00 -6.2711632e+01\n",
      "  1.3100301e+01  1.0214151e+01  3.1236277e+01]\n",
      "score: [0.00494038]\n",
      "action:[1], Number:100000\n",
      "-------------------------\n",
      "Finished\n",
      "[1090.465297141299, 107.32, 70.16, 146.41, 1.0, 0.0, 0.0, -2.466788775, -0.5672052238, -0.1475155644, 42.30524752, 44.89929352, 52.58918069, -98.67248174, -15.74083296, 29.12710167, 29.43194077, 7.763541354, 9.869031232]\n",
      "end_total_asset:1197.785297141299\n",
      "Sharpe:  0.16095853729063217\n",
      "[1.00000000e+03 3.05728571e+01 4.09200000e+01 5.61800000e+01\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 5.60928564e-01\n",
      " 3.58722215e-01 6.24669814e-01 6.21337044e+01 5.67090572e+01\n",
      " 5.85111632e+01 1.68824972e+02 2.09000651e+00 8.31548524e+01\n",
      " 3.37911464e+01 1.15370605e+01 1.08349581e+01]\n",
      "Episode 23\tFrame 104144 \tAverage Score: 0.05[1.00000000e+03 3.05728571e+01 4.09200000e+01 5.61800000e+01\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 5.60928564e-01\n",
      " 3.58722215e-01 6.24669814e-01 6.21337044e+01 5.67090572e+01\n",
      " 5.85111632e+01 1.68824972e+02 2.09000651e+00 8.31548524e+01\n",
      " 3.37911464e+01 1.15370605e+01 1.08349581e+01]\n",
      "Finished\n",
      "[681.8384923128225, 107.32, 70.16, 146.41, 1.0, 0.0, 3.0, -2.466788775, -0.5672052238, -0.1475155644, 42.30524752, 44.89929352, 52.58918069, -98.67248174, -15.74083296, 29.12710167, 29.43194077, 7.763541354, 9.869031232]\n",
      "end_total_asset:1228.3884923128226\n",
      "Sharpe:  0.1853770695182112\n",
      "[1.00000000e+03 3.05728571e+01 4.09200000e+01 5.61800000e+01\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 5.60928564e-01\n",
      " 3.58722215e-01 6.24669814e-01 6.21337044e+01 5.67090572e+01\n",
      " 5.85111632e+01 1.68824972e+02 2.09000651e+00 8.31548524e+01\n",
      " 3.37911464e+01 1.15370605e+01 1.08349581e+01]\n",
      "Episode 24\tFrame 108672 \tAverage Score: 0.05[1.00000000e+03 3.05728571e+01 4.09200000e+01 5.61800000e+01\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 5.60928564e-01\n",
      " 3.58722215e-01 6.24669814e-01 6.21337044e+01 5.67090572e+01\n",
      " 5.85111632e+01 1.68824972e+02 2.09000651e+00 8.31548524e+01\n",
      " 3.37911464e+01 1.15370605e+01 1.08349581e+01]\n",
      "Finished\n",
      "[1125.0197993170702, 107.32, 70.16, 146.41, 0.0, 3.0, 2.0, -2.466788775, -0.5672052238, -0.1475155644, 42.30524752, 44.89929352, 52.58918069, -98.67248174, -15.74083296, 29.12710167, 29.43194077, 7.763541354, 9.869031232]\n",
      "end_total_asset:1628.3197993170702\n",
      "Sharpe:  0.4206104901119232\n",
      "[1.00000000e+03 3.05728571e+01 4.09200000e+01 5.61800000e+01\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 5.60928564e-01\n",
      " 3.58722215e-01 6.24669814e-01 6.21337044e+01 5.67090572e+01\n",
      " 5.85111632e+01 1.68824972e+02 2.09000651e+00 8.31548524e+01\n",
      " 3.37911464e+01 1.15370605e+01 1.08349581e+01]\n",
      "Episode 25\tFrame 113200 \tAverage Score: 0.05[1.00000000e+03 3.05728571e+01 4.09200000e+01 5.61800000e+01\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 5.60928564e-01\n",
      " 3.58722215e-01 6.24669814e-01 6.21337044e+01 5.67090572e+01\n",
      " 5.85111632e+01 1.68824972e+02 2.09000651e+00 8.31548524e+01\n",
      " 3.37911464e+01 1.15370605e+01 1.08349581e+01]\n",
      "Finished\n",
      "[1227.3822385399699, 107.32, 70.16, 146.41, 0.0, 0.0, 0.0, -2.466788775, -0.5672052238, -0.1475155644, 42.30524752, 44.89929352, 52.58918069, -98.67248174, -15.74083296, 29.12710167, 29.43194077, 7.763541354, 9.869031232]\n",
      "end_total_asset:1227.3822385399699\n",
      "Sharpe:  0.19802865135205963\n",
      "[1.00000000e+03 3.05728571e+01 4.09200000e+01 5.61800000e+01\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 5.60928564e-01\n",
      " 3.58722215e-01 6.24669814e-01 6.21337044e+01 5.67090572e+01\n",
      " 5.85111632e+01 1.68824972e+02 2.09000651e+00 8.31548524e+01\n",
      " 3.37911464e+01 1.15370605e+01 1.08349581e+01]\n",
      "Episode 26\tFrame 117728 \tAverage Score: 0.05[1.00000000e+03 3.05728571e+01 4.09200000e+01 5.61800000e+01\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 5.60928564e-01\n",
      " 3.58722215e-01 6.24669814e-01 6.21337044e+01 5.67090572e+01\n",
      " 5.85111632e+01 1.68824972e+02 2.09000651e+00 8.31548524e+01\n",
      " 3.37911464e+01 1.15370605e+01 1.08349581e+01]\n",
      "Finished\n",
      "[970.0896348827721, 107.32, 70.16, 146.41, 4.0, 0.0, 3.0, -2.466788775, -0.5672052238, -0.1475155644, 42.30524752, 44.89929352, 52.58918069, -98.67248174, -15.74083296, 29.12710167, 29.43194077, 7.763541354, 9.869031232]\n",
      "end_total_asset:1838.599634882772\n",
      "Sharpe:  0.5406822408514385\n",
      "[1.00000000e+03 3.05728571e+01 4.09200000e+01 5.61800000e+01\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 5.60928564e-01\n",
      " 3.58722215e-01 6.24669814e-01 6.21337044e+01 5.67090572e+01\n",
      " 5.85111632e+01 1.68824972e+02 2.09000651e+00 8.31548524e+01\n",
      " 3.37911464e+01 1.15370605e+01 1.08349581e+01]\n",
      "Episode 27\tFrame 122256 \tAverage Score: 0.05[1.00000000e+03 3.05728571e+01 4.09200000e+01 5.61800000e+01\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 5.60928564e-01\n",
      " 3.58722215e-01 6.24669814e-01 6.21337044e+01 5.67090572e+01\n",
      " 5.85111632e+01 1.68824972e+02 2.09000651e+00 8.31548524e+01\n",
      " 3.37911464e+01 1.15370605e+01 1.08349581e+01]\n",
      "Finished\n",
      "[75.88747900716064, 107.32, 70.16, 146.41, 4.0, 0.0, 6.0, -2.466788775, -0.5672052238, -0.1475155644, 42.30524752, 44.89929352, 52.58918069, -98.67248174, -15.74083296, 29.12710167, 29.43194077, 7.763541354, 9.869031232]\n",
      "end_total_asset:1383.6274790071607\n",
      "Sharpe:  0.2933817246123679\n",
      "[1.00000000e+03 3.05728571e+01 4.09200000e+01 5.61800000e+01\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 5.60928564e-01\n",
      " 3.58722215e-01 6.24669814e-01 6.21337044e+01 5.67090572e+01\n",
      " 5.85111632e+01 1.68824972e+02 2.09000651e+00 8.31548524e+01\n",
      " 3.37911464e+01 1.15370605e+01 1.08349581e+01]\n",
      "Episode 28\tFrame 126784 \tAverage Score: 0.05[1.00000000e+03 3.05728571e+01 4.09200000e+01 5.61800000e+01\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 5.60928564e-01\n",
      " 3.58722215e-01 6.24669814e-01 6.21337044e+01 5.67090572e+01\n",
      " 5.85111632e+01 1.68824972e+02 2.09000651e+00 8.31548524e+01\n",
      " 3.37911464e+01 1.15370605e+01 1.08349581e+01]\n",
      "Finished\n",
      "[1168.3931292570328, 107.32, 70.16, 146.41, 1.0, 1.0, 1.0, -2.466788775, -0.5672052238, -0.1475155644, 42.30524752, 44.89929352, 52.58918069, -98.67248174, -15.74083296, 29.12710167, 29.43194077, 7.763541354, 9.869031232]\n",
      "end_total_asset:1492.2831292570327\n",
      "Sharpe:  0.3629410267887662\n",
      "[1.00000000e+03 3.05728571e+01 4.09200000e+01 5.61800000e+01\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 5.60928564e-01\n",
      " 3.58722215e-01 6.24669814e-01 6.21337044e+01 5.67090572e+01\n",
      " 5.85111632e+01 1.68824972e+02 2.09000651e+00 8.31548524e+01\n",
      " 3.37911464e+01 1.15370605e+01 1.08349581e+01]\n",
      "Episode 29\tFrame 131312 \tAverage Score: 0.05[1.00000000e+03 3.05728571e+01 4.09200000e+01 5.61800000e+01\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 5.60928564e-01\n",
      " 3.58722215e-01 6.24669814e-01 6.21337044e+01 5.67090572e+01\n",
      " 5.85111632e+01 1.68824972e+02 2.09000651e+00 8.31548524e+01\n",
      " 3.37911464e+01 1.15370605e+01 1.08349581e+01]\n",
      "Finished\n",
      "[839.7497585299599, 107.32, 70.16, 146.41, 1.0, 2.0, 2.0, -2.466788775, -0.5672052238, -0.1475155644, 42.30524752, 44.89929352, 52.58918069, -98.67248174, -15.74083296, 29.12710167, 29.43194077, 7.763541354, 9.869031232]\n",
      "end_total_asset:1380.2097585299598\n",
      "Sharpe:  0.28225404332693194\n",
      "[1.00000000e+03 3.05728571e+01 4.09200000e+01 5.61800000e+01\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 5.60928564e-01\n",
      " 3.58722215e-01 6.24669814e-01 6.21337044e+01 5.67090572e+01\n",
      " 5.85111632e+01 1.68824972e+02 2.09000651e+00 8.31548524e+01\n",
      " 3.37911464e+01 1.15370605e+01 1.08349581e+01]\n",
      "Episode 30\tFrame 135840 \tAverage Score: 0.05[1.00000000e+03 3.05728571e+01 4.09200000e+01 5.61800000e+01\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 5.60928564e-01\n",
      " 3.58722215e-01 6.24669814e-01 6.21337044e+01 5.67090572e+01\n",
      " 5.85111632e+01 1.68824972e+02 2.09000651e+00 8.31548524e+01\n",
      " 3.37911464e+01 1.15370605e+01 1.08349581e+01]\n",
      "Finished\n",
      "[442.07787922556975, 107.32, 70.16, 146.41, 2.0, 6.0, 3.0, -2.466788775, -0.5672052238, -0.1475155644, 42.30524752, 44.89929352, 52.58918069, -98.67248174, -15.74083296, 29.12710167, 29.43194077, 7.763541354, 9.869031232]\n",
      "end_total_asset:1516.9078792255696\n",
      "Sharpe:  0.3690309798284375\n",
      "[1.00000000e+03 3.05728571e+01 4.09200000e+01 5.61800000e+01\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 5.60928564e-01\n",
      " 3.58722215e-01 6.24669814e-01 6.21337044e+01 5.67090572e+01\n",
      " 5.85111632e+01 1.68824972e+02 2.09000651e+00 8.31548524e+01\n",
      " 3.37911464e+01 1.15370605e+01 1.08349581e+01]\n",
      "Episode 31\tFrame 140368 \tAverage Score: 0.05[1.00000000e+03 3.05728571e+01 4.09200000e+01 5.61800000e+01\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 5.60928564e-01\n",
      " 3.58722215e-01 6.24669814e-01 6.21337044e+01 5.67090572e+01\n",
      " 5.85111632e+01 1.68824972e+02 2.09000651e+00 8.31548524e+01\n",
      " 3.37911464e+01 1.15370605e+01 1.08349581e+01]\n",
      "Finished\n",
      "[540.1042657770487, 107.32, 70.16, 146.41, 1.0, 4.0, 1.0, -2.466788775, -0.5672052238, -0.1475155644, 42.30524752, 44.89929352, 52.58918069, -98.67248174, -15.74083296, 29.12710167, 29.43194077, 7.763541354, 9.869031232]\n",
      "end_total_asset:1074.4742657770487\n",
      "Sharpe:  0.09065932017423349\n",
      "[1.00000000e+03 3.05728571e+01 4.09200000e+01 5.61800000e+01\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 5.60928564e-01\n",
      " 3.58722215e-01 6.24669814e-01 6.21337044e+01 5.67090572e+01\n",
      " 5.85111632e+01 1.68824972e+02 2.09000651e+00 8.31548524e+01\n",
      " 3.37911464e+01 1.15370605e+01 1.08349581e+01]\n",
      "Episode 32\tFrame 144896 \tAverage Score: 0.05[1.00000000e+03 3.05728571e+01 4.09200000e+01 5.61800000e+01\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 5.60928564e-01\n",
      " 3.58722215e-01 6.24669814e-01 6.21337044e+01 5.67090572e+01\n",
      " 5.85111632e+01 1.68824972e+02 2.09000651e+00 8.31548524e+01\n",
      " 3.37911464e+01 1.15370605e+01 1.08349581e+01]\n",
      "Finished\n",
      "[241.28751483425137, 107.32, 70.16, 146.41, 4.0, 2.0, 2.0, -2.466788775, -0.5672052238, -0.1475155644, 42.30524752, 44.89929352, 52.58918069, -98.67248174, -15.74083296, 29.12710167, 29.43194077, 7.763541354, 9.869031232]\n",
      "end_total_asset:1103.7075148342512\n",
      "Sharpe:  0.11230386419379394\n",
      "[1.00000000e+03 3.05728571e+01 4.09200000e+01 5.61800000e+01\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 5.60928564e-01\n",
      " 3.58722215e-01 6.24669814e-01 6.21337044e+01 5.67090572e+01\n",
      " 5.85111632e+01 1.68824972e+02 2.09000651e+00 8.31548524e+01\n",
      " 3.37911464e+01 1.15370605e+01 1.08349581e+01]\n",
      "Episode 33\tFrame 149424 \tAverage Score: 0.05[1.00000000e+03 3.05728571e+01 4.09200000e+01 5.61800000e+01\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 5.60928564e-01\n",
      " 3.58722215e-01 6.24669814e-01 6.21337044e+01 5.67090572e+01\n",
      " 5.85111632e+01 1.68824972e+02 2.09000651e+00 8.31548524e+01\n",
      " 3.37911464e+01 1.15370605e+01 1.08349581e+01]\n",
      "Finished\n",
      "[1818.2244057827647, 107.32, 70.16, 146.41, 2.0, 1.0, 1.0, -2.466788775, -0.5672052238, -0.1475155644, 42.30524752, 44.89929352, 52.58918069, -98.67248174, -15.74083296, 29.12710167, 29.43194077, 7.763541354, 9.869031232]\n",
      "end_total_asset:2249.4344057827648\n",
      "Sharpe:  0.774178821570137\n",
      "[1.00000000e+03 3.05728571e+01 4.09200000e+01 5.61800000e+01\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 5.60928564e-01\n",
      " 3.58722215e-01 6.24669814e-01 6.21337044e+01 5.67090572e+01\n",
      " 5.85111632e+01 1.68824972e+02 2.09000651e+00 8.31548524e+01\n",
      " 3.37911464e+01 1.15370605e+01 1.08349581e+01]\n",
      "Episode 34\tFrame 153952 \tAverage Score: 0.05[1.00000000e+03 3.05728571e+01 4.09200000e+01 5.61800000e+01\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 5.60928564e-01\n",
      " 3.58722215e-01 6.24669814e-01 6.21337044e+01 5.67090572e+01\n",
      " 5.85111632e+01 1.68824972e+02 2.09000651e+00 8.31548524e+01\n",
      " 3.37911464e+01 1.15370605e+01 1.08349581e+01]\n",
      "Finished\n",
      "[376.93014683141354, 107.32, 70.16, 146.41, 4.0, 4.0, 4.0, -2.466788775, -0.5672052238, -0.1475155644, 42.30524752, 44.89929352, 52.58918069, -98.67248174, -15.74083296, 29.12710167, 29.43194077, 7.763541354, 9.869031232]\n",
      "end_total_asset:1672.4901468314135\n",
      "Sharpe:  0.47024103528927036\n",
      "[1.00000000e+03 3.05728571e+01 4.09200000e+01 5.61800000e+01\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 5.60928564e-01\n",
      " 3.58722215e-01 6.24669814e-01 6.21337044e+01 5.67090572e+01\n",
      " 5.85111632e+01 1.68824972e+02 2.09000651e+00 8.31548524e+01\n",
      " 3.37911464e+01 1.15370605e+01 1.08349581e+01]\n",
      "Episode 35\tFrame 158480 \tAverage Score: 0.05[1.00000000e+03 3.05728571e+01 4.09200000e+01 5.61800000e+01\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 5.60928564e-01\n",
      " 3.58722215e-01 6.24669814e-01 6.21337044e+01 5.67090572e+01\n",
      " 5.85111632e+01 1.68824972e+02 2.09000651e+00 8.31548524e+01\n",
      " 3.37911464e+01 1.15370605e+01 1.08349581e+01]\n",
      "Finished\n",
      "[408.17820539152916, 107.32, 70.16, 146.41, 5.0, 3.0, 3.0, -2.466788775, -0.5672052238, -0.1475155644, 42.30524752, 44.89929352, 52.58918069, -98.67248174, -15.74083296, 29.12710167, 29.43194077, 7.763541354, 9.869031232]\n",
      "end_total_asset:1594.4882053915292\n",
      "Sharpe:  0.4330441333443688\n",
      "[1.00000000e+03 3.05728571e+01 4.09200000e+01 5.61800000e+01\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 5.60928564e-01\n",
      " 3.58722215e-01 6.24669814e-01 6.21337044e+01 5.67090572e+01\n",
      " 5.85111632e+01 1.68824972e+02 2.09000651e+00 8.31548524e+01\n",
      " 3.37911464e+01 1.15370605e+01 1.08349581e+01]\n",
      "Episode 36\tFrame 163008 \tAverage Score: 0.05[1.00000000e+03 3.05728571e+01 4.09200000e+01 5.61800000e+01\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 5.60928564e-01\n",
      " 3.58722215e-01 6.24669814e-01 6.21337044e+01 5.67090572e+01\n",
      " 5.85111632e+01 1.68824972e+02 2.09000651e+00 8.31548524e+01\n",
      " 3.37911464e+01 1.15370605e+01 1.08349581e+01]\n",
      "Finished\n",
      "[337.1344609156422, 107.32, 70.16, 146.41, 5.0, 3.0, 1.0, -2.466788775, -0.5672052238, -0.1475155644, 42.30524752, 44.89929352, 52.58918069, -98.67248174, -15.74083296, 29.12710167, 29.43194077, 7.763541354, 9.869031232]\n",
      "end_total_asset:1230.624460915642\n",
      "Sharpe:  0.18649712143176722\n",
      "[1.00000000e+03 3.05728571e+01 4.09200000e+01 5.61800000e+01\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 5.60928564e-01\n",
      " 3.58722215e-01 6.24669814e-01 6.21337044e+01 5.67090572e+01\n",
      " 5.85111632e+01 1.68824972e+02 2.09000651e+00 8.31548524e+01\n",
      " 3.37911464e+01 1.15370605e+01 1.08349581e+01]\n",
      "Episode 37\tFrame 167536 \tAverage Score: 0.05[1.00000000e+03 3.05728571e+01 4.09200000e+01 5.61800000e+01\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 5.60928564e-01\n",
      " 3.58722215e-01 6.24669814e-01 6.21337044e+01 5.67090572e+01\n",
      " 5.85111632e+01 1.68824972e+02 2.09000651e+00 8.31548524e+01\n",
      " 3.37911464e+01 1.15370605e+01 1.08349581e+01]\n",
      "Finished\n",
      "[1147.3332126043013, 107.32, 70.16, 146.41, 0.0, 2.0, 1.0, -2.466788775, -0.5672052238, -0.1475155644, 42.30524752, 44.89929352, 52.58918069, -98.67248174, -15.74083296, 29.12710167, 29.43194077, 7.763541354, 9.869031232]\n",
      "end_total_asset:1434.0632126043013\n",
      "Sharpe:  0.3515178862377437\n",
      "[1.00000000e+03 3.05728571e+01 4.09200000e+01 5.61800000e+01\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 5.60928564e-01\n",
      " 3.58722215e-01 6.24669814e-01 6.21337044e+01 5.67090572e+01\n",
      " 5.85111632e+01 1.68824972e+02 2.09000651e+00 8.31548524e+01\n",
      " 3.37911464e+01 1.15370605e+01 1.08349581e+01]\n",
      "Episode 38\tFrame 172064 \tAverage Score: 0.05[1.00000000e+03 3.05728571e+01 4.09200000e+01 5.61800000e+01\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 5.60928564e-01\n",
      " 3.58722215e-01 6.24669814e-01 6.21337044e+01 5.67090572e+01\n",
      " 5.85111632e+01 1.68824972e+02 2.09000651e+00 8.31548524e+01\n",
      " 3.37911464e+01 1.15370605e+01 1.08349581e+01]\n",
      "Finished\n",
      "[19.043420340008637, 107.32, 70.16, 146.41, 3.0, 3.0, 6.0, -2.466788775, -0.5672052238, -0.1475155644, 42.30524752, 44.89929352, 52.58918069, -98.67248174, -15.74083296, 29.12710167, 29.43194077, 7.763541354, 9.869031232]\n",
      "end_total_asset:1429.9434203400087\n",
      "Sharpe:  0.2893058023253629\n",
      "[1.00000000e+03 3.05728571e+01 4.09200000e+01 5.61800000e+01\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 5.60928564e-01\n",
      " 3.58722215e-01 6.24669814e-01 6.21337044e+01 5.67090572e+01\n",
      " 5.85111632e+01 1.68824972e+02 2.09000651e+00 8.31548524e+01\n",
      " 3.37911464e+01 1.15370605e+01 1.08349581e+01]\n",
      "Episode 39\tFrame 176592 \tAverage Score: 0.05[1.00000000e+03 3.05728571e+01 4.09200000e+01 5.61800000e+01\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 5.60928564e-01\n",
      " 3.58722215e-01 6.24669814e-01 6.21337044e+01 5.67090572e+01\n",
      " 5.85111632e+01 1.68824972e+02 2.09000651e+00 8.31548524e+01\n",
      " 3.37911464e+01 1.15370605e+01 1.08349581e+01]\n",
      "Finished\n",
      "[1308.4904645027711, 107.32, 70.16, 146.41, 1.0, 3.0, 1.0, -2.466788775, -0.5672052238, -0.1475155644, 42.30524752, 44.89929352, 52.58918069, -98.67248174, -15.74083296, 29.12710167, 29.43194077, 7.763541354, 9.869031232]\n",
      "end_total_asset:1772.700464502771\n",
      "Sharpe:  0.5046494163855187\n",
      "[1.00000000e+03 3.05728571e+01 4.09200000e+01 5.61800000e+01\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 5.60928564e-01\n",
      " 3.58722215e-01 6.24669814e-01 6.21337044e+01 5.67090572e+01\n",
      " 5.85111632e+01 1.68824972e+02 2.09000651e+00 8.31548524e+01\n",
      " 3.37911464e+01 1.15370605e+01 1.08349581e+01]\n",
      "Episode 40\tFrame 181120 \tAverage Score: 0.05[1.00000000e+03 3.05728571e+01 4.09200000e+01 5.61800000e+01\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 5.60928564e-01\n",
      " 3.58722215e-01 6.24669814e-01 6.21337044e+01 5.67090572e+01\n",
      " 5.85111632e+01 1.68824972e+02 2.09000651e+00 8.31548524e+01\n",
      " 3.37911464e+01 1.15370605e+01 1.08349581e+01]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [57]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    380\u001b[0m eps_fixed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    381\u001b[0m t0 \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m--> 382\u001b[0m final_average100 \u001b[38;5;241m=\u001b[39m \u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menv_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframes\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m600000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meps_fixed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meps_fixed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meps_frames\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmin_eps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.025\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    383\u001b[0m t1 \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m    385\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining time: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124mmin\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mround\u001b[39m((t1\u001b[38;5;241m-\u001b[39mt0)\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m60\u001b[39m,\u001b[38;5;241m2\u001b[39m)))\n",
      "Input \u001b[0;32mIn [57]\u001b[0m, in \u001b[0;36mrun\u001b[0;34m(env, frames, eps_fixed, eps_frames, min_eps)\u001b[0m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;66;03m#print(\"env_trainNext State: {}\".format(next_state.shape))\u001b[39;00m\n\u001b[1;32m    289\u001b[0m next_state \u001b[38;5;241m=\u001b[39m next_state[\u001b[38;5;241m0\u001b[39m,:]\n\u001b[0;32m--> 291\u001b[0m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreward\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnext_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdone\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwriter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    294\u001b[0m state \u001b[38;5;241m=\u001b[39m next_state\n\u001b[1;32m    295\u001b[0m score \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m reward\n",
      "Input \u001b[0;32mIn [39]\u001b[0m, in \u001b[0;36mDQN_Agent.step\u001b[0;34m(self, state, action, reward, next_state, done, writer)\u001b[0m\n\u001b[1;32m     71\u001b[0m experiences \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmemory\u001b[38;5;241m.\u001b[39msample()\n\u001b[1;32m     72\u001b[0m \u001b[38;5;66;03m#print(\"experiences:{}\".format(experiences))\u001b[39;00m\n\u001b[0;32m---> 73\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexperiences\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mQ_updates \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     75\u001b[0m writer\u001b[38;5;241m.\u001b[39madd_scalar(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mQ_loss\u001b[39m\u001b[38;5;124m\"\u001b[39m, loss, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mQ_updates)\n",
      "Input \u001b[0;32mIn [39]\u001b[0m, in \u001b[0;36mDQN_Agent.learn\u001b[0;34m(self, experiences)\u001b[0m\n\u001b[1;32m    176\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m    177\u001b[0m \u001b[38;5;66;03m#clip_grad_norm_(self.qnetwork_local.parameters(),1)\u001b[39;00m\n\u001b[0;32m--> 178\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    180\u001b[0m \u001b[38;5;66;03m# ------------------- update target network ------------------- #\u001b[39;00m\n\u001b[1;32m    181\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msoft_update(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mqnetwork_local, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mqnetwork_target)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/optim/optimizer.py:88\u001b[0m, in \u001b[0;36mOptimizer._hook_for_profile.<locals>.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m profile_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOptimizer.step#\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m.step\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(obj\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mrecord_function(profile_name):\n\u001b[0;32m---> 88\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/autograd/grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclone():\n\u001b[0;32m---> 27\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/optim/adam.py:141\u001b[0m, in \u001b[0;36mAdam.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    138\u001b[0m             \u001b[38;5;66;03m# record the step after step update\u001b[39;00m\n\u001b[1;32m    139\u001b[0m             state_steps\u001b[38;5;241m.\u001b[39mappend(state[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstep\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m--> 141\u001b[0m     \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madam\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    142\u001b[0m \u001b[43m           \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    143\u001b[0m \u001b[43m           \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    144\u001b[0m \u001b[43m           \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    145\u001b[0m \u001b[43m           \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    146\u001b[0m \u001b[43m           \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    147\u001b[0m \u001b[43m           \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mamsgrad\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    148\u001b[0m \u001b[43m           \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    149\u001b[0m \u001b[43m           \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    150\u001b[0m \u001b[43m           \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[43m           \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    152\u001b[0m \u001b[43m           \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43meps\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    153\u001b[0m \u001b[43m           \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmaximize\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/optim/_functional.py:98\u001b[0m, in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;66;03m# Decay the first and second moment running average coefficient\u001b[39;00m\n\u001b[1;32m     97\u001b[0m exp_avg\u001b[38;5;241m.\u001b[39mmul_(beta1)\u001b[38;5;241m.\u001b[39madd_(grad, alpha\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m beta1)\n\u001b[0;32m---> 98\u001b[0m \u001b[43mexp_avg_sq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmul_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39maddcmul_(grad, grad\u001b[38;5;241m.\u001b[39mconj(), value\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m beta2)\n\u001b[1;32m     99\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m amsgrad:\n\u001b[1;32m    100\u001b[0m     \u001b[38;5;66;03m# Maintains the maximum of all 2nd moment running avg. till now\u001b[39;00m\n\u001b[1;32m    101\u001b[0m     torch\u001b[38;5;241m.\u001b[39mmaximum(max_exp_avg_sqs[i], exp_avg_sq, out\u001b[38;5;241m=\u001b[39mmax_exp_avg_sqs[i])\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from IPython import display\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from gym.utils import seeding\n",
    "import gym\n",
    "import os\n",
    "from gym import spaces\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "\n",
    "# shares normalization factor\n",
    "# 100 shares per trade\n",
    "HMAX_NORMALIZE = 100\n",
    "# initial amount of money we have in our account\n",
    "INITIAL_ACCOUNT_BALANCE= 1000\n",
    "# total number of stocks in our portfolio\n",
    "STOCK_DIM = 3\n",
    "# transaction fee: 1/1000 reasonable percentage\n",
    "TRANSACTION_FEE_PERCENT = 0.001\n",
    "REWARD_SCALING = 1e-4\n",
    "\n",
    "class StockEnvTrain(gym.Env):\n",
    "    \"\"\"A stock trading environment for OpenAI gym\"\"\"\n",
    "    metadata = {'render.modes': ['human']}\n",
    "\n",
    "    def __init__(self, df,day = 0):\n",
    "        #super(StockEnv, self).__init__()\n",
    "        #money = 10 , scope = 1\n",
    "        self.day = day\n",
    "        self.df = df\n",
    "        self.agent_stock_iteration_index = 0\n",
    "\n",
    "        # action_space normalization and shape is STOCK_DIM\n",
    "        self.action_space = spaces.Box(low = -1, high = 1,shape = (STOCK_DIM,)) \n",
    "        # Shape = 181: [Current Balance]+[prices 1-30]+[owned shares 1-30] \n",
    "        # +[macd 1-30]+ [rsi 1-30] + [cci 1-30] + [adx 1-30]\n",
    "        self.observation_space = spaces.Box(low=0, high=np.inf, shape = (19,))\n",
    "        # load data from a pandas dataframe\n",
    "        #print('df: {}'.format(self.df))\n",
    "        #print('day: {}'.format(self.day))\n",
    "        self.data = self.df.loc[self.day,:]\n",
    "        #print(self.data.Close)\n",
    "        self.terminal = False\n",
    "\n",
    "\n",
    "\n",
    "        # initalize state\n",
    "        self.state = [INITIAL_ACCOUNT_BALANCE] + \\\n",
    "                      self.data.Close.values.tolist() + \\\n",
    "                      [0]*STOCK_DIM + \\\n",
    "                      self.data.macd.values.tolist() + \\\n",
    "                      self.data.rsi.values.tolist() + \\\n",
    "                      self.data.cci.values.tolist() + \\\n",
    "                      self.data.adx.values.tolist()\n",
    "        # initialize reward\n",
    "        self.reward = 0\n",
    "        self.cost = 0\n",
    "        # memorize all the total balance change\n",
    "        self.asset_memory = [INITIAL_ACCOUNT_BALANCE]\n",
    "        self.rewards_memory = []\n",
    "        self.final_asset_value = 0\n",
    "        self.trades = 0\n",
    "        #self.reset()\n",
    "        self._seed()\n",
    "\n",
    "\n",
    "    def _sell_stock(self, index, action):\n",
    "        # perform sell action based on the sign of the action\n",
    "        if self.state[index+STOCK_DIM+1] > 0:\n",
    "            #update balance\n",
    "            self.state[0] += \\\n",
    "            self.state[index+1]*min(abs(action),self.state[index+STOCK_DIM+1]) * \\\n",
    "             (1- TRANSACTION_FEE_PERCENT)\n",
    "\n",
    "            self.state[index+STOCK_DIM+1] -= min(abs(action), self.state[index+STOCK_DIM+1])\n",
    "            self.cost +=self.state[index+1]*min(abs(action),self.state[index+STOCK_DIM+1]) * \\\n",
    "             TRANSACTION_FEE_PERCENT\n",
    "            self.trades+=1\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "    \n",
    "    def _buy_stock(self, index, action):\n",
    "        # perform buy action based on the sign of the action\n",
    "        available_amount = self.state[0] // self.state[index+1]\n",
    "        # print('available_amount:{}'.format(available_amount))\n",
    "\n",
    "        #update balance\n",
    "        self.state[0] -= self.state[index+1]*min(available_amount, action)* \\\n",
    "                          (1+ TRANSACTION_FEE_PERCENT)\n",
    "\n",
    "        self.state[index+STOCK_DIM+1] += min(available_amount, action)\n",
    "\n",
    "        self.cost+=self.state[index+1]*min(available_amount, action)* \\\n",
    "                          TRANSACTION_FEE_PERCENT\n",
    "        self.trades+=1\n",
    "        \n",
    "    def step(self, actions):\n",
    "        # print(self.day)\n",
    "        self.terminal = self.day >= len(self.df.index.unique())-1\n",
    "        #print(actions)\n",
    "        self.actions = actions\n",
    "        if self.terminal:\n",
    "            print(\"Finished\")\n",
    "            print(self.state)\n",
    "            end_total_asset = self.state[0]+ \\\n",
    "            sum(np.array(self.state[1:(STOCK_DIM+1)])*np.array(self.state[(STOCK_DIM+1):(STOCK_DIM*2+1)]))\n",
    "\n",
    "            print(\"end_total_asset:{}\".format(end_total_asset))\n",
    "            df_total_value = pd.DataFrame(self.asset_memory)\n",
    "            #df_total_value.to_csv('results/account_value_train.csv')\n",
    "            #print(\"total_reward:{}\".format(self.state[0]+sum(np.array(self.state[1:(STOCK_DIM+1)])*np.array(self.state[(STOCK_DIM+1):61]))- INITIAL_ACCOUNT_BALANCE ))\n",
    "            #print(\"total_cost: \", self.cost)\n",
    "            #print(\"total_trades: \", self.trades)\n",
    "            df_total_value.columns = ['account_value']\n",
    "            df_total_value['daily_return']=df_total_value.pct_change(1)\n",
    "            sharpe = (252**0.5)*df_total_value['daily_return'].mean()/ \\\n",
    "                  df_total_value['daily_return'].std()\n",
    "            print(\"Sharpe: \",sharpe)\n",
    "            #print(\"=================================\")\n",
    "            df_rewards = pd.DataFrame(self.rewards_memory)\n",
    "            #df_rewards.to_csv('results/account_rewards_train.csv')\n",
    "\n",
    "            # print('total asset: {}'.format(self.state[0]+ sum(np.array(self.state[1:29])*np.array(self.state[29:]))))\n",
    "            #with open('obs.pkl', 'wb') as f:  \n",
    "            #    pickle.dump(self.state, f)\n",
    "\n",
    "            return self.state, self.reward, self.terminal,{}\n",
    "\n",
    "        else:\n",
    "            # print(np.array(self.state[1:29]))\n",
    "            #print(\"The actions is: {}\".format(self.actions))\n",
    "\n",
    "            #action = np.array([4,4,5])\n",
    "            #actions = np.array([4,0,0,0,0,0,0,0,4,0,4,0,-3,0,0,0,0,0,0,0,4,0,0,0,0,0,0,0,0,0])\n",
    "\n",
    "            #actions = self.actions * HMAX_NORMALIZE #WHY??\n",
    "            #print(\"actions-index------:{}\".format(actions))\n",
    "            #actions = (actions.astype(int))\n",
    "\n",
    "            begin_total_asset = self.state[0]+ \\\n",
    "            sum(np.array(self.state[1:(STOCK_DIM+1)])*np.array(self.state[(STOCK_DIM+1):(STOCK_DIM*2+1)]))\n",
    "            #print(\"begin_total_asset:{}\".format(begin_total_asset))\n",
    "\n",
    "            argsort_actions = np.argsort(actions) #TODO: this may not be touched.\n",
    "            #print(\"The actions is: {}\".format(actions))\n",
    "\n",
    "            sell_index = argsort_actions[:np.where(actions == 0)[0].shape[0]]\n",
    "            #sell_index = argsort_actions[4,0,0,0,0,0,0,0,4,0,4,0,-3,0,0,0,0,0,0,0,4,0,0,0,0,0,0,0,0,0]\n",
    "            #print(\"sell-index------:{}\".format(sell_index))\n",
    "            buy_index = argsort_actions[::-1][:np.where(actions == 2)[0].shape[0]]\n",
    "            #buy_index = argsort_actions[::-1][4,0,0,0,0,0,0,0,4,0,4,0,-3,0,0,0,0,0,0,0,4,0,0,0,0,0,0,0,0,0]\n",
    "            #print(\"buy-index------:{}\".format(buy_index))\n",
    "\n",
    "            for index in sell_index:\n",
    "            # print('take sell action'.format(actions[index]))\n",
    "                #print(\"--------Action Shape:{}\".format(actions.shape))\n",
    "                self._sell_stock(index+ self.agent_stock_iteration_index, 1)\n",
    "\n",
    "            for index in buy_index:\n",
    "                #print(\"--------Action Shape:{}\".format(actions.shape))\n",
    "            # print('take buy action: {}'.format(actions[index]))\n",
    "                self._buy_stock(index+ self.agent_stock_iteration_index, 1)\n",
    "                \n",
    "            \n",
    "            #print(\"self.day:{}\".format(self.day))\n",
    "            \n",
    "            \n",
    "\n",
    "                \n",
    "            #load next state\n",
    "            # print(\"stock_shares:{}\".format(self.state[29:]))\n",
    "            self.state =  [self.state[0]] + \\\n",
    "                self.data.Close.values.tolist() + \\\n",
    "                list(self.state[(STOCK_DIM+1):(STOCK_DIM*2+1)]) + \\\n",
    "                self.data.macd.values.tolist() + \\\n",
    "                self.data.rsi.values.tolist() + \\\n",
    "                self.data.cci.values.tolist() + \\\n",
    "                self.data.adx.values.tolist()\n",
    "\n",
    "            end_total_asset = self.state[0]+ \\\n",
    "            sum(np.array(self.state[1:(STOCK_DIM+1)])*np.array(self.state[(STOCK_DIM+1):(STOCK_DIM*2+1)]))\n",
    "            self.asset_memory.append(end_total_asset)\n",
    "            #print(\"end_total_asset:{}\".format(end_total_asset))\n",
    "\n",
    "            self.reward = end_total_asset - begin_total_asset            \n",
    "            # print(\"step_reward:{}\".format(self.reward))\n",
    "            self.rewards_memory.append(self.reward)\n",
    "\n",
    "            self.reward = self.reward*REWARD_SCALING\n",
    "            \n",
    "            self.agent_stock_iteration_index += 1 \n",
    "            if self.agent_stock_iteration_index ==3:\n",
    "                self.day += 1\n",
    "                self.data = self.df.loc[self.day,:]\n",
    "                self.agent_stock_iteration_index = 0\n",
    "            \n",
    "            \n",
    "\n",
    "        return self.state, self.reward, self.terminal, {}\n",
    "\n",
    "    def reset(self):\n",
    "        self.final_asset_value = 0\n",
    "        self.trades = 0\n",
    "        self.asset_memory = [INITIAL_ACCOUNT_BALANCE]\n",
    "        self.day = 0\n",
    "        self.data = self.df.loc[self.day,:]\n",
    "        self.cost = 0\n",
    "        self.trades = 0\n",
    "        self.terminal = False \n",
    "        self.rewards_memory = []\n",
    "        self.agent_stock_iteration_index = 0\n",
    "        #initiate state\n",
    "        self.state = [INITIAL_ACCOUNT_BALANCE] + \\\n",
    "                      self.data.Close.values.tolist() + \\\n",
    "                      [0]*STOCK_DIM + \\\n",
    "                      self.data.macd.values.tolist() + \\\n",
    "                      self.data.rsi.values.tolist() + \\\n",
    "                      self.data.cci.values.tolist() + \\\n",
    "                      self.data.adx.values.tolist() \n",
    "        # iteration += 1 \n",
    "        #print(\"[0]*STOCK_DIM:{}\".format([0]*STOCK_DIM))\n",
    "        #print(\"self.state:{}\".format(len(self.state)))\n",
    "        print(np.array(self.state))\n",
    "        return np.array(self.state)\n",
    "    \n",
    "    def render(self, mode='human'):\n",
    "        return self.state\n",
    "\n",
    "    def _seed(self, seed=None):\n",
    "        self.np_random, seed = seeding.np_random(seed)\n",
    "        return [seed]\n",
    "\n",
    "\n",
    "def run(env,frames=1000, eps_fixed=False, eps_frames=1e6, min_eps=0.01):\n",
    "    \"\"\"Deep Q-Learning.\n",
    "    \n",
    "    Params\n",
    "    ======\n",
    "        n_episodes (int): maximum number of training episodes\n",
    "        max_t (int): maximum number of timesteps per episode\n",
    "        eps_start (float): starting value of epsilon, for epsilon-greedy action selection\n",
    "        eps_end (float): minimum value of epsilon\n",
    "        eps_decay (float): multiplicative factor (per episode) for decreasing epsilon\n",
    "    \"\"\"\n",
    "    scores = []                        # list containing scores from each episode\n",
    "    scores_window = deque(maxlen=100)  # last 100 scores\n",
    "    output_history = []\n",
    "    frame = 0\n",
    "    if eps_fixed:\n",
    "        eps = 0\n",
    "    else:\n",
    "        eps = 1\n",
    "    eps_start = 1\n",
    "    i_episode = 1\n",
    "    state = env.reset()\n",
    "    state = state[0,:]\n",
    "    #print(\"state space:{}\".format(state[0,:].shape))\n",
    "    score = 0                  \n",
    "    for frame in range(1, frames+1):\n",
    "        \n",
    "        if frame  == 0:\n",
    "            # inital state\n",
    "\n",
    "            initial = True\n",
    "        else:\n",
    "            # previous state\n",
    "            initial = False\n",
    "        #print('initial state:{}'.format(initial))\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        action = agent.act(state, eps) #TODO: getting one dimension back.\n",
    "        \n",
    "        \n",
    "        action = np.array([action])\n",
    "        \n",
    "        \n",
    "        next_state, reward, done, info = env_train.step([action]) #TODO: Wants a list of actions of size a\n",
    "\n",
    "        #print(\"env_trainNext State: {}\".format(next_state.shape))\n",
    "\n",
    "        next_state = next_state[0,:]\n",
    "        \n",
    "        agent.step(state, action, reward, next_state, done, writer)\n",
    "        \n",
    "\n",
    "        state = next_state\n",
    "        score += reward\n",
    "        # linear annealing to the min epsilon value until eps_frames and from there slowly decease epsilon to 0 until the end of training\n",
    "        if eps_fixed == False:\n",
    "            if frame < eps_frames:\n",
    "                eps = max(eps_start - (frame*(1/eps_frames)), min_eps)\n",
    "            else:\n",
    "                eps = max(min_eps - min_eps*((frame-eps_frames)/(frames-eps_frames)), 0.001)\n",
    "\n",
    "        # evaluation runs\n",
    "        if frame % 100000 == 0:\n",
    "            print(\"score: {}\".format(state))\n",
    "            print(\"score: {}\".format(score))\n",
    "            #print(\"state: {}\".format(state))\n",
    "            print(\"action:{}, Number:{}\".format(action,frame))\n",
    "            print(\"-------------------------\")\n",
    "        \n",
    "        if done:\n",
    "            scores_window.append(score)       # save most recent score\n",
    "            scores.append(score)              # save most recent score\n",
    "            writer.add_scalar(\"Average100\", np.mean(scores_window), frame)\n",
    "            output_history.append(np.mean(scores_window))\n",
    "            print('\\rEpisode {}\\tFrame {} \\tAverage Score: {:.2f}'.format(i_episode, frame, np.mean(scores_window)), end=\"\")\n",
    "            if i_episode % 100 == 0:\n",
    "                print('\\rEpisode {}\\tFrame {}\\tAverage Score: {:.2f}'.format(i_episode,frame, np.mean(scores_window)))\n",
    "            i_episode +=1 \n",
    "\n",
    "            state = env.reset()\n",
    "            state = state[0,:]\n",
    "            score = 0              \n",
    "\n",
    "    return output_history\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "        # read and preprocess data\n",
    "    preprocessed_path = \"done_3stocks.csv\"\n",
    "    if os.path.exists(preprocessed_path):\n",
    "        data = pd.read_csv(preprocessed_path, index_col=0)\n",
    "\n",
    "    unique_trade_date = data[(data.datadate > 20151001)&(data.datadate <= 20200707)].datadate.unique()\n",
    "    #print(unique_trade_date)\n",
    "\n",
    "    \n",
    "    train = data_split(data, start=20100101, end=20160101)\n",
    "    \n",
    "    env_train = DummyVecEnv([lambda: StockEnvTrain(train)])\n",
    "    \n",
    "    writer = SummaryWriter(\"runs/\"+\"IQN_CP_5\")\n",
    "    seed = 1\n",
    "    BUFFER_SIZE = 10000\n",
    "    BATCH_SIZE = 8\n",
    "    GAMMA = 0.99\n",
    "    TAU = 1e-2\n",
    "    LR = 1e-3\n",
    "    UPDATE_EVERY = 1\n",
    "    n_step = 1\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(\"Using \", device)\n",
    "\n",
    "\n",
    "    action_size     = env_train.action_space.shape[0]\n",
    "\n",
    "    \n",
    "    \n",
    "    print('Action Space: {}'.format(action_size))\n",
    "    state_size = env_train.observation_space.shape[0]\n",
    "    print('State Space: {}'.format(state_size))\n",
    "\n",
    "    agent = DQN_Agent(state_size=19,    #181 #4\n",
    "                        action_size=3, #30 #7\n",
    "                        layer_size=512, #512, #512\n",
    "                        n_step=n_step,\n",
    "                        BATCH_SIZE=BATCH_SIZE, \n",
    "                        BUFFER_SIZE=BUFFER_SIZE, \n",
    "                        LR=LR, \n",
    "                        TAU=TAU, \n",
    "                        GAMMA=GAMMA, \n",
    "                        UPDATE_EVERY=UPDATE_EVERY, \n",
    "                        device=device, \n",
    "                        seed=seed)\n",
    "\n",
    "\n",
    "\n",
    "    # set epsilon frames to 0 so no epsilon exploration\n",
    "    eps_fixed = False\n",
    "    t0 = time.time()\n",
    "    final_average100 = run(env=env_train, frames = 600000, eps_fixed=eps_fixed, eps_frames=5000, min_eps=0.025)\n",
    "    t1 = time.time()\n",
    "    \n",
    "    print(\"Training time: {}min\".format(round((t1-t0)/60,2)))\n",
    "    torch.save(agent.qnetwork_local.state_dict(), \"IQN\"+\".pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
