{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "import random\n",
    "import math\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from collections import deque, namedtuple\n",
    "import time\n",
    "import gym\n",
    "import os\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "def weight_init(layers):\n",
    "    for layer in layers:\n",
    "        torch.nn.init.kaiming_normal_(layer.weight, nonlinearity='relu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3.common.vec_env.base_vec_env import VecEnv, VecEnvStepReturn, VecEnvWrapper\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from stockstats import StockDataFrame as Sdf\n",
    "\n",
    "import datetime\n",
    "\n",
    "import datetime\n",
    "import os\n",
    "\n",
    "TRAINING_DATA_FILE = \"dataprocessing/Yfinance_Data.csv\"\n",
    "\n",
    "now = datetime.datetime.now()\n",
    "TRAINED_MODEL_DIR = f\"trained_models/{now}\"\n",
    "os.makedirs(TRAINED_MODEL_DIR)\n",
    "\n",
    "TESTING_DATA_FILE = \"test.csv\"\n",
    "\n",
    "def load_dataset(*, file_name: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    load csv dataset from path\n",
    "    :return: (df) pandas dataframe\n",
    "    \"\"\"\n",
    "    # _data = pd.read_csv(f\"{config.DATASET_DIR}/{file_name}\")\n",
    "    _data = pd.read_csv(file_name)\n",
    "\n",
    "    return _data\n",
    "\n",
    "\n",
    "def data_split(df, start, end):\n",
    "    \"\"\"\n",
    "    split the dataset into training or testing using date\n",
    "    :param data: (df) pandas dataframe, start, end\n",
    "    :return: (df) pandas dataframe\n",
    "    \"\"\"\n",
    "    data = df[(df.datadate >= start) & (df.datadate < end)]\n",
    "    data = data.sort_values(['datadate', 'tic'], ignore_index=True)\n",
    "\n",
    "\n",
    "    # data  = data[final_columns]\n",
    "    data.index = data.datadate.factorize()[0]\n",
    "\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def calculate_price(df):\n",
    "    \"\"\"\n",
    "    calcualte adjusted close price, open-high-low price and volume\n",
    "    :param data: (df) pandas dataframe\n",
    "    :return: (df) pandas dataframe\n",
    "    \"\"\"\n",
    "    data = df.copy()\n",
    "\n",
    "    data = data[['Date', 'tic', 'Close', 'Open', 'High', 'Low', 'Volume','datadate']]\n",
    "    data = data.sort_values(['tic', 'datadate'], ignore_index=True)\n",
    "    return data\n",
    "\n",
    "\n",
    "def add_technical_indicator(df):\n",
    "    \"\"\"\n",
    "    calcualte technical indicators\n",
    "    use stockstats package to add technical inidactors\n",
    "    :param data: (df) pandas dataframe\n",
    "    :return: (df) pandas dataframe\n",
    "    \"\"\"\n",
    "    stock = Sdf.retype(df.copy())\n",
    "\n",
    "    #print(stock)\n",
    "\n",
    "    unique_ticker = stock.tic.unique()\n",
    "\n",
    "    macd = pd.DataFrame()\n",
    "    rsi = pd.DataFrame()\n",
    "    cci = pd.DataFrame()\n",
    "    dx = pd.DataFrame()\n",
    "\n",
    "    # temp = stock[stock.tic == unique_ticker[0]]['macd']\n",
    "    for i in range(len(unique_ticker)):\n",
    "        ## macd\n",
    "        temp_macd = stock[stock.tic == unique_ticker[i]]['macd']\n",
    "        temp_macd = pd.DataFrame(temp_macd)\n",
    "        macd = macd.append(temp_macd, ignore_index=True)\n",
    "        ## rsi\n",
    "        temp_rsi = stock[stock.tic == unique_ticker[i]]['rsi_30']\n",
    "        temp_rsi = pd.DataFrame(temp_rsi)\n",
    "        rsi = rsi.append(temp_rsi, ignore_index=True)\n",
    "        ## cci\n",
    "        temp_cci = stock[stock.tic == unique_ticker[i]]['cci_30']\n",
    "        temp_cci = pd.DataFrame(temp_cci)\n",
    "        cci = cci.append(temp_cci, ignore_index=True)\n",
    "        ## adx\n",
    "        temp_dx = stock[stock.tic == unique_ticker[i]]['dx_30']\n",
    "        temp_dx = pd.DataFrame(temp_dx)\n",
    "        dx = dx.append(temp_dx, ignore_index=True)\n",
    "\n",
    "    df['macd'] = macd\n",
    "    df['rsi'] = rsi\n",
    "    df['cci'] = cci\n",
    "    df['adx'] = dx\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def preprocess_data():\n",
    "    \"\"\"data preprocessing pipeline\"\"\"\n",
    "    start = datetime.datetime(2010, 12, 1)\n",
    "    df = load_dataset(file_name=TRAINING_DATA_FILE)\n",
    "    # get data after 2010\n",
    "    # df = df[df.Date >= start]\n",
    "    # calcualte adjusted price\n",
    "    df_preprocess = calculate_price(df)\n",
    "    # add technical indicators using stockstats\n",
    "    df_final = add_technical_indicator(df_preprocess)\n",
    "    # fill the missing values at the beginning\n",
    "    df_final.fillna(method='bfill', inplace=True)\n",
    "    return df_final\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IQN(nn.Module):\n",
    "    def __init__(self, state_size, action_size,layer_size, n_step, seed, layer_type=\"ff\"):\n",
    "        super(IQN, self).__init__()\n",
    "        self.seed = torch.manual_seed(seed)\n",
    "        self.input_shape = state_size\n",
    "        self.action_size = action_size\n",
    "        self.K = 32\n",
    "        self.N = 8\n",
    "        self.n_cos = 64\n",
    "        self.layer_size = layer_size\n",
    "        self.pis = torch.FloatTensor([np.pi*i for i in range(self.n_cos)]).view(1,1,self.n_cos).to(device) # Starting from 0 as in the paper \n",
    "\n",
    "        self.head = nn.Linear(self.input_shape, layer_size) # cound be a cnn \n",
    "        self.cos_embedding = nn.Linear(self.n_cos, layer_size)\n",
    "        self.ff_1 = nn.Linear(layer_size, layer_size)\n",
    "        self.ff_2 = nn.Linear(layer_size, action_size)\n",
    "        #weight_init([self.head_1, self.ff_1])\n",
    "\n",
    "\n",
    "        \n",
    "    def calc_cos(self, batch_size, n_tau=8):\n",
    "        \"\"\"\n",
    "        Calculating the cosinus values depending on the number of tau samples\n",
    "        \"\"\"\n",
    "        taus = torch.rand(batch_size, n_tau).to(device).unsqueeze(-1) #(batch_size, n_tau, 1)\n",
    "        cos = torch.cos(taus*self.pis)\n",
    "\n",
    "        assert cos.shape == (batch_size,n_tau,self.n_cos), \"cos shape is incorrect\"\n",
    "        return cos, taus\n",
    "    \n",
    "    def forward(self, input, num_tau=8):\n",
    "        \"\"\"\n",
    "        Quantile Calculation depending on the number of tau\n",
    "        \n",
    "        Return:\n",
    "        quantiles [ shape of (batch_size, num_tau, action_size)]\n",
    "        taus [shape of ((batch_size, num_tau, 1))]\n",
    "        \n",
    "        \"\"\"\n",
    "        batch_size = input.shape[0]\n",
    "        \n",
    "        #print(\"self.head(input):{}\".format(self.head(input).shape))\n",
    "        \n",
    "        x = torch.relu(self.head(input))\n",
    "    \n",
    "        #print(\"batch_size:{}\".format(batch_size))\n",
    "        #print(\"X:{}\".format(x.shape))\n",
    "        \n",
    "        cos, taus = self.calc_cos(batch_size, num_tau) # cos shape (batch, num_tau, layer_size)\n",
    "        \n",
    "        #print(\"cos:{}\".format(cos.shape))\n",
    "        #print(\"taus:{}\".format(taus.shape))\n",
    "        \n",
    "        \n",
    "        cos = cos.view(batch_size*num_tau, self.n_cos)\n",
    "        cos_x = torch.relu(self.cos_embedding(cos)).view(batch_size, num_tau, self.layer_size) # (batch, n_tau, layer)\n",
    "        \n",
    "        # x has shape (batch, layer_size) for multiplication –> reshape to (batch, 1, layer)\n",
    "        #x = (x.unsqueeze(1)*cos_x).view(batch_size*num_tau, self.layer_size)\n",
    "        #print(\"x:{},cos_x Shape:{},batch_size:{},layer_size:{}\".format(x.shape,cos_x.shape,batch_size,self.layer_size))\n",
    "        x = (x.unsqueeze(1) * cos_x).view(batch_size * num_tau, self.layer_size)\n",
    "        #print(\"---------°°°°°°°°°------X----------°°°°°°°°°°-------:{}\".format(x.shape))\n",
    "        \n",
    "        x = torch.relu(self.ff_1(x))\n",
    "        out = self.ff_2(x)\n",
    "        #print(\"---------°°°°°°°°°------out----------°°°°°°°°°°-------:{}\".format(out.shape))\n",
    "        \n",
    "        return out.view(batch_size, num_tau, self.action_size), taus\n",
    "    \n",
    "    def get_action(self, inputs):\n",
    "        quantiles, _ = self.forward(inputs, self.K)\n",
    "        #print(\"quantiles:{}\".format(quantiles.shape))\n",
    "        actions = quantiles.mean(dim=1) #TODO: actions space= torch.Size([1, 32, 30])\n",
    "        #print(\"action space quantile:{}\".format(actions))\n",
    "        return actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    \"\"\"Fixed-size buffer to store experience tuples.\"\"\"\n",
    "\n",
    "    def __init__(self, buffer_size, batch_size, device, seed, gamma, n_step=1):\n",
    "        \"\"\"Initialize a ReplayBuffer object.\n",
    "        Params\n",
    "        ======\n",
    "            buffer_size (int): maximum size of buffer\n",
    "            batch_size (int): size of each training batch\n",
    "            seed (int): random seed\n",
    "        \"\"\"\n",
    "        self.device = device\n",
    "        self.memory = deque(maxlen=buffer_size)  \n",
    "        self.batch_size = batch_size\n",
    "        self.experience = namedtuple(\"Experience\", field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
    "        self.seed = random.seed(seed)\n",
    "        self.gamma = gamma\n",
    "        self.n_step = n_step\n",
    "        self.n_step_buffer = deque(maxlen=self.n_step)\n",
    "    \n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Add a new experience to memory.\"\"\"\n",
    "        #print(\"before:\", state,action,reward,next_state, done)\n",
    "        self.n_step_buffer.append((state, action, reward, next_state, done))\n",
    "        if len(self.n_step_buffer) == self.n_step:\n",
    "            state, action, reward, next_state, done = self.calc_multistep_return()\n",
    "            #print(\"after:\",state,action,reward,next_state, done)\n",
    "            e = self.experience(state, action, reward, next_state, done)\n",
    "            self.memory.append(e)\n",
    "    \n",
    "    def calc_multistep_return(self):\n",
    "        Return = 0\n",
    "        for idx in range(self.n_step):\n",
    "            Return += self.gamma**idx * self.n_step_buffer[idx][2]\n",
    "        \n",
    "        return self.n_step_buffer[0][0], self.n_step_buffer[0][1], Return, self.n_step_buffer[-1][3], self.n_step_buffer[-1][4]\n",
    "        \n",
    "    \n",
    "    \n",
    "    def sample(self):\n",
    "        \"\"\"Randomly sample a batch of experiences from memory.\"\"\"\n",
    "        experiences = random.sample(self.memory, k=self.batch_size)\n",
    "\n",
    "        states = torch.from_numpy(np.stack([e.state for e in experiences if e is not None])).float().to(self.device)\n",
    "        actions = torch.from_numpy(np.vstack([e.action for e in experiences if e is not None])).long().to(self.device)\n",
    "        rewards = torch.from_numpy(np.vstack([e.reward for e in experiences if e is not None])).float().to(self.device)\n",
    "        next_states = torch.from_numpy(np.stack([e.next_state for e in experiences if e is not None])).float().to(self.device)\n",
    "        dones = torch.from_numpy(np.vstack([e.done for e in experiences if e is not None]).astype(np.uint8)).float().to(self.device)\n",
    "  \n",
    "        return (states, actions, rewards, next_states, dones)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Return the current size of internal memory.\"\"\"\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class DQN_Agent():\n",
    "    \"\"\"Interacts with and learns from the environment.\"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 state_size,\n",
    "                 action_size,\n",
    "                 layer_size,\n",
    "                 n_step,\n",
    "                 BATCH_SIZE,\n",
    "                 BUFFER_SIZE,\n",
    "                 LR,\n",
    "                 TAU,\n",
    "                 GAMMA,\n",
    "                 UPDATE_EVERY,\n",
    "                 device,\n",
    "                 seed):\n",
    "        \"\"\"Initialize an Agent object.\n",
    "        \n",
    "        Params\n",
    "        ======\n",
    "            state_size (int): dimension of each state\n",
    "            action_size (int): dimension of each action\n",
    "            layer_size (int): size of the hidden layer\n",
    "            BATCH_SIZE (int): size of the training batch\n",
    "            BUFFER_SIZE (int): size of the replay memory\n",
    "            LR (float): learning rate\n",
    "            TAU (float): tau for soft updating the network weights\n",
    "            GAMMA (float): discount factor\n",
    "            UPDATE_EVERY (int): update frequency\n",
    "            device (str): device that is used for the compute\n",
    "            seed (int): random seed\n",
    "        \"\"\"\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.seed = random.seed(seed)\n",
    "        self.device = device\n",
    "        self.TAU = TAU\n",
    "        self.GAMMA = GAMMA\n",
    "        self.UPDATE_EVERY = UPDATE_EVERY\n",
    "        self.BATCH_SIZE = BATCH_SIZE\n",
    "        self.Q_updates = 0\n",
    "        self.n_step = n_step\n",
    "        self.action = []\n",
    "\n",
    "        self.action_step = 30\n",
    "\n",
    "        # IQN-Network\n",
    "        self.qnetwork_local = IQN(state_size, action_size,layer_size, n_step, seed).to(device)\n",
    "        self.qnetwork_target = IQN(state_size, action_size,layer_size, n_step, seed).to(device)\n",
    "\n",
    "        self.optimizer = optim.Adam(self.qnetwork_local.parameters(), lr=LR)\n",
    "        #print(self.qnetwork_local)\n",
    "        \n",
    "        # Replay memory\n",
    "        self.memory = ReplayBuffer(BUFFER_SIZE, BATCH_SIZE, self.device, seed, self.GAMMA, n_step)\n",
    "        \n",
    "        # Initialize time step (for updating every UPDATE_EVERY steps)\n",
    "        self.t_step = 0\n",
    "    \n",
    "    def step(self, state, action, reward, next_state, done, writer):\n",
    "        # Save experience in replay memory\n",
    "        #print(\"to memory action:{},state:{},next_state\".format(action,state.shape,next_state.shape))\n",
    "        self.memory.add(state, action, reward, next_state, done)\n",
    "        \n",
    "        # Learn every UPDATE_EVERY time steps.\n",
    "        self.t_step = (self.t_step + 1) % self.UPDATE_EVERY\n",
    "        if self.t_step == 0:\n",
    "            # If enough samples are available in memory, get random subset and learn\n",
    "            if len(self.memory) > self.BATCH_SIZE:\n",
    "                \n",
    "                experiences = self.memory.sample()\n",
    "                #print(\"experiences:{}\".format(experiences))\n",
    "                loss = self.learn(experiences)\n",
    "                self.Q_updates += 1\n",
    "                writer.add_scalar(\"Q_loss\", loss, self.Q_updates)\n",
    "\n",
    "    def act(self, state, eps=0.):\n",
    "        \"\"\"Returns actions for given state as per current policy. Acting only every 4 frames!\n",
    "        \n",
    "        Params\n",
    "        ======\n",
    "            frame: to adjust epsilon\n",
    "            state (array_like): current state\n",
    "            \n",
    "        \"\"\"\n",
    "        #print(\"without np.array:{}\".format(state.shape))\n",
    "\n",
    "        state = np.array(state)\n",
    "\n",
    "        #print(\"this is the state space before torch.from_numpy:{}\".format(state.shape))\n",
    "        \n",
    "        state = torch.from_numpy(state).float().unsqueeze(0).to(self.device) #WHY?\n",
    "        \n",
    "        #print(\"this is the state space after torch.from_numpy:{}\".format(state.shape))\n",
    "        \n",
    "        \n",
    "        self.qnetwork_local.eval() #WHY?\n",
    "        \n",
    "        \n",
    "        with torch.no_grad():\n",
    "            #print(\"this is the state space:{}\".format(state.shape))\n",
    "            action_values = self.qnetwork_local.get_action(state) # 30 dimensions are coming back.\n",
    "            #print('action_value:{}'.format(action_values))\n",
    "        self.qnetwork_local.train()\n",
    "\n",
    "        # Epsilon-greedy action selection\n",
    "        if random.random() > eps: # select greedy action if random number is higher than epsilon or noisy network is used!\n",
    "            action = np.argmax(action_values.cpu().data.numpy())\n",
    "            self.last_action = action\n",
    "            return action\n",
    "        else:\n",
    "            action = random.choice(np.arange(self.action_size))\n",
    "            self.last_action = action \n",
    "            return action\n",
    "\n",
    "\n",
    "\n",
    "    def learn(self, experiences):\n",
    "        \"\"\"Update value parameters using given batch of experience tuples.\n",
    "        Params\n",
    "        ======\n",
    "            experiences (Tuple[torch.Tensor]): tuple of (s, a, r, s', done) tuples \n",
    "            gamma (float): discount factor\n",
    "        \"\"\"\n",
    "        self.optimizer.zero_grad()\n",
    "        states, actions, rewards, next_states, dones = experiences\n",
    "        #print(\"learning states:{}, next_states:{}\".format(states.shape, next_states.shape))\n",
    "        # Get max predicted Q values (for next states) from target model\n",
    "        Q_targets_next, _ = self.qnetwork_target(next_states)\n",
    "        \n",
    "        #print(\"--Q_targets_next :{}\".format(Q_targets_next.shape))\n",
    "        \n",
    "        #print(\"---->Q_targets_next:{}\".format(Q_targets_next))\n",
    "        #print('------------------------------------')\n",
    "        #print(\"--Q_targets_next detach max:{}\".format(Q_targets_next.detach().max(2)))\n",
    "        \n",
    "        \n",
    "        \n",
    "        #print(\"--Q_targets_next.detach().max(2)[0].unsqueeze(1):{}\".format(Q_targets_next.detach().max(2)[0].unsqueeze(1)))\n",
    "        Q_targets_next = Q_targets_next.detach().max(2)[0].unsqueeze(1) # (batch_size, 1, N)\n",
    "        \n",
    "        # Compute Q targets for current states \n",
    "        Q_targets = rewards.unsqueeze(-1) + (self.GAMMA**self.n_step * Q_targets_next * (1. - dones.unsqueeze(-1)))\n",
    "        # Get expected Q values from local model\n",
    "        Q_expected, taus = self.qnetwork_local(states)\n",
    "        \n",
    "        #print(\"rewards:{}\".format(rewards.shape))\n",
    "        #print(\"Q_targets_Shape:{}\".format(Q_targets.shape))\n",
    "        #print(\"actions shape:{}\".format(actions.shape))\n",
    "        #print(\"Q_expected shape:{}\".format(Q_expected.shape))\n",
    "        #print(\"actions.unsqueeze(-1).shape:{}\".format(actions.unsqueeze(-1).shape))\n",
    "        #print(\"actions:{}\".format(actions))\n",
    "        #print(\"Q_expected:{}\".format(Q_expected))\n",
    "        #print(\"actions.unsqueeze(-1){}\".format(actions.unsqueeze(-1)))\n",
    "        Q_expected_2 = Q_expected.gather(2, actions.unsqueeze(-1))\n",
    "\n",
    "        #print(\"Q_expected.gather(2, actions.unsqueeze(-1):{}\".format(Q_expected_2.shape))\n",
    "        \n",
    "        Q_expected = Q_expected.gather(2, actions[0].unsqueeze(-1).expand(self.BATCH_SIZE, 8, 1))\n",
    "        #print(\"Final what we need Q_expected-----:{}\".format(Q_expected.shape))\n",
    "\n",
    "        # Quantile Huber loss\n",
    "        td_error = Q_targets - Q_expected\n",
    "        #print(\"td_error.shape:{}\".format(td_error.shape))\n",
    "        #print(\"Q_expected.shape:{}\".format(Q_expected.shape))\n",
    "        #print(\"td_error:{}\".format(td_error.shape))\n",
    "        assert td_error.shape == (self.BATCH_SIZE, 8, 8), \"wrong td error shape\"\n",
    "        huber_l = calculate_huber_loss(td_error, 1.0)\n",
    "        quantil_l = abs(taus -(td_error.detach() < 0).float()) * huber_l / 1.0\n",
    "        \n",
    "        loss = quantil_l.sum(dim=1).mean(dim=1) # , keepdim=True if per weights get multipl\n",
    "        loss = loss.mean()\n",
    "\n",
    "\n",
    "        # Minimize the loss\n",
    "        loss.backward()\n",
    "        #clip_grad_norm_(self.qnetwork_local.parameters(),1)\n",
    "        self.optimizer.step()\n",
    "\n",
    "        # ------------------- update target network ------------------- #\n",
    "        self.soft_update(self.qnetwork_local, self.qnetwork_target)\n",
    "        return loss.detach().cpu().numpy()            \n",
    "\n",
    "    def soft_update(self, local_model, target_model):\n",
    "        \"\"\"Soft update model parameters.\n",
    "        θ_target = τ*θ_local + (1 - τ)*θ_target\n",
    "        Params\n",
    "        ======\n",
    "            local_model (PyTorch model): weights will be copied from\n",
    "            target_model (PyTorch model): weights will be copied to\n",
    "            tau (float): interpolation parameter \n",
    "        \"\"\"\n",
    "        for target_param, local_param in zip(target_model.parameters(), local_model.parameters()):\n",
    "            target_param.data.copy_(self.TAU*local_param.data + (1.0-self.TAU)*target_param.data)\n",
    "\n",
    "\n",
    "def calculate_huber_loss(td_errors, k=1.0):\n",
    "    \"\"\"\n",
    "    Calculate huber loss element-wisely depending on kappa k.\n",
    "    \"\"\"\n",
    "    loss = torch.where(td_errors.abs() <= k, 0.5 * td_errors.pow(2), k * (td_errors.abs() - 0.5 * k))\n",
    "    #print('this is huber loss: {}'.format(loss.shape))\n",
    "    assert loss.shape == (td_errors.shape[0], 8, 8), \"huber loss has wrong shape\"\n",
    "    return loss\n",
    "    \n",
    "def eval_runs(eps, frame):\n",
    "    \"\"\"\n",
    "    Makes an evaluation run with the current epsilon\n",
    "    \"\"\"\n",
    "    print(\"-----------------------------------------evaluating-----------------------------------------\")\n",
    "    env = gym.make(\"Acrobot-v1\") # TODO:\n",
    "    reward_batch = []\n",
    "    for i in range(5):\n",
    "        state = env.reset()\n",
    "        rewards = 0\n",
    "        while True:\n",
    "            action = agent.act(state, eps)\n",
    "            state, reward, done, _ = env.step(action)\n",
    "            rewards += reward\n",
    "            if done:\n",
    "                break\n",
    "        reward_batch.append(rewards)\n",
    "        \n",
    "    writer.add_scalar(\"Reward\", np.mean(reward_batch), frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using  cpu\n",
      "Action Space: 3\n",
      "State Space: 19\n",
      "[1.00000000e+06 3.05728571e+01 4.09200000e+01 5.61800000e+01\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 5.60928564e-01\n",
      " 3.58722215e-01 6.24669814e-01 6.21337044e+01 5.67090572e+01\n",
      " 5.85111632e+01 1.68824972e+02 2.09000651e+00 8.31548524e+01\n",
      " 3.37911464e+01 1.15370605e+01 1.08349581e+01]\n",
      "score: [0.06665883]\n",
      "action:[1], Number:1000\n",
      "-------------------------\n",
      "score: [1.1107471]\n",
      "action:[0], Number:2000\n",
      "-------------------------\n",
      "score: [0.8839561]\n",
      "action:[0], Number:3000\n",
      "-------------------------\n",
      "Finished\n",
      "Sharpe:  0.2310388731276227\n",
      "[1.00000000e+06 3.05728571e+01 4.09200000e+01 5.61800000e+01\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 5.60928564e-01\n",
      " 3.58722215e-01 6.24669814e-01 6.21337044e+01 5.67090572e+01\n",
      " 5.85111632e+01 1.68824972e+02 2.09000651e+00 8.31548524e+01\n",
      " 3.37911464e+01 1.15370605e+01 1.08349581e+01]\n",
      "Episode 1\tFrame 3020 \tAverage Score: 0.77[1.00000000e+06 3.05728571e+01 4.09200000e+01 5.61800000e+01\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 5.60928564e-01\n",
      " 3.58722215e-01 6.24669814e-01 6.21337044e+01 5.67090572e+01\n",
      " 5.85111632e+01 1.68824972e+02 2.09000651e+00 8.31548524e+01\n",
      " 3.37911464e+01 1.15370605e+01 1.08349581e+01]\n",
      "score: [0.5346775]\n",
      "action:[2], Number:4000\n",
      "-------------------------\n",
      "score: [3.829155]\n",
      "action:[0], Number:5000\n",
      "-------------------------\n",
      "score: [3.9490633]\n",
      "action:[1], Number:6000\n",
      "-------------------------\n",
      "Finished\n",
      "Sharpe:  0.3350384141131626\n",
      "[1.00000000e+06 3.05728571e+01 4.09200000e+01 5.61800000e+01\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 5.60928564e-01\n",
      " 3.58722215e-01 6.24669814e-01 6.21337044e+01 5.67090572e+01\n",
      " 5.85111632e+01 1.68824972e+02 2.09000651e+00 8.31548524e+01\n",
      " 3.37911464e+01 1.15370605e+01 1.08349581e+01]\n",
      "Episode 2\tFrame 6039 \tAverage Score: 2.20[1.00000000e+06 3.05728571e+01 4.09200000e+01 5.61800000e+01\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 5.60928564e-01\n",
      " 3.58722215e-01 6.24669814e-01 6.21337044e+01 5.67090572e+01\n",
      " 5.85111632e+01 1.68824972e+02 2.09000651e+00 8.31548524e+01\n",
      " 3.37911464e+01 1.15370605e+01 1.08349581e+01]\n",
      "score: [0.02554431]\n",
      "action:[2], Number:7000\n",
      "-------------------------\n",
      "score: [15.818062]\n",
      "action:[1], Number:8000\n",
      "-------------------------\n",
      "score: [15.630928]\n",
      "action:[2], Number:9000\n",
      "-------------------------\n",
      "Finished\n",
      "Sharpe:  0.41796980496053354\n",
      "[1.00000000e+06 3.05728571e+01 4.09200000e+01 5.61800000e+01\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 5.60928564e-01\n",
      " 3.58722215e-01 6.24669814e-01 6.21337044e+01 5.67090572e+01\n",
      " 5.85111632e+01 1.68824972e+02 2.09000651e+00 8.31548524e+01\n",
      " 3.37911464e+01 1.15370605e+01 1.08349581e+01]\n",
      "Episode 3\tFrame 9058 \tAverage Score: 5.99[1.00000000e+06 3.05728571e+01 4.09200000e+01 5.61800000e+01\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 5.60928564e-01\n",
      " 3.58722215e-01 6.24669814e-01 6.21337044e+01 5.67090572e+01\n",
      " 5.85111632e+01 1.68824972e+02 2.09000651e+00 8.31548524e+01\n",
      " 3.37911464e+01 1.15370605e+01 1.08349581e+01]\n",
      "score: [0.63665426]\n",
      "action:[0], Number:10000\n",
      "-------------------------\n",
      "score: [9.241258]\n",
      "action:[1], Number:11000\n",
      "-------------------------\n",
      "score: [9.91583]\n",
      "action:[1], Number:12000\n",
      "-------------------------\n",
      "Finished\n",
      "Sharpe:  0.5590079887530526\n",
      "[1.00000000e+06 3.05728571e+01 4.09200000e+01 5.61800000e+01\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 5.60928564e-01\n",
      " 3.58722215e-01 6.24669814e-01 6.21337044e+01 5.67090572e+01\n",
      " 5.85111632e+01 1.68824972e+02 2.09000651e+00 8.31548524e+01\n",
      " 3.37911464e+01 1.15370605e+01 1.08349581e+01]\n",
      "Episode 4\tFrame 12077 \tAverage Score: 6.82[1.00000000e+06 3.05728571e+01 4.09200000e+01 5.61800000e+01\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 5.60928564e-01\n",
      " 3.58722215e-01 6.24669814e-01 6.21337044e+01 5.67090572e+01\n",
      " 5.85111632e+01 1.68824972e+02 2.09000651e+00 8.31548524e+01\n",
      " 3.37911464e+01 1.15370605e+01 1.08349581e+01]\n",
      "score: [1.0639174]\n",
      "action:[2], Number:13000\n",
      "-------------------------\n",
      "score: [49.255123]\n",
      "action:[1], Number:14000\n",
      "-------------------------\n",
      "score: [51.986313]\n",
      "action:[1], Number:15000\n",
      "-------------------------\n",
      "Finished\n",
      "Sharpe:  0.4588182934897747\n",
      "[1.00000000e+06 3.05728571e+01 4.09200000e+01 5.61800000e+01\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 5.60928564e-01\n",
      " 3.58722215e-01 6.24669814e-01 6.21337044e+01 5.67090572e+01\n",
      " 5.85111632e+01 1.68824972e+02 2.09000651e+00 8.31548524e+01\n",
      " 3.37911464e+01 1.15370605e+01 1.08349581e+01]\n",
      "Episode 5\tFrame 15096 \tAverage Score: 14.87[1.00000000e+06 3.05728571e+01 4.09200000e+01 5.61800000e+01\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 5.60928564e-01\n",
      " 3.58722215e-01 6.24669814e-01 6.21337044e+01 5.67090572e+01\n",
      " 5.85111632e+01 1.68824972e+02 2.09000651e+00 8.31548524e+01\n",
      " 3.37911464e+01 1.15370605e+01 1.08349581e+01]\n",
      "score: [0.01357435]\n",
      "action:[2], Number:16000\n",
      "-------------------------\n",
      "score: [13.633976]\n",
      "action:[2], Number:17000\n",
      "-------------------------\n",
      "score: [17.212593]\n",
      "action:[0], Number:18000\n",
      "-------------------------\n",
      "Finished\n",
      "Sharpe:  0.7020750204139002\n",
      "[1.00000000e+06 3.05728571e+01 4.09200000e+01 5.61800000e+01\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 5.60928564e-01\n",
      " 3.58722215e-01 6.24669814e-01 6.21337044e+01 5.67090572e+01\n",
      " 5.85111632e+01 1.68824972e+02 2.09000651e+00 8.31548524e+01\n",
      " 3.37911464e+01 1.15370605e+01 1.08349581e+01]\n",
      "Episode 6\tFrame 18115 \tAverage Score: 15.26[1.00000000e+06 3.05728571e+01 4.09200000e+01 5.61800000e+01\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 5.60928564e-01\n",
      " 3.58722215e-01 6.24669814e-01 6.21337044e+01 5.67090572e+01\n",
      " 5.85111632e+01 1.68824972e+02 2.09000651e+00 8.31548524e+01\n",
      " 3.37911464e+01 1.15370605e+01 1.08349581e+01]\n",
      "score: [-1.0749931]\n",
      "action:[0], Number:19000\n",
      "-------------------------\n",
      "score: [2.0442035]\n",
      "action:[2], Number:20000\n",
      "-------------------------\n",
      "score: [2.1867914]\n",
      "action:[0], Number:21000\n",
      "-------------------------\n",
      "Finished\n",
      "Sharpe:  0.13514174497462872\n",
      "[1.00000000e+06 3.05728571e+01 4.09200000e+01 5.61800000e+01\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 5.60928564e-01\n",
      " 3.58722215e-01 6.24669814e-01 6.21337044e+01 5.67090572e+01\n",
      " 5.85111632e+01 1.68824972e+02 2.09000651e+00 8.31548524e+01\n",
      " 3.37911464e+01 1.15370605e+01 1.08349581e+01]\n",
      "Episode 7\tFrame 21134 \tAverage Score: 13.54[1.00000000e+06 3.05728571e+01 4.09200000e+01 5.61800000e+01\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 5.60928564e-01\n",
      " 3.58722215e-01 6.24669814e-01 6.21337044e+01 5.67090572e+01\n",
      " 5.85111632e+01 1.68824972e+02 2.09000651e+00 8.31548524e+01\n",
      " 3.37911464e+01 1.15370605e+01 1.08349581e+01]\n",
      "score: [0.14428043]\n",
      "action:[1], Number:22000\n",
      "-------------------------\n",
      "score: [19.180183]\n",
      "action:[2], Number:23000\n",
      "-------------------------\n",
      "score: [23.669266]\n",
      "action:[0], Number:24000\n",
      "-------------------------\n",
      "Finished\n",
      "Sharpe:  0.43989598294441573\n",
      "[1.00000000e+06 3.05728571e+01 4.09200000e+01 5.61800000e+01\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 5.60928564e-01\n",
      " 3.58722215e-01 6.24669814e-01 6.21337044e+01 5.67090572e+01\n",
      " 5.85111632e+01 1.68824972e+02 2.09000651e+00 8.31548524e+01\n",
      " 3.37911464e+01 1.15370605e+01 1.08349581e+01]\n",
      "Episode 8\tFrame 24153 \tAverage Score: 14.92[1.00000000e+06 3.05728571e+01 4.09200000e+01 5.61800000e+01\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 5.60928564e-01\n",
      " 3.58722215e-01 6.24669814e-01 6.21337044e+01 5.67090572e+01\n",
      " 5.85111632e+01 1.68824972e+02 2.09000651e+00 8.31548524e+01\n",
      " 3.37911464e+01 1.15370605e+01 1.08349581e+01]\n",
      "score: [0.09279703]\n",
      "action:[0], Number:25000\n",
      "-------------------------\n",
      "score: [8.121085]\n",
      "action:[0], Number:26000\n",
      "-------------------------\n",
      "score: [13.045349]\n",
      "action:[0], Number:27000\n",
      "-------------------------\n",
      "Finished\n",
      "Sharpe:  0.313770560827471\n",
      "[1.00000000e+06 3.05728571e+01 4.09200000e+01 5.61800000e+01\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 5.60928564e-01\n",
      " 3.58722215e-01 6.24669814e-01 6.21337044e+01 5.67090572e+01\n",
      " 5.85111632e+01 1.68824972e+02 2.09000651e+00 8.31548524e+01\n",
      " 3.37911464e+01 1.15370605e+01 1.08349581e+01]\n",
      "Episode 9\tFrame 27172 \tAverage Score: 14.75[1.00000000e+06 3.05728571e+01 4.09200000e+01 5.61800000e+01\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 5.60928564e-01\n",
      " 3.58722215e-01 6.24669814e-01 6.21337044e+01 5.67090572e+01\n",
      " 5.85111632e+01 1.68824972e+02 2.09000651e+00 8.31548524e+01\n",
      " 3.37911464e+01 1.15370605e+01 1.08349581e+01]\n",
      "score: [-0.7229712]\n",
      "action:[1], Number:28000\n",
      "-------------------------\n",
      "score: [24.376043]\n",
      "action:[1], Number:29000\n",
      "-------------------------\n",
      "score: [47.730698]\n",
      "action:[0], Number:30000\n",
      "-------------------------\n",
      "Finished\n",
      "Sharpe:  0.4754253228165444\n",
      "[1.00000000e+06 3.05728571e+01 4.09200000e+01 5.61800000e+01\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 5.60928564e-01\n",
      " 3.58722215e-01 6.24669814e-01 6.21337044e+01 5.67090572e+01\n",
      " 5.85111632e+01 1.68824972e+02 2.09000651e+00 8.31548524e+01\n",
      " 3.37911464e+01 1.15370605e+01 1.08349581e+01]\n",
      "Episode 10\tFrame 30191 \tAverage Score: 17.39[1.00000000e+06 3.05728571e+01 4.09200000e+01 5.61800000e+01\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 5.60928564e-01\n",
      " 3.58722215e-01 6.24669814e-01 6.21337044e+01 5.67090572e+01\n",
      " 5.85111632e+01 1.68824972e+02 2.09000651e+00 8.31548524e+01\n",
      " 3.37911464e+01 1.15370605e+01 1.08349581e+01]\n",
      "score: [-0.02140243]\n",
      "action:[0], Number:31000\n",
      "-------------------------\n",
      "score: [2.4781466]\n",
      "action:[1], Number:32000\n",
      "-------------------------\n",
      "score: [3.1226623]\n",
      "action:[0], Number:33000\n",
      "-------------------------\n",
      "Finished\n",
      "Sharpe:  0.31077340200592973\n",
      "[1.00000000e+06 3.05728571e+01 4.09200000e+01 5.61800000e+01\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 5.60928564e-01\n",
      " 3.58722215e-01 6.24669814e-01 6.21337044e+01 5.67090572e+01\n",
      " 5.85111632e+01 1.68824972e+02 2.09000651e+00 8.31548524e+01\n",
      " 3.37911464e+01 1.15370605e+01 1.08349581e+01]\n",
      "Episode 11\tFrame 33210 \tAverage Score: 16.05[1.00000000e+06 3.05728571e+01 4.09200000e+01 5.61800000e+01\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 5.60928564e-01\n",
      " 3.58722215e-01 6.24669814e-01 6.21337044e+01 5.67090572e+01\n",
      " 5.85111632e+01 1.68824972e+02 2.09000651e+00 8.31548524e+01\n",
      " 3.37911464e+01 1.15370605e+01 1.08349581e+01]\n",
      "score: [-0.08916558]\n",
      "action:[0], Number:34000\n",
      "-------------------------\n",
      "score: [0.18527022]\n",
      "action:[0], Number:35000\n",
      "-------------------------\n",
      "score: [0.36587644]\n",
      "action:[1], Number:36000\n",
      "-------------------------\n",
      "Finished\n",
      "Sharpe:  0.18645540564286742\n",
      "[1.00000000e+06 3.05728571e+01 4.09200000e+01 5.61800000e+01\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 5.60928564e-01\n",
      " 3.58722215e-01 6.24669814e-01 6.21337044e+01 5.67090572e+01\n",
      " 5.85111632e+01 1.68824972e+02 2.09000651e+00 8.31548524e+01\n",
      " 3.37911464e+01 1.15370605e+01 1.08349581e+01]\n",
      "Episode 12\tFrame 36229 \tAverage Score: 14.75[1.00000000e+06 3.05728571e+01 4.09200000e+01 5.61800000e+01\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 5.60928564e-01\n",
      " 3.58722215e-01 6.24669814e-01 6.21337044e+01 5.67090572e+01\n",
      " 5.85111632e+01 1.68824972e+02 2.09000651e+00 8.31548524e+01\n",
      " 3.37911464e+01 1.15370605e+01 1.08349581e+01]\n",
      "score: [0.00854744]\n",
      "action:[1], Number:37000\n",
      "-------------------------\n",
      "score: [0.02865094]\n",
      "action:[0], Number:38000\n",
      "-------------------------\n",
      "score: [0.10654532]\n",
      "action:[2], Number:39000\n",
      "-------------------------\n",
      "Finished\n",
      "Sharpe:  -0.03422972224409643\n",
      "[1.00000000e+06 3.05728571e+01 4.09200000e+01 5.61800000e+01\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 5.60928564e-01\n",
      " 3.58722215e-01 6.24669814e-01 6.21337044e+01 5.67090572e+01\n",
      " 5.85111632e+01 1.68824972e+02 2.09000651e+00 8.31548524e+01\n",
      " 3.37911464e+01 1.15370605e+01 1.08349581e+01]\n",
      "Episode 13\tFrame 39248 \tAverage Score: 13.47[1.00000000e+06 3.05728571e+01 4.09200000e+01 5.61800000e+01\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 5.60928564e-01\n",
      " 3.58722215e-01 6.24669814e-01 6.21337044e+01 5.67090572e+01\n",
      " 5.85111632e+01 1.68824972e+02 2.09000651e+00 8.31548524e+01\n",
      " 3.37911464e+01 1.15370605e+01 1.08349581e+01]\n",
      "score: [3.6719465]\n",
      "action:[0], Number:40000\n",
      "-------------------------\n",
      "score: [9.952831]\n",
      "action:[1], Number:41000\n",
      "-------------------------\n",
      "score: [15.520131]\n",
      "action:[1], Number:42000\n",
      "-------------------------\n",
      "Finished\n",
      "Sharpe:  0.34160544081967775\n",
      "[1.00000000e+06 3.05728571e+01 4.09200000e+01 5.61800000e+01\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 5.60928564e-01\n",
      " 3.58722215e-01 6.24669814e-01 6.21337044e+01 5.67090572e+01\n",
      " 5.85111632e+01 1.68824972e+02 2.09000651e+00 8.31548524e+01\n",
      " 3.37911464e+01 1.15370605e+01 1.08349581e+01]\n",
      "Episode 14\tFrame 42267 \tAverage Score: 13.47[1.00000000e+06 3.05728571e+01 4.09200000e+01 5.61800000e+01\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 5.60928564e-01\n",
      " 3.58722215e-01 6.24669814e-01 6.21337044e+01 5.67090572e+01\n",
      " 5.85111632e+01 1.68824972e+02 2.09000651e+00 8.31548524e+01\n",
      " 3.37911464e+01 1.15370605e+01 1.08349581e+01]\n",
      "score: [0.62545776]\n",
      "action:[0], Number:43000\n",
      "-------------------------\n",
      "score: [0.6643591]\n",
      "action:[1], Number:44000\n",
      "-------------------------\n",
      "score: [0.29777983]\n",
      "action:[2], Number:45000\n",
      "-------------------------\n",
      "Finished\n",
      "Sharpe:  -0.009841675307665077\n",
      "[1.00000000e+06 3.05728571e+01 4.09200000e+01 5.61800000e+01\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 5.60928564e-01\n",
      " 3.58722215e-01 6.24669814e-01 6.21337044e+01 5.67090572e+01\n",
      " 5.85111632e+01 1.68824972e+02 2.09000651e+00 8.31548524e+01\n",
      " 3.37911464e+01 1.15370605e+01 1.08349581e+01]\n",
      "Episode 15\tFrame 45286 \tAverage Score: 12.53[1.00000000e+06 3.05728571e+01 4.09200000e+01 5.61800000e+01\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 5.60928564e-01\n",
      " 3.58722215e-01 6.24669814e-01 6.21337044e+01 5.67090572e+01\n",
      " 5.85111632e+01 1.68824972e+02 2.09000651e+00 8.31548524e+01\n",
      " 3.37911464e+01 1.15370605e+01 1.08349581e+01]\n",
      "score: [-0.00318947]\n",
      "action:[1], Number:46000\n",
      "-------------------------\n",
      "score: [6.5928903]\n",
      "action:[2], Number:47000\n",
      "-------------------------\n",
      "score: [19.75523]\n",
      "action:[1], Number:48000\n",
      "-------------------------\n",
      "Finished\n",
      "Sharpe:  0.2417403805266506\n",
      "[1.00000000e+06 3.05728571e+01 4.09200000e+01 5.61800000e+01\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 5.60928564e-01\n",
      " 3.58722215e-01 6.24669814e-01 6.21337044e+01 5.67090572e+01\n",
      " 5.85111632e+01 1.68824972e+02 2.09000651e+00 8.31548524e+01\n",
      " 3.37911464e+01 1.15370605e+01 1.08349581e+01]\n",
      "Episode 16\tFrame 48305 \tAverage Score: 12.66[1.00000000e+06 3.05728571e+01 4.09200000e+01 5.61800000e+01\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 5.60928564e-01\n",
      " 3.58722215e-01 6.24669814e-01 6.21337044e+01 5.67090572e+01\n",
      " 5.85111632e+01 1.68824972e+02 2.09000651e+00 8.31548524e+01\n",
      " 3.37911464e+01 1.15370605e+01 1.08349581e+01]\n",
      "score: [5.939653]\n",
      "action:[2], Number:49000\n",
      "-------------------------\n",
      "score: [25.605543]\n",
      "action:[1], Number:50000\n",
      "-------------------------\n",
      "score: [51.547382]\n",
      "action:[2], Number:51000\n",
      "-------------------------\n",
      "Finished\n",
      "Sharpe:  0.46518884456159454\n",
      "[1.00000000e+06 3.05728571e+01 4.09200000e+01 5.61800000e+01\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 5.60928564e-01\n",
      " 3.58722215e-01 6.24669814e-01 6.21337044e+01 5.67090572e+01\n",
      " 5.85111632e+01 1.68824972e+02 2.09000651e+00 8.31548524e+01\n",
      " 3.37911464e+01 1.15370605e+01 1.08349581e+01]\n",
      "Episode 17\tFrame 51324 \tAverage Score: 14.57[1.00000000e+06 3.05728571e+01 4.09200000e+01 5.61800000e+01\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 5.60928564e-01\n",
      " 3.58722215e-01 6.24669814e-01 6.21337044e+01 5.67090572e+01\n",
      " 5.85111632e+01 1.68824972e+02 2.09000651e+00 8.31548524e+01\n",
      " 3.37911464e+01 1.15370605e+01 1.08349581e+01]\n",
      "score: [0.]\n",
      "action:[1], Number:52000\n",
      "-------------------------\n",
      "score: [2.2961693]\n",
      "action:[1], Number:53000\n",
      "-------------------------\n",
      "score: [4.7787247]\n",
      "action:[2], Number:54000\n",
      "-------------------------\n",
      "Finished\n",
      "Sharpe:  0.5723756562231186\n",
      "[1.00000000e+06 3.05728571e+01 4.09200000e+01 5.61800000e+01\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 5.60928564e-01\n",
      " 3.58722215e-01 6.24669814e-01 6.21337044e+01 5.67090572e+01\n",
      " 5.85111632e+01 1.68824972e+02 2.09000651e+00 8.31548524e+01\n",
      " 3.37911464e+01 1.15370605e+01 1.08349581e+01]\n",
      "Episode 18\tFrame 54343 \tAverage Score: 14.03[1.00000000e+06 3.05728571e+01 4.09200000e+01 5.61800000e+01\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 5.60928564e-01\n",
      " 3.58722215e-01 6.24669814e-01 6.21337044e+01 5.67090572e+01\n",
      " 5.85111632e+01 1.68824972e+02 2.09000651e+00 8.31548524e+01\n",
      " 3.37911464e+01 1.15370605e+01 1.08349581e+01]\n",
      "score: [0.0477883]\n",
      "action:[2], Number:55000\n",
      "-------------------------\n",
      "score: [3.9116874]\n",
      "action:[2], Number:56000\n",
      "-------------------------\n",
      "score: [5.1249948]\n",
      "action:[1], Number:57000\n",
      "-------------------------\n",
      "Finished\n",
      "Sharpe:  0.3017798595221611\n",
      "[1.00000000e+06 3.05728571e+01 4.09200000e+01 5.61800000e+01\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 5.60928564e-01\n",
      " 3.58722215e-01 6.24669814e-01 6.21337044e+01 5.67090572e+01\n",
      " 5.85111632e+01 1.68824972e+02 2.09000651e+00 8.31548524e+01\n",
      " 3.37911464e+01 1.15370605e+01 1.08349581e+01]\n",
      "Episode 19\tFrame 57362 \tAverage Score: 13.54[1.00000000e+06 3.05728571e+01 4.09200000e+01 5.61800000e+01\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 5.60928564e-01\n",
      " 3.58722215e-01 6.24669814e-01 6.21337044e+01 5.67090572e+01\n",
      " 5.85111632e+01 1.68824972e+02 2.09000651e+00 8.31548524e+01\n",
      " 3.37911464e+01 1.15370605e+01 1.08349581e+01]\n",
      "score: [0.3067464]\n",
      "action:[0], Number:58000\n",
      "-------------------------\n",
      "score: [1.1000851]\n",
      "action:[0], Number:59000\n",
      "-------------------------\n",
      "score: [1.1653436]\n",
      "action:[1], Number:60000\n",
      "-------------------------\n",
      "Training time: 7.69min\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from IPython import display\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from gym.utils import seeding\n",
    "import gym\n",
    "import os\n",
    "from gym import spaces\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "\n",
    "# shares normalization factor\n",
    "# 100 shares per trade\n",
    "HMAX_NORMALIZE = 100\n",
    "# initial amount of money we have in our account\n",
    "INITIAL_ACCOUNT_BALANCE= 1000000\n",
    "# total number of stocks in our portfolio\n",
    "STOCK_DIM = 3\n",
    "# transaction fee: 1/1000 reasonable percentage\n",
    "TRANSACTION_FEE_PERCENT = 0.001\n",
    "REWARD_SCALING = 1e-4\n",
    "\n",
    "class StockEnvTrain(gym.Env):\n",
    "    \"\"\"A stock trading environment for OpenAI gym\"\"\"\n",
    "    metadata = {'render.modes': ['human']}\n",
    "\n",
    "    def __init__(self, df,day = 0):\n",
    "        #super(StockEnv, self).__init__()\n",
    "        #money = 10 , scope = 1\n",
    "        self.day = day\n",
    "        self.df = df\n",
    "        self.agent_stock_iteration_index = 0\n",
    "\n",
    "        # action_space normalization and shape is STOCK_DIM\n",
    "        self.action_space = spaces.Box(low = -1, high = 1,shape = (STOCK_DIM,)) \n",
    "        # Shape = 181: [Current Balance]+[prices 1-30]+[owned shares 1-30] \n",
    "        # +[macd 1-30]+ [rsi 1-30] + [cci 1-30] + [adx 1-30]\n",
    "        self.observation_space = spaces.Box(low=0, high=np.inf, shape = (19,))\n",
    "        # load data from a pandas dataframe\n",
    "        #print('df: {}'.format(self.df))\n",
    "        #print('day: {}'.format(self.day))\n",
    "        self.data = self.df.loc[self.day,:]\n",
    "        #print(self.data.Close)\n",
    "        self.terminal = False\n",
    "\n",
    "\n",
    "\n",
    "        # initalize state\n",
    "        self.state = [INITIAL_ACCOUNT_BALANCE] + \\\n",
    "                      self.data.Close.values.tolist() + \\\n",
    "                      [0]*STOCK_DIM + \\\n",
    "                      self.data.macd.values.tolist() + \\\n",
    "                      self.data.rsi.values.tolist() + \\\n",
    "                      self.data.cci.values.tolist() + \\\n",
    "                      self.data.adx.values.tolist()\n",
    "        # initialize reward\n",
    "        self.reward = 0\n",
    "        self.cost = 0\n",
    "        # memorize all the total balance change\n",
    "        self.asset_memory = [INITIAL_ACCOUNT_BALANCE]\n",
    "        self.rewards_memory = []\n",
    "        self.trades = 0\n",
    "        #self.reset()\n",
    "        self._seed()\n",
    "\n",
    "\n",
    "    def _sell_stock(self, index, action):\n",
    "        # perform sell action based on the sign of the action\n",
    "        if self.state[index+STOCK_DIM+1] > 0:\n",
    "            #update balance\n",
    "            self.state[0] += \\\n",
    "            self.state[index+1]*min(abs(action),self.state[index+STOCK_DIM+1]) * \\\n",
    "             (1- TRANSACTION_FEE_PERCENT)\n",
    "\n",
    "            self.state[index+STOCK_DIM+1] -= min(abs(action), self.state[index+STOCK_DIM+1])\n",
    "            self.cost +=self.state[index+1]*min(abs(action),self.state[index+STOCK_DIM+1]) * \\\n",
    "             TRANSACTION_FEE_PERCENT\n",
    "            self.trades+=1\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "    \n",
    "    def _buy_stock(self, index, action):\n",
    "        # perform buy action based on the sign of the action\n",
    "        available_amount = self.state[0] // self.state[index+1]\n",
    "        # print('available_amount:{}'.format(available_amount))\n",
    "\n",
    "        #update balance\n",
    "        self.state[0] -= self.state[index+1]*min(available_amount, action)* \\\n",
    "                          (1+ TRANSACTION_FEE_PERCENT)\n",
    "\n",
    "        self.state[index+STOCK_DIM+1] += min(available_amount, action)\n",
    "\n",
    "        self.cost+=self.state[index+1]*min(available_amount, action)* \\\n",
    "                          TRANSACTION_FEE_PERCENT\n",
    "        self.trades+=1\n",
    "        \n",
    "    def step(self, actions):\n",
    "        # print(self.day)\n",
    "        self.terminal = self.day >= len(self.df.index.unique())-1\n",
    "        #print(actions)\n",
    "        self.actions = actions\n",
    "        if self.terminal:\n",
    "            print(\"Finished\")\n",
    "\n",
    "            end_total_asset = self.state[0]+ \\\n",
    "            sum(np.array(self.state[1:(STOCK_DIM+1)])*np.array(self.state[(STOCK_DIM+1):(STOCK_DIM*2+1)]))\n",
    "\n",
    "            #print(\"end_total_asset:{}\".format(end_total_asset))\n",
    "            df_total_value = pd.DataFrame(self.asset_memory)\n",
    "            #df_total_value.to_csv('results/account_value_train.csv')\n",
    "            #print(\"total_reward:{}\".format(self.state[0]+sum(np.array(self.state[1:(STOCK_DIM+1)])*np.array(self.state[(STOCK_DIM+1):61]))- INITIAL_ACCOUNT_BALANCE ))\n",
    "            #print(\"total_cost: \", self.cost)\n",
    "            #print(\"total_trades: \", self.trades)\n",
    "            df_total_value.columns = ['account_value']\n",
    "            df_total_value['daily_return']=df_total_value.pct_change(1)\n",
    "            sharpe = (252**0.5)*df_total_value['daily_return'].mean()/ \\\n",
    "                  df_total_value['daily_return'].std()\n",
    "            print(\"Sharpe: \",sharpe)\n",
    "            #print(\"=================================\")\n",
    "            df_rewards = pd.DataFrame(self.rewards_memory)\n",
    "            #df_rewards.to_csv('results/account_rewards_train.csv')\n",
    "\n",
    "            # print('total asset: {}'.format(self.state[0]+ sum(np.array(self.state[1:29])*np.array(self.state[29:]))))\n",
    "            #with open('obs.pkl', 'wb') as f:  \n",
    "            #    pickle.dump(self.state, f)\n",
    "\n",
    "            return self.state, self.reward, self.terminal,{}\n",
    "\n",
    "        else:\n",
    "            # print(np.array(self.state[1:29]))\n",
    "            #print(\"The actions is: {}\".format(self.actions))\n",
    "\n",
    "            #action = np.array([4,4,5])\n",
    "            #actions = np.array([4,0,0,0,0,0,0,0,4,0,4,0,-3,0,0,0,0,0,0,0,4,0,0,0,0,0,0,0,0,0])\n",
    "\n",
    "            #actions = self.actions * HMAX_NORMALIZE #WHY??\n",
    "            #print(\"actions-index------:{}\".format(actions))\n",
    "            #actions = (actions.astype(int))\n",
    "\n",
    "            begin_total_asset = self.state[0]+ \\\n",
    "            sum(np.array(self.state[1:(STOCK_DIM+1)])*np.array(self.state[(STOCK_DIM+1):(STOCK_DIM*2+1)]))\n",
    "            #print(\"begin_total_asset:{}\".format(begin_total_asset))\n",
    "\n",
    "            argsort_actions = np.argsort(actions) #TODO: this may not be touched.\n",
    "            #print(\"The actions is: {}\".format(actions))\n",
    "\n",
    "            sell_index = argsort_actions[:np.where(actions == 0)[0].shape[0]]\n",
    "            #sell_index = argsort_actions[4,0,0,0,0,0,0,0,4,0,4,0,-3,0,0,0,0,0,0,0,4,0,0,0,0,0,0,0,0,0]\n",
    "            #print(\"sell-index------:{}\".format(sell_index))\n",
    "            buy_index = argsort_actions[::-1][:np.where(actions == 2)[0].shape[0]]\n",
    "            #buy_index = argsort_actions[::-1][4,0,0,0,0,0,0,0,4,0,4,0,-3,0,0,0,0,0,0,0,4,0,0,0,0,0,0,0,0,0]\n",
    "            #print(\"buy-index------:{}\".format(buy_index))\n",
    "\n",
    "            for index in sell_index:\n",
    "            # print('take sell action'.format(actions[index]))\n",
    "                #print(\"--------Action Shape:{}\".format(actions.shape))\n",
    "                self._sell_stock(index+ self.agent_stock_iteration_index, 10)\n",
    "\n",
    "            for index in buy_index:\n",
    "                #print(\"--------Action Shape:{}\".format(actions.shape))\n",
    "            # print('take buy action: {}'.format(actions[index]))\n",
    "                self._buy_stock(index+ self.agent_stock_iteration_index, 10)\n",
    "                \n",
    "            \n",
    "            #print(\"self.day:{}\".format(self.day))\n",
    "            \n",
    "            if self.agent_stock_iteration_index ==2:\n",
    "                self.day += 1\n",
    "                self.data = self.df.loc[self.day,:]\n",
    "                self.agent_stock_iteration_index = 0\n",
    "                \n",
    "            #load next state\n",
    "            # print(\"stock_shares:{}\".format(self.state[29:]))\n",
    "            self.state =  [self.state[0]] + \\\n",
    "                self.data.Close.values.tolist() + \\\n",
    "                list(self.state[(STOCK_DIM+1):(STOCK_DIM*2+1)]) + \\\n",
    "                self.data.macd.values.tolist() + \\\n",
    "                self.data.rsi.values.tolist() + \\\n",
    "                self.data.cci.values.tolist() + \\\n",
    "                self.data.adx.values.tolist()\n",
    "\n",
    "            end_total_asset = self.state[0]+ \\\n",
    "            sum(np.array(self.state[1:(STOCK_DIM+1)])*np.array(self.state[(STOCK_DIM+1):(STOCK_DIM*2+1)]))\n",
    "            self.asset_memory.append(end_total_asset)\n",
    "            #print(\"end_total_asset:{}\".format(end_total_asset))\n",
    "\n",
    "            self.reward = end_total_asset - begin_total_asset            \n",
    "            # print(\"step_reward:{}\".format(self.reward))\n",
    "            self.rewards_memory.append(self.reward)\n",
    "\n",
    "            self.reward = self.reward*REWARD_SCALING\n",
    "\n",
    "            self.agent_stock_iteration_index += 1 \n",
    "\n",
    "        return self.state, self.reward, self.terminal, {}\n",
    "\n",
    "    def reset(self):\n",
    "        self.asset_memory = [INITIAL_ACCOUNT_BALANCE]\n",
    "        self.day = 0\n",
    "        self.data = self.df.loc[self.day,:]\n",
    "        self.cost = 0\n",
    "        self.trades = 0\n",
    "        self.terminal = False \n",
    "        self.rewards_memory = []\n",
    "        #initiate state\n",
    "        self.state = [INITIAL_ACCOUNT_BALANCE] + \\\n",
    "                      self.data.Close.values.tolist() + \\\n",
    "                      [0]*STOCK_DIM + \\\n",
    "                      self.data.macd.values.tolist() + \\\n",
    "                      self.data.rsi.values.tolist() + \\\n",
    "                      self.data.cci.values.tolist() + \\\n",
    "                      self.data.adx.values.tolist() \n",
    "        # iteration += 1 \n",
    "        #print(\"[0]*STOCK_DIM:{}\".format([0]*STOCK_DIM))\n",
    "        #print(\"self.state:{}\".format(len(self.state)))\n",
    "        print(np.array(self.state))\n",
    "        return np.array(self.state)\n",
    "    \n",
    "    def render(self, mode='human'):\n",
    "        return self.state\n",
    "\n",
    "    def _seed(self, seed=None):\n",
    "        self.np_random, seed = seeding.np_random(seed)\n",
    "        return [seed]\n",
    "\n",
    "\n",
    "def run(env,frames=1000, eps_fixed=False, eps_frames=1e6, min_eps=0.01):\n",
    "    \"\"\"Deep Q-Learning.\n",
    "    \n",
    "    Params\n",
    "    ======\n",
    "        n_episodes (int): maximum number of training episodes\n",
    "        max_t (int): maximum number of timesteps per episode\n",
    "        eps_start (float): starting value of epsilon, for epsilon-greedy action selection\n",
    "        eps_end (float): minimum value of epsilon\n",
    "        eps_decay (float): multiplicative factor (per episode) for decreasing epsilon\n",
    "    \"\"\"\n",
    "    scores = []                        # list containing scores from each episode\n",
    "    scores_window = deque(maxlen=100)  # last 100 scores\n",
    "    output_history = []\n",
    "    frame = 0\n",
    "    if eps_fixed:\n",
    "        eps = 0\n",
    "    else:\n",
    "        eps = 1\n",
    "    eps_start = 1\n",
    "    i_episode = 1\n",
    "    state = env.reset()\n",
    "    state = state[0,:]\n",
    "    #print(\"state space:{}\".format(state[0,:].shape))\n",
    "    score = 0                  \n",
    "    for frame in range(1, frames+1):\n",
    "        \n",
    "        if frame  == 0:\n",
    "            # inital state\n",
    "\n",
    "            initial = True\n",
    "        else:\n",
    "            # previous state\n",
    "            initial = False\n",
    "        #print('initial state:{}'.format(initial))\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        action = agent.act(state, eps) #TODO: getting one dimension back.\n",
    "        \n",
    "        \n",
    "        action = np.array([action])\n",
    "        \n",
    "        \n",
    "        next_state, reward, done, info = env_train.step([action]) #TODO: Wants a list of actions of size a\n",
    "\n",
    "        #print(\"env_trainNext State: {}\".format(next_state.shape))\n",
    "\n",
    "        next_state = next_state[0,:]\n",
    "        \n",
    "        agent.step(state, action, reward, next_state, done, writer)\n",
    "        \n",
    "\n",
    "        state = next_state\n",
    "        score += reward\n",
    "        # linear annealing to the min epsilon value until eps_frames and from there slowly decease epsilon to 0 until the end of training\n",
    "        if eps_fixed == False:\n",
    "            if frame < eps_frames:\n",
    "                eps = max(eps_start - (frame*(1/eps_frames)), min_eps)\n",
    "            else:\n",
    "                eps = max(min_eps - min_eps*((frame-eps_frames)/(frames-eps_frames)), 0.001)\n",
    "\n",
    "        # evaluation runs\n",
    "        if frame % 1000 == 0:\n",
    "            print(\"score: {}\".format(score))\n",
    "            #print(\"state: {}\".format(state))\n",
    "            print(\"action:{}, Number:{}\".format(action,frame))\n",
    "            print(\"-------------------------\")\n",
    "        \n",
    "        if done:\n",
    "            scores_window.append(score)       # save most recent score\n",
    "            scores.append(score)              # save most recent score\n",
    "            writer.add_scalar(\"Average100\", np.mean(scores_window), frame)\n",
    "            output_history.append(np.mean(scores_window))\n",
    "            print('\\rEpisode {}\\tFrame {} \\tAverage Score: {:.2f}'.format(i_episode, frame, np.mean(scores_window)), end=\"\")\n",
    "            if i_episode % 100 == 0:\n",
    "                print('\\rEpisode {}\\tFrame {}\\tAverage Score: {:.2f}'.format(i_episode,frame, np.mean(scores_window)))\n",
    "            i_episode +=1 \n",
    "\n",
    "            state = env.reset()\n",
    "            state = state[0,:]\n",
    "            score = 0              \n",
    "\n",
    "    return output_history\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "        # read and preprocess data\n",
    "    preprocessed_path = \"done_3stocks.csv\"\n",
    "    if os.path.exists(preprocessed_path):\n",
    "        data = pd.read_csv(preprocessed_path, index_col=0)\n",
    "\n",
    "    unique_trade_date = data[(data.datadate > 20151001)&(data.datadate <= 20200707)].datadate.unique()\n",
    "    #print(unique_trade_date)\n",
    "\n",
    "    \n",
    "    train = data_split(data, start=20100101, end=20160101)\n",
    "    \n",
    "    env_train = DummyVecEnv([lambda: StockEnvTrain(train)])\n",
    "    \n",
    "    writer = SummaryWriter(\"runs/\"+\"IQN_CP_5\")\n",
    "    seed = 1\n",
    "    BUFFER_SIZE = 10000\n",
    "    BATCH_SIZE = 8\n",
    "    GAMMA = 0.99\n",
    "    TAU = 1e-2\n",
    "    LR = 1e-3\n",
    "    UPDATE_EVERY = 1\n",
    "    n_step = 1\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(\"Using \", device)\n",
    "\n",
    "\n",
    "    action_size     = env_train.action_space.shape[0]\n",
    "\n",
    "    \n",
    "    \n",
    "    print('Action Space: {}'.format(action_size))\n",
    "    state_size = env_train.observation_space.shape[0]\n",
    "    print('State Space: {}'.format(state_size))\n",
    "\n",
    "    agent = DQN_Agent(state_size=19,    #181 #4\n",
    "                        action_size=3, #30 #7\n",
    "                        layer_size=512, #512, #512\n",
    "                        n_step=n_step,\n",
    "                        BATCH_SIZE=BATCH_SIZE, \n",
    "                        BUFFER_SIZE=BUFFER_SIZE, \n",
    "                        LR=LR, \n",
    "                        TAU=TAU, \n",
    "                        GAMMA=GAMMA, \n",
    "                        UPDATE_EVERY=UPDATE_EVERY, \n",
    "                        device=device, \n",
    "                        seed=seed)\n",
    "\n",
    "\n",
    "\n",
    "    # set epsilon frames to 0 so no epsilon exploration\n",
    "    eps_fixed = False\n",
    "    t0 = time.time()\n",
    "    final_average100 = run(env=env_train, frames = 60000, eps_fixed=eps_fixed, eps_frames=5000, min_eps=0.025)\n",
    "    t1 = time.time()\n",
    "    \n",
    "    print(\"Training time: {}min\".format(round((t1-t0)/60,2)))\n",
    "    torch.save(agent.qnetwork_local.state_dict(), \"IQN\"+\".pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
