{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "import random\n",
    "import math\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from collections import deque, namedtuple\n",
    "import time\n",
    "import gym\n",
    "import os\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "def weight_init(layers):\n",
    "    for layer in layers:\n",
    "        torch.nn.init.kaiming_normal_(layer.weight, nonlinearity='relu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3.common.vec_env.base_vec_env import VecEnv, VecEnvStepReturn, VecEnvWrapper\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from stockstats import StockDataFrame as Sdf\n",
    "\n",
    "import datetime\n",
    "\n",
    "import datetime\n",
    "import os\n",
    "\n",
    "TRAINING_DATA_FILE = \"dataprocessing/Yfinance_Data.csv\"\n",
    "\n",
    "now = datetime.datetime.now()\n",
    "TRAINED_MODEL_DIR = f\"trained_models/{now}\"\n",
    "os.makedirs(TRAINED_MODEL_DIR)\n",
    "\n",
    "TESTING_DATA_FILE = \"test.csv\"\n",
    "\n",
    "def load_dataset(*, file_name: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    load csv dataset from path\n",
    "    :return: (df) pandas dataframe\n",
    "    \"\"\"\n",
    "    # _data = pd.read_csv(f\"{config.DATASET_DIR}/{file_name}\")\n",
    "    _data = pd.read_csv(file_name)\n",
    "\n",
    "    return _data\n",
    "\n",
    "\n",
    "def data_split(df, start, end):\n",
    "    \"\"\"\n",
    "    split the dataset into training or testing using date\n",
    "    :param data: (df) pandas dataframe, start, end\n",
    "    :return: (df) pandas dataframe\n",
    "    \"\"\"\n",
    "    data = df[(df.datadate >= start) & (df.datadate < end)]\n",
    "    data = data.sort_values(['datadate', 'tic'], ignore_index=True)\n",
    "\n",
    "\n",
    "    # data  = data[final_columns]\n",
    "    data.index = data.datadate.factorize()[0]\n",
    "\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def calculate_price(df):\n",
    "    \"\"\"\n",
    "    calcualte adjusted close price, open-high-low price and volume\n",
    "    :param data: (df) pandas dataframe\n",
    "    :return: (df) pandas dataframe\n",
    "    \"\"\"\n",
    "    data = df.copy()\n",
    "\n",
    "    data = data[['Date', 'tic', 'Close', 'Open', 'High', 'Low', 'Volume','datadate']]\n",
    "    data = data.sort_values(['tic', 'datadate'], ignore_index=True)\n",
    "    return data\n",
    "\n",
    "\n",
    "def add_technical_indicator(df):\n",
    "    \"\"\"\n",
    "    calcualte technical indicators\n",
    "    use stockstats package to add technical inidactors\n",
    "    :param data: (df) pandas dataframe\n",
    "    :return: (df) pandas dataframe\n",
    "    \"\"\"\n",
    "    stock = Sdf.retype(df.copy())\n",
    "\n",
    "    #print(stock)\n",
    "\n",
    "    unique_ticker = stock.tic.unique()\n",
    "\n",
    "    macd = pd.DataFrame()\n",
    "    rsi = pd.DataFrame()\n",
    "    cci = pd.DataFrame()\n",
    "    dx = pd.DataFrame()\n",
    "\n",
    "    # temp = stock[stock.tic == unique_ticker[0]]['macd']\n",
    "    for i in range(len(unique_ticker)):\n",
    "        ## macd\n",
    "        temp_macd = stock[stock.tic == unique_ticker[i]]['macd']\n",
    "        temp_macd = pd.DataFrame(temp_macd)\n",
    "        macd = macd.append(temp_macd, ignore_index=True)\n",
    "        ## rsi\n",
    "        temp_rsi = stock[stock.tic == unique_ticker[i]]['rsi_30']\n",
    "        temp_rsi = pd.DataFrame(temp_rsi)\n",
    "        rsi = rsi.append(temp_rsi, ignore_index=True)\n",
    "        ## cci\n",
    "        temp_cci = stock[stock.tic == unique_ticker[i]]['cci_30']\n",
    "        temp_cci = pd.DataFrame(temp_cci)\n",
    "        cci = cci.append(temp_cci, ignore_index=True)\n",
    "        ## adx\n",
    "        temp_dx = stock[stock.tic == unique_ticker[i]]['dx_30']\n",
    "        temp_dx = pd.DataFrame(temp_dx)\n",
    "        dx = dx.append(temp_dx, ignore_index=True)\n",
    "\n",
    "    df['macd'] = macd\n",
    "    df['rsi'] = rsi\n",
    "    df['cci'] = cci\n",
    "    df['adx'] = dx\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def preprocess_data():\n",
    "    \"\"\"data preprocessing pipeline\"\"\"\n",
    "    start = datetime.datetime(2010, 12, 1)\n",
    "    df = load_dataset(file_name=TRAINING_DATA_FILE)\n",
    "    # get data after 2010\n",
    "    # df = df[df.Date >= start]\n",
    "    # calcualte adjusted price\n",
    "    df_preprocess = calculate_price(df)\n",
    "    # add technical indicators using stockstats\n",
    "    df_final = add_technical_indicator(df_preprocess)\n",
    "    # fill the missing values at the beginning\n",
    "    df_final.fillna(method='bfill', inplace=True)\n",
    "    return df_final\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IQN(nn.Module):\n",
    "    def __init__(self, state_size, action_size,layer_size, n_step, seed, layer_type=\"ff\"):\n",
    "        super(IQN, self).__init__()\n",
    "        self.seed = torch.manual_seed(seed)\n",
    "        self.input_shape = state_size\n",
    "        self.action_size = action_size\n",
    "        self.K = 32\n",
    "        self.N = 8\n",
    "        self.n_cos = 64\n",
    "        self.layer_size = layer_size\n",
    "        self.pis = torch.FloatTensor([np.pi*i for i in range(self.n_cos)]).view(1,1,self.n_cos).to(device) # Starting from 0 as in the paper \n",
    "\n",
    "        self.head = nn.Linear(self.input_shape, layer_size) # cound be a cnn \n",
    "        self.cos_embedding = nn.Linear(self.n_cos, layer_size)\n",
    "        self.ff_1 = nn.Linear(layer_size, layer_size)\n",
    "        self.ff_2 = nn.Linear(layer_size, action_size)\n",
    "        #weight_init([self.head_1, self.ff_1])\n",
    "\n",
    "\n",
    "        \n",
    "    def calc_cos(self, batch_size, n_tau=8):\n",
    "        \"\"\"\n",
    "        Calculating the cosinus values depending on the number of tau samples\n",
    "        \"\"\"\n",
    "        taus = torch.rand(batch_size, n_tau).to(device).unsqueeze(-1) #(batch_size, n_tau, 1)\n",
    "        cos = torch.cos(taus*self.pis)\n",
    "\n",
    "        assert cos.shape == (batch_size,n_tau,self.n_cos), \"cos shape is incorrect\"\n",
    "        return cos, taus\n",
    "    \n",
    "    def forward(self, input, num_tau=8):\n",
    "        \"\"\"\n",
    "        Quantile Calculation depending on the number of tau\n",
    "        \n",
    "        Return:\n",
    "        quantiles [ shape of (batch_size, num_tau, action_size)]\n",
    "        taus [shape of ((batch_size, num_tau, 1))]\n",
    "        \n",
    "        \"\"\"\n",
    "        batch_size = input.shape[0]\n",
    "        \n",
    "        #print(\"self.head(input):{}\".format(self.head(input).shape))\n",
    "        \n",
    "        x = torch.relu(self.head(input))\n",
    "    \n",
    "        #print(\"batch_size:{}\".format(batch_size))\n",
    "        #print(\"X:{}\".format(x.shape))\n",
    "        \n",
    "        cos, taus = self.calc_cos(batch_size, num_tau) # cos shape (batch, num_tau, layer_size)\n",
    "        \n",
    "        #print(\"cos:{}\".format(cos.shape))\n",
    "        #print(\"taus:{}\".format(taus.shape))\n",
    "        \n",
    "        \n",
    "        cos = cos.view(batch_size*num_tau, self.n_cos)\n",
    "        cos_x = torch.relu(self.cos_embedding(cos)).view(batch_size, num_tau, self.layer_size) # (batch, n_tau, layer)\n",
    "        \n",
    "        # x has shape (batch, layer_size) for multiplication –> reshape to (batch, 1, layer)\n",
    "        #x = (x.unsqueeze(1)*cos_x).view(batch_size*num_tau, self.layer_size)\n",
    "        #print(\"x:{},cos_x Shape:{},batch_size:{},layer_size:{}\".format(x.shape,cos_x.shape,batch_size,self.layer_size))\n",
    "        x = (x.unsqueeze(1) * cos_x).view(batch_size * num_tau, self.layer_size)\n",
    "        #print(\"---------°°°°°°°°°------X----------°°°°°°°°°°-------:{}\".format(x.shape))\n",
    "        \n",
    "        x = torch.relu(self.ff_1(x))\n",
    "        out = self.ff_2(x)\n",
    "        #print(\"---------°°°°°°°°°------out----------°°°°°°°°°°-------:{}\".format(out.shape))\n",
    "        \n",
    "        return out.view(batch_size, num_tau, self.action_size), taus\n",
    "    \n",
    "    def get_action(self, inputs):\n",
    "        quantiles, _ = self.forward(inputs, self.K)\n",
    "        #print(\"quantiles:{}\".format(quantiles.shape))\n",
    "        actions = quantiles.mean(dim=1) #TODO: actions space= torch.Size([1, 32, 30])\n",
    "        #print(\"action space quantile:{}\".format(actions))\n",
    "        return actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "tags": [
     "1prio"
    ]
   },
   "outputs": [],
   "source": [
    "class PrioritizedReplay(object):\n",
    "    \"\"\"\n",
    "    Proportional Prioritization\n",
    "    \"\"\"\n",
    "    def __init__(self, device, capacity, batch_size, seed=1, gamma=0.99, n_step=1, alpha=0.6, beta_start = 0.4, beta_frames=100000, parallel_env=4):\n",
    "        self.alpha = alpha\n",
    "        self.beta_start = beta_start\n",
    "        self.beta_frames = beta_frames\n",
    "        self.device = device\n",
    "        self.frame = 1 #for beta calculation\n",
    "        self.batch_size = batch_size\n",
    "        self.capacity   = capacity\n",
    "        self.buffer     = []\n",
    "        self.pos        = 0\n",
    "        self.experience = namedtuple(\"Experience\", field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
    "        self.priorities = np.zeros((capacity,), dtype=np.float32)\n",
    "        self.seed = np.random.seed(seed)\n",
    "        self.n_step = n_step\n",
    "        self.parallel_env = parallel_env\n",
    "        self.n_step_buffer = deque(maxlen=self.n_step)\n",
    "        self.gamma = gamma\n",
    "        self.memory = deque(maxlen=capacity)  \n",
    "\n",
    "    \n",
    "    def calc_multistep_return(self):\n",
    "        Return = 0\n",
    "        for idx in range(self.n_step):\n",
    "            Return += self.gamma**idx * self.n_step_buffer[idx][2]\n",
    "        \n",
    "        return self.n_step_buffer[0][0], self.n_step_buffer[0][1], Return, self.n_step_buffer[-1][3], self.n_step_buffer[-1][4]\n",
    "        \n",
    "    \n",
    "    def beta_by_frame(self, frame_idx):\n",
    "        \"\"\"\n",
    "        Linearly increases beta from beta_start to 1 over time from 1 to beta_frames.\n",
    "        \n",
    "        3.4 ANNEALING THE BIAS (Paper: PER)\n",
    "        We therefore exploit the flexibility of annealing the amount of importance-sampling\n",
    "        correction over time, by defining a schedule on the exponent \n",
    "        that reaches 1 only at the end of learning. In practice, we linearly anneal from its initial value 0 to 1\n",
    "        \"\"\"\n",
    "        return min(1.0, self.beta_start + frame_idx * (1.0 - self.beta_start) / self.beta_frames)\n",
    "\n",
    "    \n",
    "    \n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Add a new experience to memory.\"\"\"\n",
    "        #print(\"before:\", state,action,reward,next_state, done)\n",
    "        self.n_step_buffer.append((state, action, reward, next_state, done))\n",
    "        if len(self.n_step_buffer) == self.n_step:\n",
    "            state, action, reward, next_state, done = self.calc_multistep_return()\n",
    "            #print(\"after:\",state,action,reward,next_state, done)\n",
    "            e = self.experience(state, action, reward, next_state, done)\n",
    "            #print(self.memory)\n",
    "            self.memory.append(e)\n",
    "            \n",
    "        max_prio = self.priorities.max() if self.buffer else 1.0 # gives max priority if buffer is not empty else 1\n",
    "\n",
    "        if len(self.buffer) < self.capacity:\n",
    "            self.buffer.append((state, action, reward, next_state, done))\n",
    "        else:\n",
    "            # puts the new data on the position of the oldes since it circles via pos variable\n",
    "            # since if len(buffer) == capacity -> pos == 0 -> oldest memory (at least for the first round?) \n",
    "            self.buffer[self.pos] = (state, action, reward, next_state, done) \n",
    "        \n",
    "        self.priorities[self.pos] = max_prio\n",
    "        self.pos = (self.pos + 1) % self.capacity # lets the pos circle in the ranges of capacity if pos+1 > cap --> new posi = 0\n",
    "\n",
    "        \n",
    "    def sample(self):\n",
    "        N = len(self.buffer)\n",
    "        if N == self.capacity:\n",
    "            prios = self.priorities\n",
    "        else:\n",
    "            prios = self.priorities[:self.pos]\n",
    "            \n",
    "        # calc P = p^a/sum(p^a)\n",
    "        probs  = prios ** self.alpha\n",
    "        P = probs/probs.sum()\n",
    "        \n",
    "        #gets the indices depending on the probability p\n",
    "        indices = np.random.choice(N, 1, p=P) \n",
    "        \n",
    "        \n",
    "        #print('indices:{}'.format(indices))\n",
    "        samples = [self.buffer[idx] for idx in indices]\n",
    "        #print('Samples Shape: '.format(len(samples)))\n",
    "        \n",
    "        beta = self.beta_by_frame(self.frame)\n",
    "        self.frame+=1\n",
    "                \n",
    "        #Compute importance-sampling weight\n",
    "        weights  = (N * P[indices])** (-beta)\n",
    "        # normalize weights\n",
    "        weights /= weights.max() \n",
    "        weights  = np.array(weights, dtype=np.float32) \n",
    "        \n",
    "        #print('indices:{}'.format(indices))\n",
    "        \n",
    "        CurrentSequence = indices[0]\n",
    "        \n",
    "        if CurrentSequence < 8:\n",
    "            if len(self.memory) < 16:\n",
    "                indices = np.random.choice(N, 8, p=P)\n",
    "                #print('second indices:{}'.format(indices))\n",
    "                SequenceOfSampling=indices\n",
    "            else: \n",
    "                SequenceOfSampling = [CurrentSequence, CurrentSequence+1,CurrentSequence+2,CurrentSequence+3,CurrentSequence+4,CurrentSequence+5,CurrentSequence+6,CurrentSequence+7]\n",
    "        else:\n",
    "            SequenceOfSampling = [CurrentSequence-7,CurrentSequence-6,CurrentSequence-5,CurrentSequence-4,CurrentSequence-3,CurrentSequence-2,CurrentSequence-1,CurrentSequence]\n",
    "        \n",
    "        #print(SequenceOfSampling)\n",
    "        #print(len(self.memory))\n",
    "        experiences = [self.memory[SequenceOfSampling[0]],self.memory[SequenceOfSampling[1]],self.memory[SequenceOfSampling[2]],self.memory[SequenceOfSampling[3]],self.memory[SequenceOfSampling[4]],self.memory[SequenceOfSampling[5]],self.memory[SequenceOfSampling[6]],self.memory[SequenceOfSampling[7]]]\n",
    "        #print(experiences)\n",
    "        \n",
    "        #print(self.buffer[SequenceOfSampling[0]])\n",
    "        \n",
    "        \n",
    "        states = torch.from_numpy(np.stack([e.state for e in experiences if e is not None])).float().to(self.device)\n",
    "        actions = torch.from_numpy(np.vstack([e.action for e in experiences if e is not None])).long().to(self.device)\n",
    "        rewards = torch.from_numpy(np.vstack([e.reward for e in experiences if e is not None])).float().to(self.device)\n",
    "        next_states = torch.from_numpy(np.stack([e.next_state for e in experiences if e is not None])).float().to(self.device)\n",
    "        dones = torch.from_numpy(np.vstack([e.done for e in experiences if e is not None]).astype(np.uint8)).float().to(self.device)\n",
    "        weights = torch.from_numpy(np.vstack(weights)).float().to(self.device)\n",
    "           \n",
    "        \n",
    "        return states, actions, rewards, next_states, dones, indices, weights\n",
    "    \n",
    "    def update_priorities(self, batch_indices, batch_priorities):\n",
    "        for idx, prio in zip(batch_indices, batch_priorities):\n",
    "            self.priorities[idx] = prio \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class DQN_Agent():\n",
    "    \"\"\"Interacts with and learns from the environment.\"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 state_size,\n",
    "                 action_size,\n",
    "                 layer_size,\n",
    "                 n_step,\n",
    "                 BATCH_SIZE,\n",
    "                 BUFFER_SIZE,\n",
    "                 LR,\n",
    "                 TAU,\n",
    "                 GAMMA,\n",
    "                 UPDATE_EVERY,\n",
    "                 device,\n",
    "                 seed):\n",
    "        \"\"\"Initialize an Agent object.\n",
    "        \n",
    "        Params\n",
    "        ======\n",
    "            state_size (int): dimension of each state\n",
    "            action_size (int): dimension of each action\n",
    "            layer_size (int): size of the hidden layer\n",
    "            BATCH_SIZE (int): size of the training batch\n",
    "            BUFFER_SIZE (int): size of the replay memory\n",
    "            LR (float): learning rate\n",
    "            TAU (float): tau for soft updating the network weights\n",
    "            GAMMA (float): discount factor\n",
    "            UPDATE_EVERY (int): update frequency\n",
    "            device (str): device that is used for the compute\n",
    "            seed (int): random seed\n",
    "        \"\"\"\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.seed = random.seed(seed)\n",
    "        self.device = device\n",
    "        self.TAU = TAU\n",
    "        self.GAMMA = GAMMA\n",
    "        self.UPDATE_EVERY = UPDATE_EVERY\n",
    "        self.BATCH_SIZE = BATCH_SIZE\n",
    "        self.Q_updates = 0\n",
    "        self.n_step = n_step\n",
    "        self.action = []\n",
    "\n",
    "        self.action_step = 30\n",
    "\n",
    "        # IQN-Network\n",
    "        self.qnetwork_local = IQN(state_size, action_size,layer_size, n_step, seed).to(device)\n",
    "        self.qnetwork_target = IQN(state_size, action_size,layer_size, n_step, seed).to(device)\n",
    "\n",
    "        self.optimizer = optim.Adam(self.qnetwork_local.parameters(), lr=LR)\n",
    "        #print(self.qnetwork_local)\n",
    "        \n",
    "        # Replay memory\n",
    "        self.memory = PrioritizedReplay(device, BUFFER_SIZE, self.BATCH_SIZE, gamma=self.GAMMA, n_step=n_step, parallel_env=1)\n",
    "        # Initialize time step (for updating every UPDATE_EVERY steps)\n",
    "        self.t_step = 0\n",
    "    \n",
    "    def step(self, state, action, reward, next_state, done, writer):\n",
    "        # Save experience in replay memory\n",
    "        #print(\"to memory action:{},state:{},next_state\".format(action,state.shape,next_state.shape))\n",
    "        self.memory.add(state, action, reward, next_state, done)\n",
    "\n",
    "        # Learn every UPDATE_EVERY time steps.\n",
    "        self.t_step = (self.t_step + 1) % self.UPDATE_EVERY\n",
    "        if self.t_step == 0:\n",
    "            # If enough samples are available in memory, get random subset and learn\n",
    "            if len(self.memory) > self.BATCH_SIZE:\n",
    "                experiences = self.memory.sample()\n",
    "                #print(\"experiences:{}\".format(experiences))\n",
    "\n",
    "                loss = self.learn(experiences)\n",
    "                self.Q_updates += 1\n",
    "                writer.add_scalar(\"Q_loss\", loss, self.Q_updates)\n",
    "\n",
    "    def act(self, state, eps=0.):\n",
    "        \"\"\"Returns actions for given state as per current policy. Acting only every 4 frames!\n",
    "        \n",
    "        Params\n",
    "        ======\n",
    "            frame: to adjust epsilon\n",
    "            state (array_like): current state\n",
    "            \n",
    "        \"\"\"\n",
    "        #print(\"without np.array:{}\".format(state.shape))\n",
    "\n",
    "        state = np.array(state)\n",
    "\n",
    "        #print(\"this is the state space before torch.from_numpy:{}\".format(state.shape))\n",
    "        \n",
    "        state = torch.from_numpy(state).float().unsqueeze(0).to(self.device) #WHY?\n",
    "        \n",
    "        #print(\"this is the state space after torch.from_numpy:{}\".format(state.shape))\n",
    "        \n",
    "        \n",
    "        self.qnetwork_local.eval() #WHY?\n",
    "        \n",
    "        \n",
    "        with torch.no_grad():\n",
    "            #print(\"this is the state space:{}\".format(state.shape))\n",
    "            action_values = self.qnetwork_local.get_action(state) # 30 dimensions are coming back.\n",
    "            #print('action_value:{}'.format(action_values))\n",
    "        self.qnetwork_local.train()\n",
    "\n",
    "        # Epsilon-greedy action selection\n",
    "        if random.random() > eps: # select greedy action if random number is higher than epsilon or noisy network is used!\n",
    "            action = np.argmax(action_values.cpu().data.numpy())\n",
    "            self.last_action = action\n",
    "            return action\n",
    "        else:\n",
    "            action = random.choice(np.arange(self.action_size))\n",
    "            self.last_action = action \n",
    "            return action\n",
    "\n",
    "\n",
    "\n",
    "    def learn(self, experiences):\n",
    "        \"\"\"Update value parameters using given batch of experience tuples.\n",
    "        Params\n",
    "        ======\n",
    "            experiences (Tuple[torch.Tensor]): tuple of (s, a, r, s', done) tuples \n",
    "            gamma (float): discount factor\n",
    "        \"\"\"\n",
    "        self.optimizer.zero_grad()\n",
    "        states, actions, rewards, next_states, dones, indices, weights = experiences\n",
    "        #print(\"learning states:{}, next_states:{}\".format(states.shape, next_states.shape))\n",
    "        # Get max predicted Q values (for next states) from target model\n",
    "        Q_targets_next, _ = self.qnetwork_target(next_states)\n",
    "        \n",
    "        #print(\"--Q_targets_next :{}\".format(Q_targets_next.shape))\n",
    "        #print(\"---->Q_targets_next:{}\".format(Q_targets_next))\n",
    "        #print('------------------------------------')\n",
    "        #print(\"--Q_targets_next detach max:{}\".format(Q_targets_next.detach().max(2)))\n",
    "        \n",
    "        #print(\"--Q_targets_next.detach().max(2)[0].unsqueeze(1):{}\".format(Q_targets_next.detach().max(2)[0].unsqueeze(1)))\n",
    "        Q_targets_next = Q_targets_next.detach().max(2)[0].unsqueeze(1) # (batch_size, 1, N)\n",
    "        \n",
    "        # Compute Q targets for current states \n",
    "        Q_targets = rewards.unsqueeze(-1) + (self.GAMMA**self.n_step * Q_targets_next * (1. - dones.unsqueeze(-1)))\n",
    "        # Get expected Q values from local model\n",
    "        Q_expected, taus = self.qnetwork_local(states)\n",
    "        \n",
    "        #print(\"rewards:{}\".format(rewards.shape))\n",
    "        #print(\"Q_targets_Shape:{}\".format(Q_targets.shape))\n",
    "        #print(\"actions shape:{}\".format(actions.shape))\n",
    "        #print(\"Q_expected shape:{}\".format(Q_expected.shape))\n",
    "        #print(\"actions.unsqueeze(-1).shape:{}\".format(actions.unsqueeze(-1).shape))\n",
    "        #print(\"actions:{}\".format(actions))\n",
    "        #print(\"Q_expected:{}\".format(Q_expected))\n",
    "        #print(\"actions.unsqueeze(-1){}\".format(actions.unsqueeze(-1)))\n",
    "        Q_expected_2 = Q_expected.gather(2, actions.unsqueeze(-1))\n",
    "\n",
    "        #print(\"Q_expected.gather(2, actions.unsqueeze(-1):{}\".format(Q_expected_2.shape))\n",
    "        \n",
    "        Q_expected = Q_expected.gather(2, actions[0].unsqueeze(-1).expand(self.BATCH_SIZE, 8, 1))\n",
    "        #print(\"Final what we need Q_expected-----:{}\".format(Q_expected.shape))\n",
    "\n",
    "        # Quantile Huber loss\n",
    "        td_error = Q_targets - Q_expected\n",
    "        #print(\"td_error.shape:{}\".format(td_error.shape))\n",
    "        #print(\"Q_expected.shape:{}\".format(Q_expected.shape))\n",
    "        #print(\"td_error:{}\".format(td_error.shape))\n",
    "        assert td_error.shape == (self.BATCH_SIZE, 8, 8), \"wrong td error shape\"\n",
    "        huber_l = calculate_huber_loss(td_error, 1.0)\n",
    "        quantil_l = abs(taus -(td_error.detach() < 0).float()) * huber_l / 1.0\n",
    "        \n",
    "        loss = quantil_l.sum(dim=1).mean(dim=1) # , keepdim=True if per weights get multipl\n",
    "        loss = loss.mean()\n",
    "\n",
    "\n",
    "        # Minimize the loss\n",
    "        loss.backward()\n",
    "        #clip_grad_norm_(self.qnetwork_local.parameters(),1)\n",
    "        self.optimizer.step()\n",
    "\n",
    "        # ------------------- update target network ------------------- #\n",
    "        return loss.detach().cpu().numpy()            \n",
    "\n",
    "    def soft_update(self, local_model, target_model):\n",
    "        \"\"\"Soft update model parameters.\n",
    "        θ_target = τ*θ_local + (1 - τ)*θ_target\n",
    "        Params\n",
    "        ======\n",
    "            local_model (PyTorch model): weights will be copied from\n",
    "            target_model (PyTorch model): weights will be copied to\n",
    "            tau (float): interpolation parameter \n",
    "        \"\"\"\n",
    "        for target_param, local_param in zip(target_model.parameters(), local_model.parameters()):\n",
    "            target_param.data.copy_(self.TAU*local_param.data + (1.0-self.TAU)*target_param.data)\n",
    "\n",
    "\n",
    "def calculate_huber_loss(td_errors, k=1.0):\n",
    "    \"\"\"\n",
    "    Calculate huber loss element-wisely depending on kappa k.\n",
    "    \"\"\"\n",
    "    loss = torch.where(td_errors.abs() <= k, 0.5 * td_errors.pow(2), k * (td_errors.abs() - 0.5 * k))\n",
    "    #print('this is huber loss: {}'.format(loss.shape))\n",
    "    assert loss.shape == (td_errors.shape[0], 8, 8), \"huber loss has wrong shape\"\n",
    "    return loss\n",
    "    \n",
    "def eval_runs(eps, frame):\n",
    "    \"\"\"\n",
    "    Makes an evaluation run with the current epsilon\n",
    "    \"\"\"\n",
    "    print(\"-----------------------------------------evaluating-----------------------------------------\")\n",
    "    env = gym.make(\"Acrobot-v1\") # TODO:\n",
    "    reward_batch = []\n",
    "    for i in range(5):\n",
    "        state = env.reset()\n",
    "        rewards = 0\n",
    "        while True:\n",
    "            action = agent.act(state, eps)\n",
    "            state, reward, done, _ = env.step(action)\n",
    "            rewards += reward\n",
    "            if done:\n",
    "                break\n",
    "        reward_batch.append(rewards)\n",
    "        \n",
    "    writer.add_scalar(\"Reward\", np.mean(reward_batch), frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using  cpu\n",
      "Action Space: 3\n",
      "State Space: 19\n",
      "[1.00000000e+03 3.05728571e+01 4.09200000e+01 5.61800000e+01\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 5.60928564e-01\n",
      " 3.58722215e-01 6.24669814e-01 6.21337044e+01 5.67090572e+01\n",
      " 5.85111632e+01 1.68824972e+02 2.09000651e+00 8.31548524e+01\n",
      " 3.37911464e+01 1.15370605e+01 1.08349581e+01]\n",
      "Finished\n",
      "[350.9819045556563, 107.32, 70.16, 146.41, 2.0, 12.0, 4.0, -2.466788775, -0.5672052238, -0.1475155644, 42.30524752, 44.89929352, 52.58918069, -98.67248174, -15.74083296, 29.12710167, 29.43194077, 7.763541354, 9.869031232]\n",
      "end_total_asset:1993.181904555656\n",
      "Sharpe:  0.4536244078048076\n",
      "[1.00000000e+03 3.05728571e+01 4.09200000e+01 5.61800000e+01\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 5.60928564e-01\n",
      " 3.58722215e-01 6.24669814e-01 6.21337044e+01 5.67090572e+01\n",
      " 5.85111632e+01 1.68824972e+02 2.09000651e+00 8.31548524e+01\n",
      " 3.37911464e+01 1.15370605e+01 1.08349581e+01]\n",
      "Episode 1\tFrame 4528 \tAverage Score: 0.10[1.00000000e+03 3.05728571e+01 4.09200000e+01 5.61800000e+01\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 5.60928564e-01\n",
      " 3.58722215e-01 6.24669814e-01 6.21337044e+01 5.67090572e+01\n",
      " 5.85111632e+01 1.68824972e+02 2.09000651e+00 8.31548524e+01\n",
      " 3.37911464e+01 1.15370605e+01 1.08349581e+01]\n",
      "Finished\n",
      "[2212.023767352793, 107.32, 70.16, 146.41, 0.0, 0.0, 1.0, -2.466788775, -0.5672052238, -0.1475155644, 42.30524752, 44.89929352, 52.58918069, -98.67248174, -15.74083296, 29.12710167, 29.43194077, 7.763541354, 9.869031232]\n",
      "end_total_asset:2358.433767352793\n",
      "Sharpe:  0.6738542986507304\n",
      "[1.00000000e+03 3.05728571e+01 4.09200000e+01 5.61800000e+01\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 5.60928564e-01\n",
      " 3.58722215e-01 6.24669814e-01 6.21337044e+01 5.67090572e+01\n",
      " 5.85111632e+01 1.68824972e+02 2.09000651e+00 8.31548524e+01\n",
      " 3.37911464e+01 1.15370605e+01 1.08349581e+01]\n",
      "Episode 2\tFrame 9056 \tAverage Score: 0.12[1.00000000e+03 3.05728571e+01 4.09200000e+01 5.61800000e+01\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 5.60928564e-01\n",
      " 3.58722215e-01 6.24669814e-01 6.21337044e+01 5.67090572e+01\n",
      " 5.85111632e+01 1.68824972e+02 2.09000651e+00 8.31548524e+01\n",
      " 3.37911464e+01 1.15370605e+01 1.08349581e+01]\n",
      "Finished\n",
      "[44.24985241859234, 107.32, 70.16, 146.41, 7.0, 15.0, 1.0, -2.466788775, -0.5672052238, -0.1475155644, 42.30524752, 44.89929352, 52.58918069, -98.67248174, -15.74083296, 29.12710167, 29.43194077, 7.763541354, 9.869031232]\n",
      "end_total_asset:1994.2998524185923\n",
      "Sharpe:  0.43280597640078994\n",
      "[1.00000000e+03 3.05728571e+01 4.09200000e+01 5.61800000e+01\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 5.60928564e-01\n",
      " 3.58722215e-01 6.24669814e-01 6.21337044e+01 5.67090572e+01\n",
      " 5.85111632e+01 1.68824972e+02 2.09000651e+00 8.31548524e+01\n",
      " 3.37911464e+01 1.15370605e+01 1.08349581e+01]\n",
      "Episode 3\tFrame 13584 \tAverage Score: 0.11[1.00000000e+03 3.05728571e+01 4.09200000e+01 5.61800000e+01\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 5.60928564e-01\n",
      " 3.58722215e-01 6.24669814e-01 6.21337044e+01 5.67090572e+01\n",
      " 5.85111632e+01 1.68824972e+02 2.09000651e+00 8.31548524e+01\n",
      " 3.37911464e+01 1.15370605e+01 1.08349581e+01]\n",
      "Finished\n",
      "[78.95383853857444, 107.32, 70.16, 146.41, 7.0, 14.0, 1.0, -2.466788775, -0.5672052238, -0.1475155644, 42.30524752, 44.89929352, 52.58918069, -98.67248174, -15.74083296, 29.12710167, 29.43194077, 7.763541354, 9.869031232]\n",
      "end_total_asset:1958.8438385385746\n",
      "Sharpe:  0.4309320239930501\n",
      "[1.00000000e+03 3.05728571e+01 4.09200000e+01 5.61800000e+01\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 5.60928564e-01\n",
      " 3.58722215e-01 6.24669814e-01 6.21337044e+01 5.67090572e+01\n",
      " 5.85111632e+01 1.68824972e+02 2.09000651e+00 8.31548524e+01\n",
      " 3.37911464e+01 1.15370605e+01 1.08349581e+01]\n",
      "Episode 4\tFrame 18112 \tAverage Score: 0.11[1.00000000e+03 3.05728571e+01 4.09200000e+01 5.61800000e+01\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 5.60928564e-01\n",
      " 3.58722215e-01 6.24669814e-01 6.21337044e+01 5.67090572e+01\n",
      " 5.85111632e+01 1.68824972e+02 2.09000651e+00 8.31548524e+01\n",
      " 3.37911464e+01 1.15370605e+01 1.08349581e+01]\n",
      "Finished\n",
      "[10.460136175712194, 107.32, 70.16, 146.41, 2.0, 15.0, 7.0, -2.466788775, -0.5672052238, -0.1475155644, 42.30524752, 44.89929352, 52.58918069, -98.67248174, -15.74083296, 29.12710167, 29.43194077, 7.763541354, 9.869031232]\n",
      "end_total_asset:2302.370136175712\n",
      "Sharpe:  0.4978587023682394\n",
      "[1.00000000e+03 3.05728571e+01 4.09200000e+01 5.61800000e+01\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 5.60928564e-01\n",
      " 3.58722215e-01 6.24669814e-01 6.21337044e+01 5.67090572e+01\n",
      " 5.85111632e+01 1.68824972e+02 2.09000651e+00 8.31548524e+01\n",
      " 3.37911464e+01 1.15370605e+01 1.08349581e+01]\n",
      "Episode 5\tFrame 22640 \tAverage Score: 0.11[1.00000000e+03 3.05728571e+01 4.09200000e+01 5.61800000e+01\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 5.60928564e-01\n",
      " 3.58722215e-01 6.24669814e-01 6.21337044e+01 5.67090572e+01\n",
      " 5.85111632e+01 1.68824972e+02 2.09000651e+00 8.31548524e+01\n",
      " 3.37911464e+01 1.15370605e+01 1.08349581e+01]\n",
      "Finished\n",
      "[1.764820034303142, 107.32, 70.16, 146.41, 6.0, 7.0, 8.0, -2.466788775, -0.5672052238, -0.1475155644, 42.30524752, 44.89929352, 52.58918069, -98.67248174, -15.74083296, 29.12710167, 29.43194077, 7.763541354, 9.869031232]\n",
      "end_total_asset:2308.084820034303\n",
      "Sharpe:  0.5460672009388615\n",
      "[1.00000000e+03 3.05728571e+01 4.09200000e+01 5.61800000e+01\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 5.60928564e-01\n",
      " 3.58722215e-01 6.24669814e-01 6.21337044e+01 5.67090572e+01\n",
      " 5.85111632e+01 1.68824972e+02 2.09000651e+00 8.31548524e+01\n",
      " 3.37911464e+01 1.15370605e+01 1.08349581e+01]\n",
      "Episode 6\tFrame 27168 \tAverage Score: 0.12[1.00000000e+03 3.05728571e+01 4.09200000e+01 5.61800000e+01\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 5.60928564e-01\n",
      " 3.58722215e-01 6.24669814e-01 6.21337044e+01 5.67090572e+01\n",
      " 5.85111632e+01 1.68824972e+02 2.09000651e+00 8.31548524e+01\n",
      " 3.37911464e+01 1.15370605e+01 1.08349581e+01]\n",
      "Finished\n",
      "[40.11229789432474, 107.32, 70.16, 146.41, 13.0, 3.0, 1.0, -2.466788775, -0.5672052238, -0.1475155644, 42.30524752, 44.89929352, 52.58918069, -98.67248174, -15.74083296, 29.12710167, 29.43194077, 7.763541354, 9.869031232]\n",
      "end_total_asset:1792.1622978943246\n",
      "Sharpe:  0.3611786565226432\n",
      "[1.00000000e+03 3.05728571e+01 4.09200000e+01 5.61800000e+01\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 5.60928564e-01\n",
      " 3.58722215e-01 6.24669814e-01 6.21337044e+01 5.67090572e+01\n",
      " 5.85111632e+01 1.68824972e+02 2.09000651e+00 8.31548524e+01\n",
      " 3.37911464e+01 1.15370605e+01 1.08349581e+01]\n",
      "Episode 7\tFrame 31696 \tAverage Score: 0.11[1.00000000e+03 3.05728571e+01 4.09200000e+01 5.61800000e+01\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 5.60928564e-01\n",
      " 3.58722215e-01 6.24669814e-01 6.21337044e+01 5.67090572e+01\n",
      " 5.85111632e+01 1.68824972e+02 2.09000651e+00 8.31548524e+01\n",
      " 3.37911464e+01 1.15370605e+01 1.08349581e+01]\n",
      "Finished\n",
      "[2064.122496567175, 107.32, 70.16, 146.41, 0.0, 0.0, 1.0, -2.466788775, -0.5672052238, -0.1475155644, 42.30524752, 44.89929352, 52.58918069, -98.67248174, -15.74083296, 29.12710167, 29.43194077, 7.763541354, 9.869031232]\n",
      "end_total_asset:2210.532496567175\n",
      "Sharpe:  0.47745107823545013\n",
      "[1.00000000e+03 3.05728571e+01 4.09200000e+01 5.61800000e+01\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 5.60928564e-01\n",
      " 3.58722215e-01 6.24669814e-01 6.21337044e+01 5.67090572e+01\n",
      " 5.85111632e+01 1.68824972e+02 2.09000651e+00 8.31548524e+01\n",
      " 3.37911464e+01 1.15370605e+01 1.08349581e+01]\n",
      "Episode 8\tFrame 36224 \tAverage Score: 0.11[1.00000000e+03 3.05728571e+01 4.09200000e+01 5.61800000e+01\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 5.60928564e-01\n",
      " 3.58722215e-01 6.24669814e-01 6.21337044e+01 5.67090572e+01\n",
      " 5.85111632e+01 1.68824972e+02 2.09000651e+00 8.31548524e+01\n",
      " 3.37911464e+01 1.15370605e+01 1.08349581e+01]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [159]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    354\u001b[0m eps_fixed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    355\u001b[0m t0 \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m--> 356\u001b[0m final_average100 \u001b[38;5;241m=\u001b[39m \u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menv_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframes\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m600000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meps_fixed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meps_fixed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meps_frames\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmin_eps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.025\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    357\u001b[0m t1 \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m    359\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining time: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124mmin\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mround\u001b[39m((t1\u001b[38;5;241m-\u001b[39mt0)\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m60\u001b[39m,\u001b[38;5;241m2\u001b[39m)))\n",
      "Input \u001b[0;32mIn [159]\u001b[0m, in \u001b[0;36mrun\u001b[0;34m(env, frames, eps_fixed, eps_frames, min_eps)\u001b[0m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m#print(\"env_trainNext State: {}\".format(next_state.shape))\u001b[39;00m\n\u001b[1;32m    265\u001b[0m next_state \u001b[38;5;241m=\u001b[39m next_state[\u001b[38;5;241m0\u001b[39m,:]\n\u001b[0;32m--> 266\u001b[0m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreward\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnext_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdone\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwriter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    268\u001b[0m state \u001b[38;5;241m=\u001b[39m next_state\n\u001b[1;32m    269\u001b[0m score \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m reward\n",
      "Input \u001b[0;32mIn [158]\u001b[0m, in \u001b[0;36mDQN_Agent.step\u001b[0;34m(self, state, action, reward, next_state, done, writer)\u001b[0m\n\u001b[1;32m     69\u001b[0m experiences \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmemory\u001b[38;5;241m.\u001b[39msample()\n\u001b[1;32m     70\u001b[0m \u001b[38;5;66;03m#print(\"experiences:{}\".format(experiences))\u001b[39;00m\n\u001b[0;32m---> 72\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexperiences\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mQ_updates \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     74\u001b[0m writer\u001b[38;5;241m.\u001b[39madd_scalar(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mQ_loss\u001b[39m\u001b[38;5;124m\"\u001b[39m, loss, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mQ_updates)\n",
      "Input \u001b[0;32mIn [158]\u001b[0m, in \u001b[0;36mDQN_Agent.learn\u001b[0;34m(self, experiences)\u001b[0m\n\u001b[1;32m    168\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mmean()\n\u001b[1;32m    171\u001b[0m \u001b[38;5;66;03m# Minimize the loss\u001b[39;00m\n\u001b[0;32m--> 172\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    173\u001b[0m \u001b[38;5;66;03m#clip_grad_norm_(self.qnetwork_local.parameters(),1)\u001b[39;00m\n\u001b[1;32m    174\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/_tensor.py:363\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    354\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    355\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    356\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    357\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    361\u001b[0m         create_graph\u001b[38;5;241m=\u001b[39mcreate_graph,\n\u001b[1;32m    362\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs)\n\u001b[0;32m--> 363\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/autograd/__init__.py:173\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    168\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    170\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 173\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    174\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from IPython import display\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from gym.utils import seeding\n",
    "import gym\n",
    "import os\n",
    "from gym import spaces\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "\n",
    "# shares normalization factor\n",
    "# 100 shares per trade\n",
    "HMAX_NORMALIZE = 100\n",
    "# initial amount of money we have in our account\n",
    "INITIAL_ACCOUNT_BALANCE= 1000\n",
    "# total number of stocks in our portfolio\n",
    "STOCK_DIM = 3\n",
    "# transaction fee: 1/1000 reasonable percentage\n",
    "TRANSACTION_FEE_PERCENT = 0.001\n",
    "REWARD_SCALING = 1e-4\n",
    "\n",
    "class StockEnvTrain(gym.Env):\n",
    "    \"\"\"A stock trading environment for OpenAI gym\"\"\"\n",
    "    metadata = {'render.modes': ['human']}\n",
    "\n",
    "    def __init__(self, df,day = 0):\n",
    "        #super(StockEnv, self).__init__()\n",
    "        #money = 10 , scope = 1\n",
    "        self.day = day\n",
    "        self.df = df\n",
    "        self.agent_stock_iteration_index = 0\n",
    "\n",
    "        # action_space normalization and shape is STOCK_DIM\n",
    "        self.action_space = spaces.Box(low = -1, high = 1,shape = (STOCK_DIM,)) \n",
    "        # Shape = 181: [Current Balance]+[prices 1-30]+[owned shares 1-30] \n",
    "        # +[macd 1-30]+ [rsi 1-30] + [cci 1-30] + [adx 1-30]\n",
    "        self.observation_space = spaces.Box(low=0, high=np.inf, shape = (19,))\n",
    "        # load data from a pandas dataframe\n",
    "        #print('df: {}'.format(self.df))\n",
    "        #print('day: {}'.format(self.day))\n",
    "        self.data = self.df.loc[self.day,:]\n",
    "        #print(self.data.Close)\n",
    "        self.terminal = False\n",
    "        # initalize state\n",
    "        self.state = [INITIAL_ACCOUNT_BALANCE] + \\\n",
    "                      self.data.Close.values.tolist() + \\\n",
    "                      [0]*STOCK_DIM + \\\n",
    "                      self.data.macd.values.tolist() + \\\n",
    "                      self.data.rsi.values.tolist() + \\\n",
    "                      self.data.cci.values.tolist() + \\\n",
    "                      self.data.adx.values.tolist()\n",
    "        # initialize reward\n",
    "        self.reward = 0\n",
    "        self.cost = 0\n",
    "        # memorize all the total balance change\n",
    "        self.asset_memory = [INITIAL_ACCOUNT_BALANCE]\n",
    "        self.rewards_memory = []\n",
    "        self.final_asset_value = 0\n",
    "        self.trades = 0\n",
    "        #self.reset()\n",
    "        self._seed()\n",
    "\n",
    "\n",
    "    def _sell_stock(self, index, action):\n",
    "        # perform sell action based on the sign of the action\n",
    "        if self.state[index+STOCK_DIM+1] > 0:\n",
    "            #update balance\n",
    "            self.state[0] += \\\n",
    "            self.state[index+1]*min(abs(action),self.state[index+STOCK_DIM+1]) * \\\n",
    "             (1- TRANSACTION_FEE_PERCENT)\n",
    "            self.state[index+STOCK_DIM+1] -= min(abs(action), self.state[index+STOCK_DIM+1])\n",
    "            self.cost +=self.state[index+1]*min(abs(action),self.state[index+STOCK_DIM+1]) * \\\n",
    "             TRANSACTION_FEE_PERCENT\n",
    "            self.trades+=1\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "    \n",
    "    def _buy_stock(self, index, action):\n",
    "        # perform buy action based on the sign of the action\n",
    "        available_amount = self.state[0] // self.state[index+1]\n",
    "        # print('available_amount:{}'.format(available_amount))\n",
    "\n",
    "        #update balance\n",
    "        self.state[0] -= self.state[index+1]*min(available_amount, action)* \\\n",
    "                          (1+ TRANSACTION_FEE_PERCENT)\n",
    "        self.state[index+STOCK_DIM+1] += min(available_amount, action)\n",
    "        self.cost+=self.state[index+1]*min(available_amount, action)* \\\n",
    "                          TRANSACTION_FEE_PERCENT\n",
    "        self.trades+=1\n",
    "        \n",
    "        \n",
    "    def step(self, actions):\n",
    "        # print(self.day)\n",
    "        self.terminal = self.day >= len(self.df.index.unique())-1\n",
    "        #print(actions)\n",
    "        self.actions = actions\n",
    "        if self.terminal:\n",
    "            print(\"Finished\")\n",
    "            print(self.state)\n",
    "            end_total_asset = self.state[0]+ \\\n",
    "            sum(np.array(self.state[1:(STOCK_DIM+1)])*np.array(self.state[(STOCK_DIM+1):(STOCK_DIM*2+1)]))\n",
    "\n",
    "            print(\"end_total_asset:{}\".format(end_total_asset))\n",
    "            df_total_value = pd.DataFrame(self.asset_memory)\n",
    "            #df_total_value.to_csv('results/account_value_train.csv')\n",
    "            #print(\"total_reward:{}\".format(self.state[0]+sum(np.array(self.state[1:(STOCK_DIM+1)])*np.array(self.state[(STOCK_DIM+1):61]))- INITIAL_ACCOUNT_BALANCE ))\n",
    "            #print(\"total_cost: \", self.cost)\n",
    "            #print(\"total_trades: \", self.trades)\n",
    "            df_total_value.columns = ['account_value']\n",
    "            df_total_value['daily_return']=df_total_value.pct_change(1)\n",
    "            sharpe = (252**0.5)*df_total_value['daily_return'].mean()/ \\\n",
    "                  df_total_value['daily_return'].std()\n",
    "            print(\"Sharpe: \",sharpe)\n",
    "            #print(\"=================================\")\n",
    "            df_rewards = pd.DataFrame(self.rewards_memory)\n",
    "            #df_rewards.to_csv('results/account_rewards_train.csv')\n",
    "\n",
    "            # print('total asset: {}'.format(self.state[0]+ sum(np.array(self.state[1:29])*np.array(self.state[29:]))))\n",
    "            #with open('obs.pkl', 'wb') as f:  \n",
    "            #    pickle.dump(self.state, \n",
    "            return self.state, self.reward, self.terminal,{}\n",
    "\n",
    "        else:\n",
    "            # print(np.array(self.state[1:29]))\n",
    "            #print(\"The actions is: {}\".format(self.actions))\n",
    "\n",
    "            #action = np.array([4,4,5])\n",
    "            #actions = np.array([4,0,0,0,0,0,0,0,4,0,4,0,-3,0,0,0,0,0,0,0,4,0,0,0,0,0,0,0,0,0])\n",
    "\n",
    "            #actions = self.actions * HMAX_NORMALIZE #WHY??\n",
    "            #print(\"actions-index------:{}\".format(actions))\n",
    "            #actions = (actions.astype(int))\n",
    "\n",
    "            begin_total_asset = self.state[0]+ \\\n",
    "            sum(np.array(self.state[1:(STOCK_DIM+1)])*np.array(self.state[(STOCK_DIM+1):(STOCK_DIM*2+1)]))\n",
    "            #print(\"begin_total_asset:{}\".format(begin_total_asset))\n",
    "\n",
    "            argsort_actions = np.argsort(actions) #TODO: this may not be touched.\n",
    "            #print(\"The actions is: {}\".format(actions))\n",
    "\n",
    "            sell_index = argsort_actions[:np.where(actions == 0)[0].shape[0]]\n",
    "            #sell_index = argsort_actions[4,0,0,0,0,0,0,0,4,0,4,0,-3,0,0,0,0,0,0,0,4,0,0,0,0,0,0,0,0,0]\n",
    "            #print(\"sell-index------:{}\".format(sell_index))\n",
    "            buy_index = argsort_actions[::-1][:np.where(actions == 2)[0].shape[0]]\n",
    "            #buy_index = argsort_actions[::-1][4,0,0,0,0,0,0,0,4,0,4,0,-3,0,0,0,0,0,0,0,4,0,0,0,0,0,0,0,0,0]\n",
    "            #print(\"buy-index------:{}\".format(buy_index))\n",
    "\n",
    "            for index in sell_index:\n",
    "            # print('take sell action'.format(actions[index]))\n",
    "                #print(\"--------Action Shape:{}\".format(actions.shape))\n",
    "                self._sell_stock(index+ self.agent_stock_iteration_index, 1)\n",
    "\n",
    "            for index in buy_index:\n",
    "                #print(\"--------Action Shape:{}\".format(actions.shape))\n",
    "            # print('take buy action: {}'.format(actions[index]))\n",
    "                self._buy_stock(index+ self.agent_stock_iteration_index, 1)\n",
    "                \n",
    "            \n",
    "            #print(\"self.day:{}\".format(self.day))\n",
    "            #load next state\n",
    "            # print(\"stock_shares:{}\".format(self.state[29:]))\n",
    "            self.state =  [self.state[0]] + \\\n",
    "                self.data.Close.values.tolist() + \\\n",
    "                list(self.state[(STOCK_DIM+1):(STOCK_DIM*2+1)]) + \\\n",
    "                self.data.macd.values.tolist() + \\\n",
    "                self.data.rsi.values.tolist() + \\\n",
    "                self.data.cci.values.tolist() + \\\n",
    "                self.data.adx.values.tolist()\n",
    "\n",
    "            end_total_asset = self.state[0]+ \\\n",
    "            sum(np.array(self.state[1:(STOCK_DIM+1)])*np.array(self.state[(STOCK_DIM+1):(STOCK_DIM*2+1)]))\n",
    "            self.asset_memory.append(end_total_asset)\n",
    "            #print(\"end_total_asset:{}\".format(end_total_asset))\n",
    "\n",
    "            self.reward = end_total_asset - begin_total_asset            \n",
    "            self.rewards_memory.append(self.reward)\n",
    "            self.reward = self.reward*REWARD_SCALING\n",
    "            self.agent_stock_iteration_index += 1 \n",
    "            if self.agent_stock_iteration_index ==3:\n",
    "                self.day += 1\n",
    "                self.data = self.df.loc[self.day,:]\n",
    "                self.agent_stock_iteration_index = 0\n",
    "            \n",
    "        return self.state, self.reward, self.terminal, {}\n",
    "\n",
    "    def reset(self):\n",
    "        self.final_asset_value = 0\n",
    "        self.trades = 0\n",
    "        self.asset_memory = [INITIAL_ACCOUNT_BALANCE]\n",
    "        self.day = 0\n",
    "        self.data = self.df.loc[self.day,:]\n",
    "        self.cost = 0\n",
    "        self.trades = 0\n",
    "        self.terminal = False \n",
    "        self.rewards_memory = []\n",
    "        self.agent_stock_iteration_index = 0\n",
    "        #initiate state\n",
    "        self.state = [INITIAL_ACCOUNT_BALANCE] + \\\n",
    "                      self.data.Close.values.tolist() + \\\n",
    "                      [0]*STOCK_DIM + \\\n",
    "                      self.data.macd.values.tolist() + \\\n",
    "                      self.data.rsi.values.tolist() + \\\n",
    "                      self.data.cci.values.tolist() + \\\n",
    "                      self.data.adx.values.tolist() \n",
    "        # iteration += 1 \n",
    "        #print(\"[0]*STOCK_DIM:{}\".format([0]*STOCK_DIM))\n",
    "        #print(\"self.state:{}\".format(len(self.state)))\n",
    "        print(np.array(self.state))\n",
    "        return np.array(self.state)\n",
    "    \n",
    "    def render(self, mode='human'):\n",
    "        return self.state\n",
    "\n",
    "    def _seed(self, seed=None):\n",
    "        self.np_random, seed = seeding.np_random(seed)\n",
    "        return [seed]\n",
    "\n",
    "\n",
    "def run(env,frames=1000, eps_fixed=False, eps_frames=1e6, min_eps=0.01):\n",
    "    \"\"\"Deep Q-Learning.\n",
    "    \n",
    "    Params\n",
    "    ======\n",
    "        n_episodes (int): maximum number of training episodes\n",
    "        max_t (int): maximum number of timesteps per episode\n",
    "        eps_start (float): starting value of epsilon, for epsilon-greedy action selection\n",
    "        eps_end (float): minimum value of epsilon\n",
    "        eps_decay (float): multiplicative factor (per episode) for decreasing epsilon\n",
    "    \"\"\"\n",
    "    scores = []                        # list containing scores from each episode\n",
    "    scores_window = deque(maxlen=100)  # last 100 scores\n",
    "    output_history = []\n",
    "    frame = 0\n",
    "    if eps_fixed:\n",
    "        eps = 0\n",
    "    else:\n",
    "        eps = 1\n",
    "    eps_start = 1\n",
    "    i_episode = 1\n",
    "    state = env.reset()\n",
    "    state = state[0,:]\n",
    "    #print(\"state space:{}\".format(state[0,:].shape))\n",
    "    score = 0                  \n",
    "    for frame in range(1, frames+1):\n",
    "        \n",
    "        if frame  == 0:\n",
    "            # inital state\n",
    "\n",
    "            initial = True\n",
    "        else:\n",
    "            # previous state\n",
    "            initial = False\n",
    "        #print('initial state:{}'.format(initial))\n",
    "\n",
    "        action = agent.act(state, eps) #TODO: getting one dimension back.\n",
    "        action = np.array([action])\n",
    "        next_state, reward, done, info = env_train.step([action]) #TODO: Wants a list of actions of size \n",
    "        #print(\"env_trainNext State: {}\".format(next_state.shape))\n",
    "        next_state = next_state[0,:]\n",
    "        agent.step(state, action, reward, next_state, done, writer)\n",
    "\n",
    "        state = next_state\n",
    "        score += reward\n",
    "        # linear annealing to the min epsilon value until eps_frames and from there slowly decease epsilon to 0 until the end of training\n",
    "        if eps_fixed == False:\n",
    "            if frame < eps_frames:\n",
    "                eps = max(eps_start - (frame*(1/eps_frames)), min_eps)\n",
    "            else:\n",
    "                eps = max(min_eps - min_eps*((frame-eps_frames)/(frames-eps_frames)), 0.001)\n",
    "\n",
    "        # evaluation runs\n",
    "        if frame % 100000 == 0:\n",
    "            print(\"score: {}\".format(state))\n",
    "            print(\"score: {}\".format(score))\n",
    "            #print(\"state: {}\".format(state))\n",
    "            print(\"action:{}, Number:{}\".format(action,frame))\n",
    "            print(\"-------------------------\")\n",
    "        \n",
    "        if done:\n",
    "            scores_window.append(score)       # save most recent score\n",
    "            scores.append(score)              # save most recent score\n",
    "            writer.add_scalar(\"Average100\", np.mean(scores_window), frame)\n",
    "            output_history.append(np.mean(scores_window))\n",
    "            print('\\rEpisode {}\\tFrame {} \\tAverage Score: {:.2f}'.format(i_episode, frame, np.mean(scores_window)), end=\"\")\n",
    "            if i_episode % 100 == 0:\n",
    "                print('\\rEpisode {}\\tFrame {}\\tAverage Score: {:.2f}'.format(i_episode,frame, np.mean(scores_window)))\n",
    "            i_episode +=1 \n",
    "\n",
    "            state = env.reset()\n",
    "            state = state[0,:]\n",
    "            score = 0              \n",
    "\n",
    "    return output_history\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "        # read and preprocess data\n",
    "    preprocessed_path = \"done_3stocks.csv\"\n",
    "    if os.path.exists(preprocessed_path):\n",
    "        data = pd.read_csv(preprocessed_path, index_col=0)\n",
    "\n",
    "    unique_trade_date = data[(data.datadate > 20151001)&(data.datadate <= 20200707)].datadate.unique()\n",
    "    #print(unique_trade_date)\n",
    "\n",
    "    \n",
    "    train = data_split(data, start=20100101, end=20160101)\n",
    "    \n",
    "    env_train = DummyVecEnv([lambda: StockEnvTrain(train)])\n",
    "    \n",
    "    writer = SummaryWriter(\"runs/\"+\"IQN_CP_5\")\n",
    "    seed = 1\n",
    "    BUFFER_SIZE = 10000\n",
    "    BATCH_SIZE = 8\n",
    "    GAMMA = 0.99\n",
    "    TAU = 1e-2\n",
    "    LR = 1e-3\n",
    "    UPDATE_EVERY = 1\n",
    "    n_step = 1\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(\"Using \", device)\n",
    "\n",
    "\n",
    "    action_size     = env_train.action_space.shape[0]\n",
    "\n",
    "    \n",
    "    \n",
    "    print('Action Space: {}'.format(action_size))\n",
    "    state_size = env_train.observation_space.shape[0]\n",
    "    print('State Space: {}'.format(state_size))\n",
    "\n",
    "    agent = DQN_Agent(state_size=19,    #181 #4\n",
    "                        action_size=3, #30 #7\n",
    "                        layer_size=512, #512, #512\n",
    "                        n_step=n_step,\n",
    "                        BATCH_SIZE=BATCH_SIZE, \n",
    "                        BUFFER_SIZE=BUFFER_SIZE, \n",
    "                        LR=LR, \n",
    "                        TAU=TAU, \n",
    "                        GAMMA=GAMMA, \n",
    "                        UPDATE_EVERY=UPDATE_EVERY, \n",
    "                        device=device, \n",
    "                        seed=seed)\n",
    "\n",
    "\n",
    "\n",
    "    # set epsilon frames to 0 so no epsilon exploration\n",
    "    eps_fixed = False\n",
    "    t0 = time.time()\n",
    "    final_average100 = run(env=env_train, frames = 600000, eps_fixed=eps_fixed, eps_frames=5000, min_eps=0.025)\n",
    "    t1 = time.time()\n",
    "    \n",
    "    print(\"Training time: {}min\".format(round((t1-t0)/60,2)))\n",
    "    torch.save(agent.qnetwork_local.state_dict(), \"IQN\"+\".pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
