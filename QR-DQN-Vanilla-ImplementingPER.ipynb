{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "import random\n",
    "import math\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from collections import deque, namedtuple\n",
    "import time\n",
    "import gym\n",
    "import os\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "def weight_init(layers):\n",
    "    for layer in layers:\n",
    "        torch.nn.init.kaiming_normal_(layer.weight, nonlinearity='relu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3.common.vec_env.base_vec_env import VecEnv, VecEnvStepReturn, VecEnvWrapper\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from stockstats import StockDataFrame as Sdf\n",
    "\n",
    "import datetime\n",
    "\n",
    "import datetime\n",
    "import os\n",
    "\n",
    "TRAINING_DATA_FILE = \"dataprocessing/Yfinance_Data.csv\"\n",
    "\n",
    "now = datetime.datetime.now()\n",
    "TRAINED_MODEL_DIR = f\"trained_models/{now}\"\n",
    "os.makedirs(TRAINED_MODEL_DIR)\n",
    "\n",
    "TESTING_DATA_FILE = \"test.csv\"\n",
    "\n",
    "def load_dataset(*, file_name: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    load csv dataset from path\n",
    "    :return: (df) pandas dataframe\n",
    "    \"\"\"\n",
    "    # _data = pd.read_csv(f\"{config.DATASET_DIR}/{file_name}\")\n",
    "    _data = pd.read_csv(file_name)\n",
    "\n",
    "    return _data\n",
    "\n",
    "\n",
    "def data_split(df, start, end):\n",
    "    \"\"\"\n",
    "    split the dataset into training or testing using date\n",
    "    :param data: (df) pandas dataframe, start, end\n",
    "    :return: (df) pandas dataframe\n",
    "    \"\"\"\n",
    "    data = df[(df.datadate >= start) & (df.datadate < end)]\n",
    "    data = data.sort_values(['datadate', 'tic'], ignore_index=True)\n",
    "\n",
    "\n",
    "    # data  = data[final_columns]\n",
    "    data.index = data.datadate.factorize()[0]\n",
    "\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def calculate_price(df):\n",
    "    \"\"\"\n",
    "    calcualte adjusted close price, open-high-low price and volume\n",
    "    :param data: (df) pandas dataframe\n",
    "    :return: (df) pandas dataframe\n",
    "    \"\"\"\n",
    "    data = df.copy()\n",
    "\n",
    "    data = data[['Date', 'tic', 'Close', 'Open', 'High', 'Low', 'Volume','datadate']]\n",
    "    data = data.sort_values(['tic', 'datadate'], ignore_index=True)\n",
    "    return data\n",
    "\n",
    "\n",
    "def add_technical_indicator(df):\n",
    "    \"\"\"\n",
    "    calcualte technical indicators\n",
    "    use stockstats package to add technical inidactors\n",
    "    :param data: (df) pandas dataframe\n",
    "    :return: (df) pandas dataframe\n",
    "    \"\"\"\n",
    "    stock = Sdf.retype(df.copy())\n",
    "\n",
    "    #print(stock)\n",
    "\n",
    "    unique_ticker = stock.tic.unique()\n",
    "\n",
    "    macd = pd.DataFrame()\n",
    "    rsi = pd.DataFrame()\n",
    "    cci = pd.DataFrame()\n",
    "    dx = pd.DataFrame()\n",
    "\n",
    "    # temp = stock[stock.tic == unique_ticker[0]]['macd']\n",
    "    for i in range(len(unique_ticker)):\n",
    "        ## macd\n",
    "        temp_macd = stock[stock.tic == unique_ticker[i]]['macd']\n",
    "        temp_macd = pd.DataFrame(temp_macd)\n",
    "        macd = macd.append(temp_macd, ignore_index=True)\n",
    "        ## rsi\n",
    "        temp_rsi = stock[stock.tic == unique_ticker[i]]['rsi_30']\n",
    "        temp_rsi = pd.DataFrame(temp_rsi)\n",
    "        rsi = rsi.append(temp_rsi, ignore_index=True)\n",
    "        ## cci\n",
    "        temp_cci = stock[stock.tic == unique_ticker[i]]['cci_30']\n",
    "        temp_cci = pd.DataFrame(temp_cci)\n",
    "        cci = cci.append(temp_cci, ignore_index=True)\n",
    "        ## adx\n",
    "        temp_dx = stock[stock.tic == unique_ticker[i]]['dx_30']\n",
    "        temp_dx = pd.DataFrame(temp_dx)\n",
    "        dx = dx.append(temp_dx, ignore_index=True)\n",
    "\n",
    "    df['macd'] = macd\n",
    "    df['rsi'] = rsi\n",
    "    df['cci'] = cci\n",
    "    df['adx'] = dx\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def preprocess_data():\n",
    "    \"\"\"data preprocessing pipeline\"\"\"\n",
    "    start = datetime.datetime(2010, 12, 1)\n",
    "    df = load_dataset(file_name=TRAINING_DATA_FILE)\n",
    "    # get data after 2010\n",
    "    # df = df[df.Date >= start]\n",
    "    # calcualte adjusted price\n",
    "    df_preprocess = calculate_price(df)\n",
    "    # add technical indicators using stockstats\n",
    "    df_final = add_technical_indicator(df_preprocess)\n",
    "    # fill the missing values at the beginning\n",
    "    df_final.fillna(method='bfill', inplace=True)\n",
    "    return df_final\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QR_DQN(nn.Module):\n",
    "    def __init__(self, state_size, action_size,layer_size, n_step, seed, N, layer_type=\"ff\"):\n",
    "        super(QR_DQN, self).__init__()\n",
    "        self.seed = torch.manual_seed(seed)\n",
    "        self.input_shape = state_size\n",
    "        self.action_size = action_size\n",
    "        self.N = N\n",
    "\n",
    "        self.head_1 = nn.Linear(self.input_shape, layer_size)\n",
    "        self.ff_1 = nn.Linear(layer_size, layer_size)\n",
    "        self.ff_2 = nn.Linear(layer_size, action_size*N)\n",
    "        weight_init([self.head_1, self.ff_1])\n",
    "\n",
    "\n",
    "    \n",
    "    def forward(self, input):\n",
    "        \"\"\"\n",
    "        \n",
    "        \"\"\"\n",
    "        x = torch.relu(self.head_1(input))\n",
    "        x = torch.relu(self.ff_1(x))\n",
    "        out = self.ff_2(x)\n",
    "        \n",
    "        return out.view(input.shape[0], self.N, self.action_size)\n",
    "    def get_action(self,input):\n",
    "        x = self.forward(input)\n",
    "        return x.mean(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PrioritizedReplay(object):\n",
    "    \"\"\"\n",
    "    Proportional Prioritization\n",
    "    \"\"\"\n",
    "    def __init__(self, device, capacity, batch_size, seed=1, gamma=0.99, n_step=1, alpha=0.6, beta_start = 0.4, beta_frames=100000, parallel_env=4):\n",
    "        self.alpha = alpha\n",
    "        self.beta_start = beta_start\n",
    "        self.beta_frames = beta_frames\n",
    "        self.device = device\n",
    "        self.frame = 1 #for beta calculation\n",
    "        self.batch_size = batch_size\n",
    "        self.capacity   = capacity\n",
    "        self.buffer     = []\n",
    "        self.pos        = 0\n",
    "        self.experience = namedtuple(\"Experience\", field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
    "        self.priorities = np.zeros((capacity,), dtype=np.float32)\n",
    "        self.seed = np.random.seed(seed)\n",
    "        self.n_step = n_step\n",
    "        self.parallel_env = parallel_env\n",
    "        self.n_step_buffer = deque(maxlen=self.n_step)\n",
    "        self.gamma = gamma\n",
    "        self.memory = deque(maxlen=capacity)  \n",
    "\n",
    "    \n",
    "    def calc_multistep_return(self):\n",
    "        Return = 0\n",
    "        for idx in range(self.n_step):\n",
    "            Return += self.gamma**idx * self.n_step_buffer[idx][2]\n",
    "        \n",
    "        return self.n_step_buffer[0][0], self.n_step_buffer[0][1], Return, self.n_step_buffer[-1][3], self.n_step_buffer[-1][4]\n",
    "        \n",
    "    \n",
    "    def beta_by_frame(self, frame_idx):\n",
    "        \"\"\"\n",
    "        Linearly increases beta from beta_start to 1 over time from 1 to beta_frames.\n",
    "        \n",
    "        3.4 ANNEALING THE BIAS (Paper: PER)\n",
    "        We therefore exploit the flexibility of annealing the amount of importance-sampling\n",
    "        correction over time, by defining a schedule on the exponent \n",
    "        that reaches 1 only at the end of learning. In practice, we linearly anneal from its initial value 0 to 1\n",
    "        \"\"\"\n",
    "        return min(1.0, self.beta_start + frame_idx * (1.0 - self.beta_start) / self.beta_frames)\n",
    "\n",
    "    \n",
    "    \n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Add a new experience to memory.\"\"\"\n",
    "        #print(\"before:\", state,action,reward,next_state, done)\n",
    "        self.n_step_buffer.append((state, action, reward, next_state, done))\n",
    "        if len(self.n_step_buffer) == self.n_step:\n",
    "            state, action, reward, next_state, done = self.calc_multistep_return()\n",
    "            #print(\"after:\",state,action,reward,next_state, done)\n",
    "            e = self.experience(state, action, reward, next_state, done)\n",
    "            #print(self.memory)\n",
    "            self.memory.append(e)\n",
    "            \n",
    "        max_prio = self.priorities.max() if self.buffer else 1.0 # gives max priority if buffer is not empty else 1\n",
    "\n",
    "        if len(self.buffer) < self.capacity:\n",
    "            self.buffer.append((state, action, reward, next_state, done))\n",
    "        else:\n",
    "            # puts the new data on the position of the oldes since it circles via pos variable\n",
    "            # since if len(buffer) == capacity -> pos == 0 -> oldest memory (at least for the first round?) \n",
    "            self.buffer[self.pos] = (state, action, reward, next_state, done) \n",
    "        \n",
    "        self.priorities[self.pos] = max_prio\n",
    "        self.pos = (self.pos + 1) % self.capacity # lets the pos circle in the ranges of capacity if pos+1 > cap --> new posi = 0\n",
    "\n",
    "        \n",
    "    def sample(self):\n",
    "        N = len(self.buffer)\n",
    "        if N == self.capacity:\n",
    "            prios = self.priorities\n",
    "        else:\n",
    "            prios = self.priorities[:self.pos]\n",
    "            \n",
    "        # calc P = p^a/sum(p^a)\n",
    "        probs  = prios ** self.alpha\n",
    "        P = probs/probs.sum()\n",
    "        \n",
    "        #gets the indices depending on the probability p\n",
    "        indices = np.random.choice(N, 1, p=P) \n",
    "        \n",
    "        \n",
    "        #print('indices:{}'.format(indices))\n",
    "        samples = [self.buffer[idx] for idx in indices]\n",
    "        #print('Samples Shape: '.format(len(samples)))\n",
    "        \n",
    "        beta = self.beta_by_frame(self.frame)\n",
    "        self.frame+=1\n",
    "                \n",
    "        #Compute importance-sampling weight\n",
    "        weights  = (N * P[indices])** (-beta)\n",
    "        # normalize weights\n",
    "        weights /= weights.max() \n",
    "        weights  = np.array(weights, dtype=np.float32) \n",
    "        \n",
    "        #print('indices:{}'.format(indices))\n",
    "        \n",
    "        CurrentSequence = indices[0]\n",
    "        \n",
    "        if CurrentSequence < 8:\n",
    "            if len(self.memory) < 16:\n",
    "                indices = np.random.choice(N, 8, p=P)\n",
    "                #print('second indices:{}'.format(indices))\n",
    "                SequenceOfSampling=indices\n",
    "            else: \n",
    "                SequenceOfSampling = [CurrentSequence, CurrentSequence+1,CurrentSequence+2,CurrentSequence+3,CurrentSequence+4,CurrentSequence+5,CurrentSequence+6,CurrentSequence+7]\n",
    "        else:\n",
    "            SequenceOfSampling = [CurrentSequence-7,CurrentSequence-6,CurrentSequence-5,CurrentSequence-4,CurrentSequence-3,CurrentSequence-2,CurrentSequence-1,CurrentSequence]\n",
    "        \n",
    "        #print(SequenceOfSampling)\n",
    "        #print(len(self.memory))\n",
    "        experiences = [self.memory[SequenceOfSampling[0]],self.memory[SequenceOfSampling[1]],self.memory[SequenceOfSampling[2]],self.memory[SequenceOfSampling[3]],self.memory[SequenceOfSampling[4]],self.memory[SequenceOfSampling[5]],self.memory[SequenceOfSampling[6]],self.memory[SequenceOfSampling[7]]]\n",
    "        #print(experiences)\n",
    "        \n",
    "        #print(self.buffer[SequenceOfSampling[0]])\n",
    "        \n",
    "        \n",
    "        states = torch.from_numpy(np.stack([e.state for e in experiences if e is not None])).float().to(self.device)\n",
    "        actions = torch.from_numpy(np.vstack([e.action for e in experiences if e is not None])).long().to(self.device)\n",
    "        rewards = torch.from_numpy(np.vstack([e.reward for e in experiences if e is not None])).float().to(self.device)\n",
    "        next_states = torch.from_numpy(np.stack([e.next_state for e in experiences if e is not None])).float().to(self.device)\n",
    "        dones = torch.from_numpy(np.vstack([e.done for e in experiences if e is not None]).astype(np.uint8)).float().to(self.device)\n",
    "        weights = torch.from_numpy(np.vstack(weights)).float().to(self.device)\n",
    "           \n",
    "        \n",
    "        return states, actions, rewards, next_states, dones, indices, weights\n",
    "    \n",
    "    def update_priorities(self, batch_indices, batch_priorities):\n",
    "        for idx, prio in zip(batch_indices, batch_priorities):\n",
    "            self.priorities[idx] = prio \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN_Agent():\n",
    "    \"\"\"Interacts with and learns from the environment.\"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 state_size,\n",
    "                 action_size,\n",
    "                 Network,\n",
    "                 layer_size,\n",
    "                 n_step,\n",
    "                 BATCH_SIZE,\n",
    "                 BUFFER_SIZE,\n",
    "                 LR,\n",
    "                 TAU,\n",
    "                 GAMMA,\n",
    "                 UPDATE_EVERY,\n",
    "                 device,\n",
    "                 seed):\n",
    "        \"\"\"Initialize an Agent object.\n",
    "        \n",
    "        Params\n",
    "        ======\n",
    "            state_size (int): dimension of each state\n",
    "            action_size (int): dimension of each action\n",
    "            Network (str): dqn network type\n",
    "            layer_size (int): size of the hidden layer\n",
    "            BATCH_SIZE (int): size of the training batch\n",
    "            BUFFER_SIZE (int): size of the replay memory\n",
    "            LR (float): learning rate\n",
    "            TAU (float): tau for soft updating the network weights\n",
    "            GAMMA (float): discount factor\n",
    "            UPDATE_EVERY (int): update frequency\n",
    "            device (str): device that is used for the compute\n",
    "            seed (int): random seed\n",
    "        \"\"\"\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.seed = random.seed(seed)\n",
    "        self.device = device\n",
    "        self.TAU = TAU\n",
    "        self.GAMMA = GAMMA\n",
    "        self.UPDATE_EVERY = UPDATE_EVERY\n",
    "        self.BATCH_SIZE = BATCH_SIZE\n",
    "        self.Q_updates = 0\n",
    "        self.n_step = n_step\n",
    "        self.N = 32\n",
    "        self.quantile_tau = torch.FloatTensor([i/self.N for i in range(1,self.N+1)]).to(device)\n",
    "\n",
    "        self.action_step = 4\n",
    "        self.last_action = None\n",
    " \n",
    "        # Q-Network\n",
    "        self.qnetwork_local = QR_DQN(state_size, action_size,layer_size, n_step, seed, self.N).to(device)\n",
    "        self.qnetwork_target = QR_DQN(state_size, action_size,layer_size, n_step, seed, self.N).to(device)\n",
    "\n",
    "        self.optimizer = optim.Adam(self.qnetwork_local.parameters(), lr=LR)\n",
    "        print(self.qnetwork_local)\n",
    "        \n",
    "        # Replay memory\n",
    "        #self.memory = ReplayBuffer(BUFFER_SIZE, BATCH_SIZE, self.device, seed, self.GAMMA, n_step)\n",
    "        self.memory = PrioritizedReplay(device, BUFFER_SIZE, self.BATCH_SIZE, gamma=self.GAMMA, n_step=n_step, parallel_env=1)\n",
    "\n",
    "        \n",
    "        # Initialize time step (for updating every UPDATE_EVERY steps)\n",
    "        self.t_step = 0\n",
    "    \n",
    "    def step(self, state, action, reward, next_state, done, writer):\n",
    "        # Save experience in replay memory\n",
    "        self.memory.add(state, action, reward, next_state, done)\n",
    "        \n",
    "        # Learn every UPDATE_EVERY time steps.\n",
    "        self.t_step = (self.t_step + 1) % self.UPDATE_EVERY\n",
    "        if self.t_step == 0:\n",
    "            # If enough samples are available in memory, get random subset and learn\n",
    "            if len(self.memory) > self.BATCH_SIZE:\n",
    "                experiences = self.memory.sample()\n",
    "                loss = self.learn(experiences)\n",
    "                self.Q_updates += 1\n",
    "                writer.add_scalar(\"Q_loss\", loss, self.Q_updates)\n",
    "\n",
    "    def act(self, state, eps=0.):\n",
    "        \"\"\"Returns actions for given state as per current policy. Acting only every 4 frames!\n",
    "        \n",
    "        Params\n",
    "        ======\n",
    "            frame: to adjust epsilon\n",
    "            state (array_like): current state\n",
    "            \n",
    "        \"\"\"\n",
    "\n",
    "        if self.action_step == 4:\n",
    "            state = np.array(state)\n",
    "\n",
    "            state = torch.from_numpy(state).float().unsqueeze(0).to(self.device)\n",
    "            self.qnetwork_local.eval()\n",
    "            with torch.no_grad():\n",
    "                action_values = self.qnetwork_local.get_action(state)\n",
    "            self.qnetwork_local.train()\n",
    "\n",
    "            # Epsilon-greedy action selection\n",
    "            if random.random() > eps: # select greedy action if random number is higher than epsilon or noisy network is used!\n",
    "                action = np.argmax(action_values.cpu().data.numpy())\n",
    "                self.last_action = action\n",
    "                return action\n",
    "            else:\n",
    "                action = random.choice(np.arange(self.action_size))\n",
    "                self.last_action = action \n",
    "                return action\n",
    "            #self.action_step = 0\n",
    "        else:\n",
    "            self.action_step += 1\n",
    "            return self.last_action\n",
    "\n",
    "    def learn(self, experiences):\n",
    "        \"\"\"Update value parameters using given batch of experience tuples.\n",
    "        Params\n",
    "        ======\n",
    "            experiences (Tuple[torch.Tensor]): tuple of (s, a, r, s', done) tuples \n",
    "            gamma (float): discount factor\n",
    "        \"\"\"\n",
    "        self.optimizer.zero_grad()\n",
    "        states, actions, rewards, next_states, dones, indices, weights = experiences\n",
    "        # Get max predicted Q values (for next states) from target model\n",
    "        Q_targets_next = self.qnetwork_target(next_states).detach().cpu() #.max(2)[0].unsqueeze(1) #(batch_size, 1, N)\n",
    "        action_indx = torch.argmax(Q_targets_next.mean(dim=1), dim=1, keepdim=True)\n",
    "\n",
    "        Q_targets_next = Q_targets_next.gather(2, action_indx.unsqueeze(-1).expand(self.BATCH_SIZE, self.N, 1)).transpose(1,2)\n",
    "\n",
    "        assert Q_targets_next.shape == (self.BATCH_SIZE,1, self.N)\n",
    "        # Compute Q targets for current states \n",
    "        Q_targets = rewards.unsqueeze(-1) + (self.GAMMA**self.n_step * Q_targets_next.to(self.device) * (1 - dones.unsqueeze(-1)))\n",
    "        # Get expected Q values from local model\n",
    "        Q_expected = self.qnetwork_local(states).gather(2, actions.unsqueeze(-1).expand(self.BATCH_SIZE, self.N, 1))\n",
    "        # Compute loss\n",
    "        td_error = Q_targets - Q_expected\n",
    "        assert td_error.shape == (self.BATCH_SIZE, self.N, self.N), \"wrong td error shape\"\n",
    "        huber_l = calculate_huber_loss(td_error, 1.0)\n",
    "        quantil_l = abs(self.quantile_tau -(td_error.detach() < 0).float()) * huber_l / 1.0\n",
    "\n",
    "        loss = quantil_l.sum(dim=1).mean(dim=1) # , keepdim=True if per weights get multipl\n",
    "        loss = loss.mean()\n",
    "        # Minimize the loss\n",
    "        loss.backward()\n",
    "        #clip_grad_norm_(self.qnetwork_local.parameters(),1)\n",
    "        self.optimizer.step()\n",
    "\n",
    "        # ------------------- update target network ------------------- #\n",
    "        self.soft_update(self.qnetwork_local, self.qnetwork_target)\n",
    "        return loss.detach().cpu().numpy()            \n",
    "\n",
    "    def soft_update(self, local_model, target_model):\n",
    "        \"\"\"Soft update model parameters.\n",
    "        θ_target = τ*θ_local + (1 - τ)*θ_target\n",
    "        Params\n",
    "        ======\n",
    "            local_model (PyTorch model): weights will be copied from\n",
    "            target_model (PyTorch model): weights will be copied to\n",
    "            tau (float): interpolation parameter \n",
    "        \"\"\"\n",
    "        for target_param, local_param in zip(target_model.parameters(), local_model.parameters()):\n",
    "            target_param.data.copy_(self.TAU*local_param.data + (1.0-self.TAU)*target_param.data)\n",
    "            \n",
    "def calculate_huber_loss(td_errors, k=1.0):\n",
    "    \"\"\"\n",
    "    Calculate huber loss element-wisely depending on kappa k.\n",
    "    \"\"\"\n",
    "    loss = torch.where(td_errors.abs() <= k, 0.5 * td_errors.pow(2), k * (td_errors.abs() - 0.5 * k))\n",
    "    assert loss.shape == (td_errors.shape[0], 32, 32), \"huber loss has wrong shape\"\n",
    "    return loss\n",
    "\n",
    "def eval_runs(eps, frame):\n",
    "    \"\"\"\n",
    "    Makes an evaluation run with the current epsilon\n",
    "    \"\"\"\n",
    "    env = gym.make(\"CartPole-v0\")\n",
    "    reward_batch = []\n",
    "    for i in range(5):\n",
    "        state = env.reset()\n",
    "        rewards = 0\n",
    "        while True:\n",
    "            action = agent.act(state, eps)\n",
    "            state, reward, done, _ = env.step(action)\n",
    "            rewards += reward\n",
    "            if done:\n",
    "                break\n",
    "        reward_batch.append(rewards)\n",
    "        \n",
    "    writer.add_scalar(\"Reward\", np.mean(reward_batch), frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using  cpu\n",
      "Action Space: 3\n",
      "State Space: 19\n",
      "QR_DQN(\n",
      "  (head_1): Linear(in_features=19, out_features=512, bias=True)\n",
      "  (ff_1): Linear(in_features=512, out_features=512, bias=True)\n",
      "  (ff_2): Linear(in_features=512, out_features=96, bias=True)\n",
      ")\n",
      "[1.00000000e+03 3.05728571e+01 4.09200000e+01 5.61800000e+01\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 5.60928564e-01\n",
      " 3.58722215e-01 6.24669814e-01 6.21337044e+01 5.67090572e+01\n",
      " 5.85111632e+01 1.68824972e+02 2.09000651e+00 8.31548524e+01\n",
      " 3.37911464e+01 1.15370605e+01 1.08349581e+01]\n",
      "Finished\n",
      "[980.8660730485719, 291.52, 124.3, 326.4, 0.0, 0.0, 0.0, 7.716996917, 1.489069302, -7.871754813, 73.17316442, 56.03029524, 40.94551713, 150.7355356, 87.1548076, -108.6209868, 48.15525942, 21.00079147, 41.59561359]\n",
      "end_total_asset:980.8660730485719\n",
      "Sharpe:  0.009260780075841373\n",
      "[1.00000000e+03 3.05728571e+01 4.09200000e+01 5.61800000e+01\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 5.60928564e-01\n",
      " 3.58722215e-01 6.24669814e-01 6.21337044e+01 5.67090572e+01\n",
      " 5.85111632e+01 1.68824972e+02 2.09000651e+00 8.31548524e+01\n",
      " 3.37911464e+01 1.15370605e+01 1.08349581e+01]\n",
      "Episode 1\tFrame 7546 \tAverage Score: -0.00[1.00000000e+03 3.05728571e+01 4.09200000e+01 5.61800000e+01\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 5.60928564e-01\n",
      " 3.58722215e-01 6.24669814e-01 6.21337044e+01 5.67090572e+01\n",
      " 5.85111632e+01 1.68824972e+02 2.09000651e+00 8.31548524e+01\n",
      " 3.37911464e+01 1.15370605e+01 1.08349581e+01]\n",
      "Finished\n",
      "[853.2926597284943, 291.52, 124.3, 326.4, 0.0, 0.0, 0.0, 7.716996917, 1.489069302, -7.871754813, 73.17316442, 56.03029524, 40.94551713, 150.7355356, 87.1548076, -108.6209868, 48.15525942, 21.00079147, 41.59561359]\n",
      "end_total_asset:853.2926597284943\n",
      "Sharpe:  -0.05824442899833642\n",
      "[1.00000000e+03 3.05728571e+01 4.09200000e+01 5.61800000e+01\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 5.60928564e-01\n",
      " 3.58722215e-01 6.24669814e-01 6.21337044e+01 5.67090572e+01\n",
      " 5.85111632e+01 1.68824972e+02 2.09000651e+00 8.31548524e+01\n",
      " 3.37911464e+01 1.15370605e+01 1.08349581e+01]\n",
      "Episode 2\tFrame 15092 \tAverage Score: -0.01[1.00000000e+03 3.05728571e+01 4.09200000e+01 5.61800000e+01\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 5.60928564e-01\n",
      " 3.58722215e-01 6.24669814e-01 6.21337044e+01 5.67090572e+01\n",
      " 5.85111632e+01 1.68824972e+02 2.09000651e+00 8.31548524e+01\n",
      " 3.37911464e+01 1.15370605e+01 1.08349581e+01]\n",
      "Finished\n",
      "[928.8648831128454, 291.52, 124.3, 326.4, 0.0, 0.0, 0.0, 7.716996917, 1.489069302, -7.871754813, 73.17316442, 56.03029524, 40.94551713, 150.7355356, 87.1548076, -108.6209868, 48.15525942, 21.00079147, 41.59561359]\n",
      "end_total_asset:928.8648831128454\n",
      "Sharpe:  -0.00929375004279553\n",
      "[1.00000000e+03 3.05728571e+01 4.09200000e+01 5.61800000e+01\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 5.60928564e-01\n",
      " 3.58722215e-01 6.24669814e-01 6.21337044e+01 5.67090572e+01\n",
      " 5.85111632e+01 1.68824972e+02 2.09000651e+00 8.31548524e+01\n",
      " 3.37911464e+01 1.15370605e+01 1.08349581e+01]\n",
      "Episode 3\tFrame 22638 \tAverage Score: -0.01[1.00000000e+03 3.05728571e+01 4.09200000e+01 5.61800000e+01\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 5.60928564e-01\n",
      " 3.58722215e-01 6.24669814e-01 6.21337044e+01 5.67090572e+01\n",
      " 5.85111632e+01 1.68824972e+02 2.09000651e+00 8.31548524e+01\n",
      " 3.37911464e+01 1.15370605e+01 1.08349581e+01]\n",
      "Finished\n",
      "[71.52174886996721, 291.52, 124.3, 326.4, 1.0, 1.0, 1.0, 7.716996917, 1.489069302, -7.871754813, 73.17316442, 56.03029524, 40.94551713, 150.7355356, 87.1548076, -108.6209868, 48.15525942, 21.00079147, 41.59561359]\n",
      "end_total_asset:813.7417488699673\n",
      "Sharpe:  -0.07786158922769176\n",
      "[1.00000000e+03 3.05728571e+01 4.09200000e+01 5.61800000e+01\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 5.60928564e-01\n",
      " 3.58722215e-01 6.24669814e-01 6.21337044e+01 5.67090572e+01\n",
      " 5.85111632e+01 1.68824972e+02 2.09000651e+00 8.31548524e+01\n",
      " 3.37911464e+01 1.15370605e+01 1.08349581e+01]\n",
      "Episode 4\tFrame 30184 \tAverage Score: -0.01[1.00000000e+03 3.05728571e+01 4.09200000e+01 5.61800000e+01\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 5.60928564e-01\n",
      " 3.58722215e-01 6.24669814e-01 6.21337044e+01 5.67090572e+01\n",
      " 5.85111632e+01 1.68824972e+02 2.09000651e+00 8.31548524e+01\n",
      " 3.37911464e+01 1.15370605e+01 1.08349581e+01]\n",
      "Finished\n",
      "[538.3201627841927, 291.52, 124.3, 326.4, 1.0, 2.0, 2.0, 7.716996917, 1.489069302, -7.871754813, 73.17316442, 56.03029524, 40.94551713, 150.7355356, 87.1548076, -108.6209868, 48.15525942, 21.00079147, 41.59561359]\n",
      "end_total_asset:1731.2401627841928\n",
      "Sharpe:  0.3458853879002085\n",
      "[1.00000000e+03 3.05728571e+01 4.09200000e+01 5.61800000e+01\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 5.60928564e-01\n",
      " 3.58722215e-01 6.24669814e-01 6.21337044e+01 5.67090572e+01\n",
      " 5.85111632e+01 1.68824972e+02 2.09000651e+00 8.31548524e+01\n",
      " 3.37911464e+01 1.15370605e+01 1.08349581e+01]\n",
      "Episode 5\tFrame 37730 \tAverage Score: 0.01[1.00000000e+03 3.05728571e+01 4.09200000e+01 5.61800000e+01\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 5.60928564e-01\n",
      " 3.58722215e-01 6.24669814e-01 6.21337044e+01 5.67090572e+01\n",
      " 5.85111632e+01 1.68824972e+02 2.09000651e+00 8.31548524e+01\n",
      " 3.37911464e+01 1.15370605e+01 1.08349581e+01]\n",
      "Finished\n",
      "[749.8588525271662, 291.52, 124.3, 326.4, 0.0, 7.0, 1.0, 7.716996917, 1.489069302, -7.871754813, 73.17316442, 56.03029524, 40.94551713, 150.7355356, 87.1548076, -108.6209868, 48.15525942, 21.00079147, 41.59561359]\n",
      "end_total_asset:1946.3588525271662\n",
      "Sharpe:  0.40357435263478314\n",
      "[1.00000000e+03 3.05728571e+01 4.09200000e+01 5.61800000e+01\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 5.60928564e-01\n",
      " 3.58722215e-01 6.24669814e-01 6.21337044e+01 5.67090572e+01\n",
      " 5.85111632e+01 1.68824972e+02 2.09000651e+00 8.31548524e+01\n",
      " 3.37911464e+01 1.15370605e+01 1.08349581e+01]\n",
      "Episode 6\tFrame 45276 \tAverage Score: 0.02[1.00000000e+03 3.05728571e+01 4.09200000e+01 5.61800000e+01\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 5.60928564e-01\n",
      " 3.58722215e-01 6.24669814e-01 6.21337044e+01 5.67090572e+01\n",
      " 5.85111632e+01 1.68824972e+02 2.09000651e+00 8.31548524e+01\n",
      " 3.37911464e+01 1.15370605e+01 1.08349581e+01]\n",
      "Finished\n",
      "[2106.9168162028254, 291.52, 124.3, 326.4, 2.0, 1.0, 1.0, 7.716996917, 1.489069302, -7.871754813, 73.17316442, 56.03029524, 40.94551713, 150.7355356, 87.1548076, -108.6209868, 48.15525942, 21.00079147, 41.59561359]\n",
      "end_total_asset:3140.656816202825\n",
      "Sharpe:  0.5759613152926337\n",
      "[1.00000000e+03 3.05728571e+01 4.09200000e+01 5.61800000e+01\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 5.60928564e-01\n",
      " 3.58722215e-01 6.24669814e-01 6.21337044e+01 5.67090572e+01\n",
      " 5.85111632e+01 1.68824972e+02 2.09000651e+00 8.31548524e+01\n",
      " 3.37911464e+01 1.15370605e+01 1.08349581e+01]\n",
      "Episode 7\tFrame 52822 \tAverage Score: 0.05[1.00000000e+03 3.05728571e+01 4.09200000e+01 5.61800000e+01\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 5.60928564e-01\n",
      " 3.58722215e-01 6.24669814e-01 6.21337044e+01 5.67090572e+01\n",
      " 5.85111632e+01 1.68824972e+02 2.09000651e+00 8.31548524e+01\n",
      " 3.37911464e+01 1.15370605e+01 1.08349581e+01]\n",
      "Finished\n",
      "[1515.774928992908, 291.52, 124.3, 326.4, 0.0, 0.0, 0.0, 7.716996917, 1.489069302, -7.871754813, 73.17316442, 56.03029524, 40.94551713, 150.7355356, 87.1548076, -108.6209868, 48.15525942, 21.00079147, 41.59561359]\n",
      "end_total_asset:1515.774928992908\n",
      "Sharpe:  0.2704286884468135\n",
      "[1.00000000e+03 3.05728571e+01 4.09200000e+01 5.61800000e+01\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 5.60928564e-01\n",
      " 3.58722215e-01 6.24669814e-01 6.21337044e+01 5.67090572e+01\n",
      " 5.85111632e+01 1.68824972e+02 2.09000651e+00 8.31548524e+01\n",
      " 3.37911464e+01 1.15370605e+01 1.08349581e+01]\n",
      "Episode 8\tFrame 60368 \tAverage Score: 0.05[1.00000000e+03 3.05728571e+01 4.09200000e+01 5.61800000e+01\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 5.60928564e-01\n",
      " 3.58722215e-01 6.24669814e-01 6.21337044e+01 5.67090572e+01\n",
      " 5.85111632e+01 1.68824972e+02 2.09000651e+00 8.31548524e+01\n",
      " 3.37911464e+01 1.15370605e+01 1.08349581e+01]\n",
      "Finished\n",
      "[57.21974276991243, 291.52, 124.3, 326.4, 3.0, 4.0, 4.0, 7.716996917, 1.489069302, -7.871754813, 73.17316442, 56.03029524, 40.94551713, 150.7355356, 87.1548076, -108.6209868, 48.15525942, 21.00079147, 41.59561359]\n",
      "end_total_asset:2734.579742769912\n",
      "Sharpe:  0.4864927858807355\n",
      "[1.00000000e+03 3.05728571e+01 4.09200000e+01 5.61800000e+01\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 5.60928564e-01\n",
      " 3.58722215e-01 6.24669814e-01 6.21337044e+01 5.67090572e+01\n",
      " 5.85111632e+01 1.68824972e+02 2.09000651e+00 8.31548524e+01\n",
      " 3.37911464e+01 1.15370605e+01 1.08349581e+01]\n",
      "Episode 9\tFrame 67914 \tAverage Score: 0.06[1.00000000e+03 3.05728571e+01 4.09200000e+01 5.61800000e+01\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 5.60928564e-01\n",
      " 3.58722215e-01 6.24669814e-01 6.21337044e+01 5.67090572e+01\n",
      " 5.85111632e+01 1.68824972e+02 2.09000651e+00 8.31548524e+01\n",
      " 3.37911464e+01 1.15370605e+01 1.08349581e+01]\n",
      "Finished\n",
      "[1916.1661169371048, 291.52, 124.3, 326.4, 0.0, 0.0, 0.0, 7.716996917, 1.489069302, -7.871754813, 73.17316442, 56.03029524, 40.94551713, 150.7355356, 87.1548076, -108.6209868, 48.15525942, 21.00079147, 41.59561359]\n",
      "end_total_asset:1916.1661169371048\n",
      "Sharpe:  0.3232688957247307\n",
      "[1.00000000e+03 3.05728571e+01 4.09200000e+01 5.61800000e+01\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 5.60928564e-01\n",
      " 3.58722215e-01 6.24669814e-01 6.21337044e+01 5.67090572e+01\n",
      " 5.85111632e+01 1.68824972e+02 2.09000651e+00 8.31548524e+01\n",
      " 3.37911464e+01 1.15370605e+01 1.08349581e+01]\n",
      "Episode 10\tFrame 75460 \tAverage Score: 0.07[1.00000000e+03 3.05728571e+01 4.09200000e+01 5.61800000e+01\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 5.60928564e-01\n",
      " 3.58722215e-01 6.24669814e-01 6.21337044e+01 5.67090572e+01\n",
      " 5.85111632e+01 1.68824972e+02 2.09000651e+00 8.31548524e+01\n",
      " 3.37911464e+01 1.15370605e+01 1.08349581e+01]\n",
      "Finished\n",
      "[126.50307153284953, 291.52, 124.3, 326.4, 3.0, 3.0, 3.0, 7.716996917, 1.489069302, -7.871754813, 73.17316442, 56.03029524, 40.94551713, 150.7355356, 87.1548076, -108.6209868, 48.15525942, 21.00079147, 41.59561359]\n",
      "end_total_asset:2353.1630715328492\n",
      "Sharpe:  0.40021112995382296\n",
      "[1.00000000e+03 3.05728571e+01 4.09200000e+01 5.61800000e+01\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 5.60928564e-01\n",
      " 3.58722215e-01 6.24669814e-01 6.21337044e+01 5.67090572e+01\n",
      " 5.85111632e+01 1.68824972e+02 2.09000651e+00 8.31548524e+01\n",
      " 3.37911464e+01 1.15370605e+01 1.08349581e+01]\n",
      "Episode 11\tFrame 83006 \tAverage Score: 0.07[1.00000000e+03 3.05728571e+01 4.09200000e+01 5.61800000e+01\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 5.60928564e-01\n",
      " 3.58722215e-01 6.24669814e-01 6.21337044e+01 5.67090572e+01\n",
      " 5.85111632e+01 1.68824972e+02 2.09000651e+00 8.31548524e+01\n",
      " 3.37911464e+01 1.15370605e+01 1.08349581e+01]\n",
      "Finished\n",
      "[1812.5357799070696, 291.52, 124.3, 326.4, 1.0, 3.0, 1.0, 7.716996917, 1.489069302, -7.871754813, 73.17316442, 56.03029524, 40.94551713, 150.7355356, 87.1548076, -108.6209868, 48.15525942, 21.00079147, 41.59561359]\n",
      "end_total_asset:2803.3557799070695\n",
      "Sharpe:  0.516547270026053\n",
      "[1.00000000e+03 3.05728571e+01 4.09200000e+01 5.61800000e+01\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 5.60928564e-01\n",
      " 3.58722215e-01 6.24669814e-01 6.21337044e+01 5.67090572e+01\n",
      " 5.85111632e+01 1.68824972e+02 2.09000651e+00 8.31548524e+01\n",
      " 3.37911464e+01 1.15370605e+01 1.08349581e+01]\n",
      "Episode 12\tFrame 90552 \tAverage Score: 0.08[1.00000000e+03 3.05728571e+01 4.09200000e+01 5.61800000e+01\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 5.60928564e-01\n",
      " 3.58722215e-01 6.24669814e-01 6.21337044e+01 5.67090572e+01\n",
      " 5.85111632e+01 1.68824972e+02 2.09000651e+00 8.31548524e+01\n",
      " 3.37911464e+01 1.15370605e+01 1.08349581e+01]\n",
      "Finished\n",
      "[127.7439023557918, 291.52, 124.3, 326.4, 2.0, 2.0, 2.0, 7.716996917, 1.489069302, -7.871754813, 73.17316442, 56.03029524, 40.94551713, 150.7355356, 87.1548076, -108.6209868, 48.15525942, 21.00079147, 41.59561359]\n",
      "end_total_asset:1612.1839023557918\n",
      "Sharpe:  0.251320718948832\n",
      "[1.00000000e+03 3.05728571e+01 4.09200000e+01 5.61800000e+01\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 5.60928564e-01\n",
      " 3.58722215e-01 6.24669814e-01 6.21337044e+01 5.67090572e+01\n",
      " 5.85111632e+01 1.68824972e+02 2.09000651e+00 8.31548524e+01\n",
      " 3.37911464e+01 1.15370605e+01 1.08349581e+01]\n",
      "Episode 13\tFrame 98098 \tAverage Score: 0.08[1.00000000e+03 3.05728571e+01 4.09200000e+01 5.61800000e+01\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 5.60928564e-01\n",
      " 3.58722215e-01 6.24669814e-01 6.21337044e+01 5.67090572e+01\n",
      " 5.85111632e+01 1.68824972e+02 2.09000651e+00 8.31548524e+01\n",
      " 3.37911464e+01 1.15370605e+01 1.08349581e+01]\n",
      "score: [612.2509      87.69857     58.41        74.03         2.\n",
      "   1.           3.           1.2390211    0.65703773   0.6688849\n",
      "  59.388367    54.4731      53.578064   224.2912     117.28417\n",
      " 124.84864     36.52754     19.173752    18.12146   ]\n",
      "score: [0.0068148]\n",
      "action:[2], Number:100000\n",
      "-------------------------\n",
      "Finished\n",
      "[920.0204758328267, 291.52, 124.3, 326.4, 1.0, 0.0, 0.0, 7.716996917, 1.489069302, -7.871754813, 73.17316442, 56.03029524, 40.94551713, 150.7355356, 87.1548076, -108.6209868, 48.15525942, 21.00079147, 41.59561359]\n",
      "end_total_asset:1211.5404758328268\n",
      "Sharpe:  0.12344677061682914\n",
      "[1.00000000e+03 3.05728571e+01 4.09200000e+01 5.61800000e+01\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 5.60928564e-01\n",
      " 3.58722215e-01 6.24669814e-01 6.21337044e+01 5.67090572e+01\n",
      " 5.85111632e+01 1.68824972e+02 2.09000651e+00 8.31548524e+01\n",
      " 3.37911464e+01 1.15370605e+01 1.08349581e+01]\n",
      "Episode 14\tFrame 105644 \tAverage Score: 0.08[1.00000000e+03 3.05728571e+01 4.09200000e+01 5.61800000e+01\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 5.60928564e-01\n",
      " 3.58722215e-01 6.24669814e-01 6.21337044e+01 5.67090572e+01\n",
      " 5.85111632e+01 1.68824972e+02 2.09000651e+00 8.31548524e+01\n",
      " 3.37911464e+01 1.15370605e+01 1.08349581e+01]\n",
      "Finished\n",
      "[830.3415786656428, 291.52, 124.3, 326.4, 6.0, 2.0, 2.0, 7.716996917, 1.489069302, -7.871754813, 73.17316442, 56.03029524, 40.94551713, 150.7355356, 87.1548076, -108.6209868, 48.15525942, 21.00079147, 41.59561359]\n",
      "end_total_asset:3480.8615786656424\n",
      "Sharpe:  0.5976274387469533\n",
      "[1.00000000e+03 3.05728571e+01 4.09200000e+01 5.61800000e+01\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 5.60928564e-01\n",
      " 3.58722215e-01 6.24669814e-01 6.21337044e+01 5.67090572e+01\n",
      " 5.85111632e+01 1.68824972e+02 2.09000651e+00 8.31548524e+01\n",
      " 3.37911464e+01 1.15370605e+01 1.08349581e+01]\n",
      "Episode 15\tFrame 113190 \tAverage Score: 0.09[1.00000000e+03 3.05728571e+01 4.09200000e+01 5.61800000e+01\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 5.60928564e-01\n",
      " 3.58722215e-01 6.24669814e-01 6.21337044e+01 5.67090572e+01\n",
      " 5.85111632e+01 1.68824972e+02 2.09000651e+00 8.31548524e+01\n",
      " 3.37911464e+01 1.15370605e+01 1.08349581e+01]\n",
      "Finished\n",
      "[66.34810596294045, 291.52, 124.3, 326.4, 6.0, 2.0, 2.0, 7.716996917, 1.489069302, -7.871754813, 73.17316442, 56.03029524, 40.94551713, 150.7355356, 87.1548076, -108.6209868, 48.15525942, 21.00079147, 41.59561359]\n",
      "end_total_asset:2716.86810596294\n",
      "Sharpe:  0.44618349865576895\n",
      "[1.00000000e+03 3.05728571e+01 4.09200000e+01 5.61800000e+01\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 5.60928564e-01\n",
      " 3.58722215e-01 6.24669814e-01 6.21337044e+01 5.67090572e+01\n",
      " 5.85111632e+01 1.68824972e+02 2.09000651e+00 8.31548524e+01\n",
      " 3.37911464e+01 1.15370605e+01 1.08349581e+01]\n",
      "Episode 16\tFrame 120736 \tAverage Score: 0.09[1.00000000e+03 3.05728571e+01 4.09200000e+01 5.61800000e+01\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 5.60928564e-01\n",
      " 3.58722215e-01 6.24669814e-01 6.21337044e+01 5.67090572e+01\n",
      " 5.85111632e+01 1.68824972e+02 2.09000651e+00 8.31548524e+01\n",
      " 3.37911464e+01 1.15370605e+01 1.08349581e+01]\n",
      "Finished\n",
      "[159.74565589703997, 291.52, 124.3, 326.4, 1.0, 7.0, 2.0, 7.716996917, 1.489069302, -7.871754813, 73.17316442, 56.03029524, 40.94551713, 150.7355356, 87.1548076, -108.6209868, 48.15525942, 21.00079147, 41.59561359]\n",
      "end_total_asset:1974.1656558970399\n",
      "Sharpe:  0.3340614297452782\n",
      "[1.00000000e+03 3.05728571e+01 4.09200000e+01 5.61800000e+01\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 5.60928564e-01\n",
      " 3.58722215e-01 6.24669814e-01 6.21337044e+01 5.67090572e+01\n",
      " 5.85111632e+01 1.68824972e+02 2.09000651e+00 8.31548524e+01\n",
      " 3.37911464e+01 1.15370605e+01 1.08349581e+01]\n",
      "Episode 17\tFrame 128282 \tAverage Score: 0.09[1.00000000e+03 3.05728571e+01 4.09200000e+01 5.61800000e+01\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 5.60928564e-01\n",
      " 3.58722215e-01 6.24669814e-01 6.21337044e+01 5.67090572e+01\n",
      " 5.85111632e+01 1.68824972e+02 2.09000651e+00 8.31548524e+01\n",
      " 3.37911464e+01 1.15370605e+01 1.08349581e+01]\n",
      "Finished\n",
      "[1485.0674173343039, 291.52, 124.3, 326.4, 0.0, 0.0, 1.0, 7.716996917, 1.489069302, -7.871754813, 73.17316442, 56.03029524, 40.94551713, 150.7355356, 87.1548076, -108.6209868, 48.15525942, 21.00079147, 41.59561359]\n",
      "end_total_asset:1811.4674173343037\n",
      "Sharpe:  0.2902001239264799\n",
      "[1.00000000e+03 3.05728571e+01 4.09200000e+01 5.61800000e+01\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 5.60928564e-01\n",
      " 3.58722215e-01 6.24669814e-01 6.21337044e+01 5.67090572e+01\n",
      " 5.85111632e+01 1.68824972e+02 2.09000651e+00 8.31548524e+01\n",
      " 3.37911464e+01 1.15370605e+01 1.08349581e+01]\n",
      "Episode 18\tFrame 135828 \tAverage Score: 0.09[1.00000000e+03 3.05728571e+01 4.09200000e+01 5.61800000e+01\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 5.60928564e-01\n",
      " 3.58722215e-01 6.24669814e-01 6.21337044e+01 5.67090572e+01\n",
      " 5.85111632e+01 1.68824972e+02 2.09000651e+00 8.31548524e+01\n",
      " 3.37911464e+01 1.15370605e+01 1.08349581e+01]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [40]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    357\u001b[0m eps_fixed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    358\u001b[0m t0 \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m--> 359\u001b[0m final_average100 \u001b[38;5;241m=\u001b[39m \u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menv_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframes\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m600000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meps_fixed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meps_fixed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meps_frames\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmin_eps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.025\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    360\u001b[0m t1 \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m    362\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining time: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124mmin\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mround\u001b[39m((t1\u001b[38;5;241m-\u001b[39mt0)\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m60\u001b[39m,\u001b[38;5;241m2\u001b[39m)))\n",
      "Input \u001b[0;32mIn [40]\u001b[0m, in \u001b[0;36mrun\u001b[0;34m(env, frames, eps_fixed, eps_frames, min_eps)\u001b[0m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m#print(\"env_trainNext State: {}\".format(next_state.shape))\u001b[39;00m\n\u001b[1;32m    265\u001b[0m next_state \u001b[38;5;241m=\u001b[39m next_state[\u001b[38;5;241m0\u001b[39m,:]\n\u001b[0;32m--> 266\u001b[0m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreward\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnext_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdone\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwriter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    268\u001b[0m state \u001b[38;5;241m=\u001b[39m next_state\n\u001b[1;32m    269\u001b[0m score \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m reward\n",
      "Input \u001b[0;32mIn [39]\u001b[0m, in \u001b[0;36mDQN_Agent.step\u001b[0;34m(self, state, action, reward, next_state, done, writer)\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmemory) \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mBATCH_SIZE:\n\u001b[1;32m     75\u001b[0m     experiences \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmemory\u001b[38;5;241m.\u001b[39msample()\n\u001b[0;32m---> 76\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexperiences\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     77\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mQ_updates \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     78\u001b[0m     writer\u001b[38;5;241m.\u001b[39madd_scalar(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mQ_loss\u001b[39m\u001b[38;5;124m\"\u001b[39m, loss, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mQ_updates)\n",
      "Input \u001b[0;32mIn [39]\u001b[0m, in \u001b[0;36mDQN_Agent.learn\u001b[0;34m(self, experiences)\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m    146\u001b[0m \u001b[38;5;66;03m# ------------------- update target network ------------------- #\u001b[39;00m\n\u001b[0;32m--> 147\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msoft_update\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mqnetwork_local\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mqnetwork_target\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    148\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\n",
      "Input \u001b[0;32mIn [39]\u001b[0m, in \u001b[0;36mDQN_Agent.soft_update\u001b[0;34m(self, local_model, target_model)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;124;03m\"\"\"Soft update model parameters.\u001b[39;00m\n\u001b[1;32m    152\u001b[0m \u001b[38;5;124;03mθ_target = τ*θ_local + (1 - τ)*θ_target\u001b[39;00m\n\u001b[1;32m    153\u001b[0m \u001b[38;5;124;03mParams\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    157\u001b[0m \u001b[38;5;124;03m    tau (float): interpolation parameter \u001b[39;00m\n\u001b[1;32m    158\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m target_param, local_param \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(target_model\u001b[38;5;241m.\u001b[39mparameters(), local_model\u001b[38;5;241m.\u001b[39mparameters()):\n\u001b[0;32m--> 160\u001b[0m     \u001b[43mtarget_param\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy_\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTAU\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mlocal_param\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1.0\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTAU\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mtarget_param\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from IPython import display\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from gym.utils import seeding\n",
    "import gym\n",
    "import os\n",
    "from gym import spaces\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "\n",
    "# shares normalization factor\n",
    "# 100 shares per trade\n",
    "HMAX_NORMALIZE = 100\n",
    "# initial amount of money we have in our account\n",
    "INITIAL_ACCOUNT_BALANCE= 1000\n",
    "# total number of stocks in our portfolio\n",
    "STOCK_DIM = 3\n",
    "# transaction fee: 1/1000 reasonable percentage\n",
    "TRANSACTION_FEE_PERCENT = 0.001\n",
    "REWARD_SCALING = 1e-4\n",
    "\n",
    "class StockEnvTrain(gym.Env):\n",
    "    \"\"\"A stock trading environment for OpenAI gym\"\"\"\n",
    "    metadata = {'render.modes': ['human']}\n",
    "\n",
    "    def __init__(self, df,day = 0):\n",
    "        #super(StockEnv, self).__init__()\n",
    "        #money = 10 , scope = 1\n",
    "        self.day = day\n",
    "        self.df = df\n",
    "        self.agent_stock_iteration_index = 0\n",
    "\n",
    "        # action_space normalization and shape is STOCK_DIM\n",
    "        self.action_space = spaces.Box(low = -1, high = 1,shape = (STOCK_DIM,)) \n",
    "        # Shape = 181: [Current Balance]+[prices 1-30]+[owned shares 1-30] \n",
    "        # +[macd 1-30]+ [rsi 1-30] + [cci 1-30] + [adx 1-30]\n",
    "        self.observation_space = spaces.Box(low=0, high=np.inf, shape = (19,))\n",
    "        # load data from a pandas dataframe\n",
    "        #print('df: {}'.format(self.df))\n",
    "        #print('day: {}'.format(self.day))\n",
    "        self.data = self.df.loc[self.day,:]\n",
    "        #print(self.data.Close)\n",
    "        self.terminal = False\n",
    "        # initalize state\n",
    "        self.state = [INITIAL_ACCOUNT_BALANCE] + \\\n",
    "                      self.data.Close.values.tolist() + \\\n",
    "                      [0]*STOCK_DIM + \\\n",
    "                      self.data.macd.values.tolist() + \\\n",
    "                      self.data.rsi.values.tolist() + \\\n",
    "                      self.data.cci.values.tolist() + \\\n",
    "                      self.data.adx.values.tolist()\n",
    "        # initialize reward\n",
    "        self.reward = 0\n",
    "        self.cost = 0\n",
    "        # memorize all the total balance change\n",
    "        self.asset_memory = [INITIAL_ACCOUNT_BALANCE]\n",
    "        self.rewards_memory = []\n",
    "        self.final_asset_value = 0\n",
    "        self.trades = 0\n",
    "        #self.reset()\n",
    "        self._seed()\n",
    "\n",
    "\n",
    "    def _sell_stock(self, index, action):\n",
    "        # perform sell action based on the sign of the action\n",
    "        if self.state[index+STOCK_DIM+1] > 0:\n",
    "            #update balance\n",
    "            self.state[0] += \\\n",
    "            self.state[index+1]*min(abs(action),self.state[index+STOCK_DIM+1]) * \\\n",
    "             (1- TRANSACTION_FEE_PERCENT)\n",
    "            self.state[index+STOCK_DIM+1] -= min(abs(action), self.state[index+STOCK_DIM+1])\n",
    "            self.cost +=self.state[index+1]*min(abs(action),self.state[index+STOCK_DIM+1]) * \\\n",
    "             TRANSACTION_FEE_PERCENT\n",
    "            self.trades+=1\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "    \n",
    "    def _buy_stock(self, index, action):\n",
    "        # perform buy action based on the sign of the action\n",
    "        available_amount = self.state[0] // self.state[index+1]\n",
    "        # print('available_amount:{}'.format(available_amount))\n",
    "\n",
    "        #update balance\n",
    "        self.state[0] -= self.state[index+1]*min(available_amount, action)* \\\n",
    "                          (1+ TRANSACTION_FEE_PERCENT)\n",
    "        self.state[index+STOCK_DIM+1] += min(available_amount, action)\n",
    "        self.cost+=self.state[index+1]*min(available_amount, action)* \\\n",
    "                          TRANSACTION_FEE_PERCENT\n",
    "        self.trades+=1\n",
    "        \n",
    "        \n",
    "    def step(self, actions):\n",
    "        # print(self.day)\n",
    "        self.terminal = self.day >= len(self.df.index.unique())-1\n",
    "        #print(actions)\n",
    "        self.actions = actions\n",
    "        if self.terminal:\n",
    "            print(\"Finished\")\n",
    "            print(self.state)\n",
    "            end_total_asset = self.state[0]+ \\\n",
    "            sum(np.array(self.state[1:(STOCK_DIM+1)])*np.array(self.state[(STOCK_DIM+1):(STOCK_DIM*2+1)]))\n",
    "\n",
    "            print(\"end_total_asset:{}\".format(end_total_asset))\n",
    "            df_total_value = pd.DataFrame(self.asset_memory)\n",
    "            #df_total_value.to_csv('results/account_value_train.csv')\n",
    "            #print(\"total_reward:{}\".format(self.state[0]+sum(np.array(self.state[1:(STOCK_DIM+1)])*np.array(self.state[(STOCK_DIM+1):61]))- INITIAL_ACCOUNT_BALANCE ))\n",
    "            #print(\"total_cost: \", self.cost)\n",
    "            #print(\"total_trades: \", self.trades)\n",
    "            df_total_value.columns = ['account_value']\n",
    "            df_total_value['daily_return']=df_total_value.pct_change(1)\n",
    "            sharpe = (252**0.5)*df_total_value['daily_return'].mean()/ \\\n",
    "                  df_total_value['daily_return'].std()\n",
    "            print(\"Sharpe: \",sharpe)\n",
    "            #print(\"=================================\")\n",
    "            df_rewards = pd.DataFrame(self.rewards_memory)\n",
    "            #df_rewards.to_csv('results/account_rewards_train.csv')\n",
    "\n",
    "            # print('total asset: {}'.format(self.state[0]+ sum(np.array(self.state[1:29])*np.array(self.state[29:]))))\n",
    "            #with open('obs.pkl', 'wb') as f:  \n",
    "            #    pickle.dump(self.state, \n",
    "            return self.state, self.reward, self.terminal,{}\n",
    "\n",
    "        else:\n",
    "            # print(np.array(self.state[1:29]))\n",
    "            #print(\"The actions is: {}\".format(self.actions))\n",
    "\n",
    "            #action = np.array([4,4,5])\n",
    "            #actions = np.array([4,0,0,0,0,0,0,0,4,0,4,0,-3,0,0,0,0,0,0,0,4,0,0,0,0,0,0,0,0,0])\n",
    "\n",
    "            #actions = self.actions * HMAX_NORMALIZE #WHY??\n",
    "            #print(\"actions-index------:{}\".format(actions))\n",
    "            #actions = (actions.astype(int))\n",
    "\n",
    "            begin_total_asset = self.state[0]+ \\\n",
    "            sum(np.array(self.state[1:(STOCK_DIM+1)])*np.array(self.state[(STOCK_DIM+1):(STOCK_DIM*2+1)]))\n",
    "            #print(\"begin_total_asset:{}\".format(begin_total_asset))\n",
    "\n",
    "            argsort_actions = np.argsort(actions) #TODO: this may not be touched.\n",
    "            #print(\"The actions is: {}\".format(actions))\n",
    "\n",
    "            sell_index = argsort_actions[:np.where(actions == 0)[0].shape[0]]\n",
    "            #sell_index = argsort_actions[4,0,0,0,0,0,0,0,4,0,4,0,-3,0,0,0,0,0,0,0,4,0,0,0,0,0,0,0,0,0]\n",
    "            #print(\"sell-index------:{}\".format(sell_index))\n",
    "            buy_index = argsort_actions[::-1][:np.where(actions == 2)[0].shape[0]]\n",
    "            #buy_index = argsort_actions[::-1][4,0,0,0,0,0,0,0,4,0,4,0,-3,0,0,0,0,0,0,0,4,0,0,0,0,0,0,0,0,0]\n",
    "            #print(\"buy-index------:{}\".format(buy_index))\n",
    "\n",
    "            for index in sell_index:\n",
    "            # print('take sell action'.format(actions[index]))\n",
    "                #print(\"--------Action Shape:{}\".format(actions.shape))\n",
    "                self._sell_stock(index+ self.agent_stock_iteration_index, 1)\n",
    "\n",
    "            for index in buy_index:\n",
    "                #print(\"--------Action Shape:{}\".format(actions.shape))\n",
    "            # print('take buy action: {}'.format(actions[index]))\n",
    "                self._buy_stock(index+ self.agent_stock_iteration_index, 1)\n",
    "                \n",
    "            \n",
    "            #print(\"self.day:{}\".format(self.day))\n",
    "            #load next state\n",
    "            # print(\"stock_shares:{}\".format(self.state[29:]))\n",
    "            self.state =  [self.state[0]] + \\\n",
    "                self.data.Close.values.tolist() + \\\n",
    "                list(self.state[(STOCK_DIM+1):(STOCK_DIM*2+1)]) + \\\n",
    "                self.data.macd.values.tolist() + \\\n",
    "                self.data.rsi.values.tolist() + \\\n",
    "                self.data.cci.values.tolist() + \\\n",
    "                self.data.adx.values.tolist()\n",
    "\n",
    "            end_total_asset = self.state[0]+ \\\n",
    "            sum(np.array(self.state[1:(STOCK_DIM+1)])*np.array(self.state[(STOCK_DIM+1):(STOCK_DIM*2+1)]))\n",
    "            self.asset_memory.append(end_total_asset)\n",
    "            #print(\"end_total_asset:{}\".format(end_total_asset))\n",
    "\n",
    "            self.reward = end_total_asset - begin_total_asset            \n",
    "            self.rewards_memory.append(self.reward)\n",
    "            self.reward = self.reward*REWARD_SCALING\n",
    "            self.agent_stock_iteration_index += 1 \n",
    "            if self.agent_stock_iteration_index ==3:\n",
    "                self.day += 1\n",
    "                self.data = self.df.loc[self.day,:]\n",
    "                self.agent_stock_iteration_index = 0\n",
    "            \n",
    "        return self.state, self.reward, self.terminal, {}\n",
    "\n",
    "    def reset(self):\n",
    "        self.final_asset_value = 0\n",
    "        self.trades = 0\n",
    "        self.asset_memory = [INITIAL_ACCOUNT_BALANCE]\n",
    "        self.day = 0\n",
    "        self.data = self.df.loc[self.day,:]\n",
    "        self.cost = 0\n",
    "        self.trades = 0\n",
    "        self.terminal = False \n",
    "        self.rewards_memory = []\n",
    "        self.agent_stock_iteration_index = 0\n",
    "        #initiate state\n",
    "        self.state = [INITIAL_ACCOUNT_BALANCE] + \\\n",
    "                      self.data.Close.values.tolist() + \\\n",
    "                      [0]*STOCK_DIM + \\\n",
    "                      self.data.macd.values.tolist() + \\\n",
    "                      self.data.rsi.values.tolist() + \\\n",
    "                      self.data.cci.values.tolist() + \\\n",
    "                      self.data.adx.values.tolist() \n",
    "        # iteration += 1 \n",
    "        #print(\"[0]*STOCK_DIM:{}\".format([0]*STOCK_DIM))\n",
    "        #print(\"self.state:{}\".format(len(self.state)))\n",
    "        print(np.array(self.state))\n",
    "        return np.array(self.state)\n",
    "    \n",
    "    def render(self, mode='human'):\n",
    "        return self.state\n",
    "\n",
    "    def _seed(self, seed=None):\n",
    "        self.np_random, seed = seeding.np_random(seed)\n",
    "        return [seed]\n",
    "\n",
    "\n",
    "def run(env,frames=1000, eps_fixed=False, eps_frames=1e6, min_eps=0.01):\n",
    "    \"\"\"Deep Q-Learning.\n",
    "    \n",
    "    Params\n",
    "    ======\n",
    "        n_episodes (int): maximum number of training episodes\n",
    "        max_t (int): maximum number of timesteps per episode\n",
    "        eps_start (float): starting value of epsilon, for epsilon-greedy action selection\n",
    "        eps_end (float): minimum value of epsilon\n",
    "        eps_decay (float): multiplicative factor (per episode) for decreasing epsilon\n",
    "    \"\"\"\n",
    "    scores = []                        # list containing scores from each episode\n",
    "    scores_window = deque(maxlen=100)  # last 100 scores\n",
    "    output_history = []\n",
    "    frame = 0\n",
    "    if eps_fixed:\n",
    "        eps = 0\n",
    "    else:\n",
    "        eps = 1\n",
    "    eps_start = 1\n",
    "    i_episode = 1\n",
    "    state = env.reset()\n",
    "    state = state[0,:]\n",
    "    #print(\"state space:{}\".format(state[0,:].shape))\n",
    "    score = 0                  \n",
    "    for frame in range(1, frames+1):\n",
    "        \n",
    "        if frame  == 0:\n",
    "            # inital state\n",
    "\n",
    "            initial = True\n",
    "        else:\n",
    "            # previous state\n",
    "            initial = False\n",
    "        #print('initial state:{}'.format(initial))\n",
    "\n",
    "        action = agent.act(state, eps) #TODO: getting one dimension back.\n",
    "        action = np.array([action])\n",
    "        next_state, reward, done, info = env_train.step([action]) #TODO: Wants a list of actions of size \n",
    "        #print(\"env_trainNext State: {}\".format(next_state.shape))\n",
    "        next_state = next_state[0,:]\n",
    "        agent.step(state, action, reward, next_state, done, writer)\n",
    "\n",
    "        state = next_state\n",
    "        score += reward\n",
    "        # linear annealing to the min epsilon value until eps_frames and from there slowly decease epsilon to 0 until the end of training\n",
    "        if eps_fixed == False:\n",
    "            if frame < eps_frames:\n",
    "                eps = max(eps_start - (frame*(1/eps_frames)), min_eps)\n",
    "            else:\n",
    "                eps = max(min_eps - min_eps*((frame-eps_frames)/(frames-eps_frames)), 0.001)\n",
    "\n",
    "        # evaluation runs\n",
    "        if frame % 100000 == 0:\n",
    "            print(\"score: {}\".format(state))\n",
    "            print(\"score: {}\".format(score))\n",
    "            #print(\"state: {}\".format(state))\n",
    "            print(\"action:{}, Number:{}\".format(action,frame))\n",
    "            print(\"-------------------------\")\n",
    "        \n",
    "        if done:\n",
    "            scores_window.append(score)       # save most recent score\n",
    "            scores.append(score)              # save most recent score\n",
    "            writer.add_scalar(\"Average100\", np.mean(scores_window), frame)\n",
    "            output_history.append(np.mean(scores_window))\n",
    "            print('\\rEpisode {}\\tFrame {} \\tAverage Score: {:.2f}'.format(i_episode, frame, np.mean(scores_window)), end=\"\")\n",
    "            if i_episode % 100 == 0:\n",
    "                print('\\rEpisode {}\\tFrame {}\\tAverage Score: {:.2f}'.format(i_episode,frame, np.mean(scores_window)))\n",
    "            i_episode +=1 \n",
    "\n",
    "            state = env.reset()\n",
    "            state = state[0,:]\n",
    "            score = 0              \n",
    "\n",
    "    return output_history\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "        # read and preprocess data\n",
    "    preprocessed_path = \"done_3stocks.csv\"\n",
    "    if os.path.exists(preprocessed_path):\n",
    "        data = pd.read_csv(preprocessed_path, index_col=0)\n",
    "\n",
    "    unique_trade_date = data[(data.datadate > 20101001)&(data.datadate <= 20200707)].datadate.unique()\n",
    "    #print(unique_trade_date)\n",
    "\n",
    "    \n",
    "    train = data_split(data, start=20100101, end=20200101)\n",
    "    \n",
    "    env_train = DummyVecEnv([lambda: StockEnvTrain(train)])\n",
    "    \n",
    "    writer = SummaryWriter(\"runs/\"+\"IQN_CP_5\")\n",
    "    seed = 1\n",
    "    BUFFER_SIZE = 10000\n",
    "    BATCH_SIZE = 8\n",
    "    GAMMA = 0.99\n",
    "    TAU = 1e-2\n",
    "    LR = 1e-3\n",
    "    UPDATE_EVERY = 1\n",
    "    n_step = 1\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(\"Using \", device)\n",
    "\n",
    "\n",
    "    action_size     = env_train.action_space.shape[0]\n",
    "\n",
    "    \n",
    "    \n",
    "    print('Action Space: {}'.format(action_size))\n",
    "    state_size = env_train.observation_space.shape[0]\n",
    "    print('State Space: {}'.format(state_size))\n",
    "\n",
    "    \n",
    "\n",
    "    agent = DQN_Agent(state_size=19,    \n",
    "                    action_size=3,\n",
    "                    Network=\"DDQN\",\n",
    "                    layer_size=512,\n",
    "                    n_step=n_step,\n",
    "                    BATCH_SIZE=BATCH_SIZE, \n",
    "                    BUFFER_SIZE=BUFFER_SIZE, \n",
    "                    LR=LR, \n",
    "                    TAU=TAU, \n",
    "                    GAMMA=GAMMA, \n",
    "                    UPDATE_EVERY=UPDATE_EVERY, \n",
    "                    device=device, \n",
    "                    seed=seed)\n",
    "\n",
    "\n",
    "\n",
    "    # set epsilon frames to 0 so no epsilon exploration\n",
    "    eps_fixed = False\n",
    "    t0 = time.time()\n",
    "    final_average100 = run(env=env_train, frames = 600000, eps_fixed=eps_fixed, eps_frames=5000, min_eps=0.025)\n",
    "    t1 = time.time()\n",
    "    \n",
    "    print(\"Training time: {}min\".format(round((t1-t0)/60,2)))\n",
    "    torch.save(agent.qnetwork_local.state_dict(), \"IQN\"+\".pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
